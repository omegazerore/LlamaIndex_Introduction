{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a763ceac-b9c3-43ec-9ed2-ccf0f59c3473",
   "metadata": {},
   "source": [
    "# 持久化 RAG 系統的完整實戰教學\n",
    "\n",
    "本文是一份工程導向的完整教學稿，目標不是只「跑得動」，而是讓你真正理解：\n",
    "\n",
    "為什麼這樣設計索引\n",
    "\n",
    ">- 為什麼 persistence 會出錯\n",
    ">- 為什麼 Retrieval 策略需要隨資料規模演進\n",
    ">- 本文將逐步建構一個 可持久化（Persistent）、可擴展（Scalable）、避免常見陷阱 的 RAG 系統。\n",
    "\n",
    "## 0️⃣ 前言：我們要解決什麼問題？\n",
    "\n",
    "在實務上，RAG 系統常見三個痛點：\n",
    "\n",
    ">- 索引與 Embedding 維度不一致，導致查詢時崩潰\n",
    ">- 向量資料明明存在磁碟，卻怎麼查都查不到\n",
    ">- 文件一多，單一相似度搜尋開始失準\n",
    "\n",
    "![caption](Gemini_Generated_Image_pzkcrgpzkcrgpzkc.png)\n",
    "\n",
    "這三個問題，其實都來自同一件事： RAG 系統是一個「狀態型系統（stateful system）」，而不是一次性腳本。\n",
    "\n",
    "本文將用 LlamaIndex + FAISS/Qdrant，從資料蒐集一路走到 Advanced Retrieval，並在每一個關鍵點說清楚「為什麼」。\n",
    "\n",
    "## 1️⃣ 資料準備與文件建模（Document Layer）\n",
    "為什麼這一步重要？\n",
    "\n",
    "RAG 系統的品質，80% 來自於 資料建模是否合理。如果一開始文件與 metadata 設計錯誤，後面的 embedding、retrieval 都只是在放大錯誤。\n",
    "\n",
    "### 資料來源選擇\n",
    "\n",
    "我們使用 Wikipedia（中文）作為資料來源，原因是：\n",
    "\n",
    ">- 內容長度中等（適合 chunking 示範）\n",
    ">- 結構鬆散（接近真實世界資料）\n",
    ">- 主題明確（方便驗證 retrieval 正確性）\n",
    "\n",
    "### Metadata 設計\n",
    "\n",
    "每份文件都會附加 metadata，例如：\n",
    "\n",
    "- author：漫畫作者\n",
    "\n",
    "為什麼要這樣做？\n",
    "\n",
    "Metadata 並不是裝飾品，而是：\n",
    "\n",
    ">- Retrieval 階段的「搜尋空間約束條件」\n",
    ">- 未來進行 metadata filtering / auto-retrieval 的基礎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a4fa99-e32f-4506-bf15-1aedd83b7501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import FlatReader\n",
    "from llama_index.core import SimpleDirectoryReader#, SummaryIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95555c5-d9df-40b9-9524-4e00cd87783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_titles = [\"鋼之鍊金術師\", \"一拳超人\", \"ONE_PIECE\", \"東京喰種\"]\n",
    "wiki_metadatas = {\n",
    "    \"鋼之鍊金術師\": {\n",
    "        \"author\": \"荒川弘\",\n",
    "    },\n",
    "    \"一拳超人\": {\n",
    "        \"author\": \"ONE\",\n",
    "    },\n",
    "    \"ONE_PIECE\": {\n",
    "        \"author\": \"尾田榮一郎\",\n",
    "    },\n",
    "    \"東京喰種\": {\n",
    "        \"author\": \"石田翠\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7651dee5-eda2-4990-bab2-79a9862ec40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia(user_agent='AI Tutorial(mengchiehling@gmail.com)', language='zh-tw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892fa8b1-6410-4d56-bd1b-63347fd6fcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "for title in wiki_titles:\n",
    "    page = wiki_wiki.page(title)\n",
    "    wiki_text = page.text\n",
    "\n",
    "    data_path = Path(\"data\")\n",
    "    if not data_path.exists():\n",
    "        Path.mkdir(data_path)\n",
    "\n",
    "    with open(data_path / f\"{title}.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "        fp.write(wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f3966f-b77e-4151-8887-ca1e72192bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "docs = SimpleDirectoryReader(input_files=[f for f in glob(\"data/*.txt\")]).load_data()\n",
    "\n",
    "for doc in docs:\n",
    "    comic_name = Path(doc.metadata['file_name']).stem\n",
    "    doc.metadata.update(wiki_metadatas[comic_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f45bbcf-d7c0-4c32-a20b-6671b44d87e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7106eed-b7e1-4380-99db-b880d83b77b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "\n",
    "from initialization import credential_init\n",
    "\n",
    "credential_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbee9e2-7c23-4c83-9c37-a642af5b7b3a",
   "metadata": {},
   "source": [
    "建立nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a9dc78-571b-43f7-a3d5-504d3c954d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=128,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"。\", \"，\", \" \"]\n",
    ").split_text\n",
    "\n",
    "semantic_splitter_node_parser = SemanticSplitterNodeParser.from_defaults(\n",
    "    embed_model = embed_model,\n",
    "    sentence_splitter=text_splitter,\n",
    "    include_metadata=True,\n",
    "    include_prev_next_rel=True,\n",
    "    breakpoint_percentile_threshold=60,\n",
    ")\n",
    "\n",
    "raw_nodes = semantic_splitter_node_parser.get_nodes_from_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7af101-733e-4e17-86e4-0402648020cb",
   "metadata": {},
   "source": [
    "## FAISS + VectorStoreIndex Persist\n",
    "\n",
    "- 建立 embedding\n",
    "- 建立 vectorstore\n",
    "- 建立 storage context\n",
    "- 建立 vectorstoreindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7d0af-d2a2-4626-b1fe-e74082243efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "\n",
    "persist_dir = \"./week_2/storage_faiss\"\n",
    "\n",
    "# Overwrite logic: remove the folder if it exists\n",
    "if os.path.exists(persist_dir):\n",
    "    shutil.rmtree(persist_dir)\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "d = 1024 # 必須與 embedding model 的輸出維度一致\n",
    "\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e53f24d-cacc-4898-a0eb-847cc1b94622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ollama_connection import llama_index_ollama\n",
    "\n",
    "ollama_llm = llama_index_ollama(model=\"gpt-oss:120b-cloud\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae0d3c-4cbb-4ce3-b5e2-eb7599ccd677",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_index = VectorStoreIndex(\n",
    "    raw_nodes,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f3a41-f924-45cd-9167-f1f54d2efa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_query_engine = faiss_index.as_query_engine(similarity_top_k=5,\n",
    "                                                 response_mode=\"compact\",\n",
    "                                                 llm=ollama_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22fd6fa-1f1b-4745-95f0-a3c3a6a58b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = faiss_query_engine.query(\"誰是最強的光頭?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682d6b1e-1d29-4d5c-9f45-31916c385271",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57232f2e-f496-46af-87dc-78e736ed7704",
   "metadata": {},
   "source": [
    "儲存 Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad455948-2b4f-442a-a7c5-b3aab4d8fc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context.persist(persist_dir=\"./week_2/storage_faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa8e955-ea02-44ec-af2e-b302417b59cf",
   "metadata": {},
   "source": [
    "讀取 Storage 重建 Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf4a7f-13b0-4200-9347-f1756a5a7f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# 【步驟 A】從本地路徑還原向量存儲物件\n",
    "# 注意：不要手動建立空的 faiss.IndexFlatL2，直接從 persist_dir 讀取\n",
    "vector_store = FaissVectorStore.from_persist_dir(\"./week_2/storage_faiss\")\n",
    "\n",
    "# 【步驟 B】構建存儲上下文\n",
    "storage_context_loaded = StorageContext.from_defaults(\n",
    "    vector_store=vector_store,\n",
    "    persist_dir=\"./week_2/storage_faiss\"\n",
    ")\n",
    "\n",
    "# 【步驟 C】載入索引並建立查詢引擎\n",
    "faiss_vector_index_loaded = load_index_from_storage(\n",
    "    storage_context_loaded,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "faiss_query_engine_loaded = faiss_vector_index_loaded.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    embed_model=embed_model,\n",
    "    llm=ollama_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad60c0b1-3a2a-4a7f-a153-b3f7364593e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = faiss_query_engine.query(\"誰是最強的光頭?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c6e127-d2ea-4e1e-949a-9d321edd8aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b8d59a-3fff-490f-b439-de4df3bb7144",
   "metadata": {},
   "source": [
    "### 更新 Storage Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654af0d5-30f3-4bd7-8f08-3f207c4e78c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"银之匙_Silver_Spoon\"\n",
    "\n",
    "page = wiki_wiki.page(title)\n",
    "wiki_text = page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931bc9c0-7c4d-45b6-9f86-3352fd63ed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "new_doc = Document(text=wiki_text, metadata={\"author\": \"荒川弘\"})\n",
    "\n",
    "new_nodes = semantic_splitter_node_parser.get_nodes_from_documents([new_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0d3150-7c5d-4b2a-9466-3423ce0f81a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確保 nodes 已經被向量化\n",
    "for node in new_nodes:\n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )\n",
    "    node.embedding = node_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbde13a-93df-4af8-812c-e6898aacf92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_nodes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9d4f7e-806c-4d4b-ae53-c6d213cdb04e",
   "metadata": {},
   "source": [
    "更新 vectorstore index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286d7198-ebb6-4233-8deb-d9f551289170",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_vector_index_loaded.insert_nodes(new_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a906a54-3d72-4a0a-b0fa-fd5444d3926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_query_engine_updated = faiss_vector_index_loaded.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    embed_model=embed_model,\n",
    "    llm=ollama_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1b9879-1ca2-40a9-b7b0-1da58b912c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = faiss_query_engine_updated.query(\"銀之匙的創作背景\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb130df3-2186-49f9-8c43-2c061faad848",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa967350-7273-4020-8561-62c5eeb2c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_vector_index_loaded.storage_context.persist(persist_dir=\"./week_2/storage_faiss_new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99d7d4e-3b1b-4097-a0a2-16854481e5e0",
   "metadata": {},
   "source": [
    "實驗是否可以正常執行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c2a2f-c30c-444a-bd34-178953da21de",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FaissVectorStore.from_persist_dir(\"./week_2/storage_faiss_new\")\n",
    "\n",
    "storage_context_loaded = StorageContext.from_defaults(\n",
    "    vector_store=vector_store,\n",
    "    persist_dir=\"./week_2/storage_faiss_new\"\n",
    ")\n",
    "\n",
    "faiss_vector_index_loaded = load_index_from_storage(\n",
    "    storage_context_loaded,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "faiss_query_engine_loaded = faiss_vector_index_loaded.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    embed_model=embed_model,\n",
    "    llm=ollama_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f61e4e-bdf5-4c38-948f-1e5452851af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = faiss_query_engine_loaded.query(\"銀之匙的創作背景\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b511009b-eece-46c4-b05f-cb7285a35f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008484c2-d0f8-4b8d-8c0d-149bb7754f12",
   "metadata": {},
   "source": [
    "### Shared Storage Context 與 Multi-Index 管理（進階）\n",
    "\n",
    "每次呼叫 VectorStoreIndex.from_documents，LlamaIndex 都會：\n",
    "\n",
    "- 先建立一個臨時 UUID index\n",
    "- 再讓你手動指定 index_id\n",
    "\n",
    "如果沒有清理，就會：\n",
    "\n",
    "- 多存一個「沒人用的 index」\n",
    "\n",
    "解法核心觀念\n",
    "\n",
    "- StorageContext 是 索引一致性的邊界\n",
    "- index_id 必須同步更新到 index_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61799a18-1f0d-4146-8a4d-e1566d924881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "persist_dir = \"./week_2/storage_shared\"\n",
    "\n",
    "# Overwrite logic: remove the folder if it exists\n",
    "if os.path.exists(persist_dir):\n",
    "    shutil.rmtree(persist_dir)\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "d = 1024 # 必須與 embedding model 的輸出維度一致\n",
    "\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store,\n",
    ")\n",
    "\n",
    "# add documents to index\n",
    "docs = SimpleDirectoryReader(input_files=[f for f in glob(\"week_2/data/*.txt\")]).load_data()\n",
    "\n",
    "for doc in docs:\n",
    "    comic_name = Path(doc.metadata['file_name']).stem\n",
    "    doc.metadata.update(wiki_metadatas[comic_name])\n",
    "\n",
    "    nodes = semantic_splitter_node_parser.get_nodes_from_documents([doc])\n",
    "    \n",
    "    index = VectorStoreIndex(\n",
    "        nodes,\n",
    "        storage_context=storage_context,\n",
    "        embed_model=embed_model,\n",
    "    )\n",
    "    # 2. To strictly clean up the auto-generated one:\n",
    "    old_id = index.index_id\n",
    "    print(old_id)\n",
    "    # Manually set the index_id on the index_struct\n",
    "    index.set_index_id(comic_name)\n",
    "    # index.index_struct.index_id = wiki_title\n",
    "    \n",
    "    # Sync the index_id back to the storage context's index store\n",
    "    storage_context.index_store.add_index_struct(index.index_struct)\n",
    "\n",
    "    # FIX: Use a try-except or check against the dictionary keys correctly\n",
    "    # Usually, storage_context.index_store.index_structs is the internal dict\n",
    "    try:\n",
    "        storage_context.index_store.delete_index_struct(old_id)\n",
    "    except Exception:\n",
    "        # If it's already gone or doesn't exist, just move on\n",
    "        pass\n",
    "\n",
    "# Persist ONCE after the loop (more efficient than inside the loop)\n",
    "storage_context.persist(persist_dir=persist_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0d55ee-eaa2-4ad2-a99a-d0b9ee877b5d",
   "metadata": {},
   "source": [
    "讀取指定index_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe8f69a-d8fe-457b-a9cb-d1c80921e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_loaded = FaissVectorStore.from_persist_dir(\"./week_2/storage_shared\")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store_loaded,\n",
    "    persist_dir=\"./week_2/storage_shared\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deb961b-1ba4-4b16-b22e-c202d84f98bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index_tokyo_ghoul = load_index_from_storage(\n",
    "    storage_context,\n",
    "    index_id = \"東京喰種\",\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "query_engine_tokyo_ghoul = vector_index_tokyo_ghoul.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    llm=ollama_llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186fbf5c-a556-44a5-a35c-ac308f8acc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_engine_tokyo_ghoul.query(\"解釋赫子\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1500b9c0-9c32-449e-952b-6d1303d777b9",
   "metadata": {},
   "source": [
    "### 加入新的index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8af51d-8534-40eb-96f5-fe5f21ea728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage_context.index_store.index_structs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592add2d-c9b9-46b7-a0a4-0e9df4dcd51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# 建立新的index\n",
    "new_index = VectorStoreIndex(\n",
    "    nodes=new_nodes,               # 你要加入的新資料\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "\n",
    "old_id = new_index.index_id\n",
    "new_index.set_index_id(\"銀之匙\")\n",
    "\n",
    "# Sync the index_id back to the storage context's index store\n",
    "storage_context.index_store.add_index_struct(index.index_struct)\n",
    "\n",
    "storage_context.index_store.delete_index_struct(old_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2ffc57-88b4-4b61-8a67-a08e9560c6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context.persist(persist_dir=\"./week_2/storage_shared_new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdbf830-c2da-4244-a0c8-02d35b5c8c02",
   "metadata": {},
   "source": [
    "### 更新舊的index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5644e025-ea5b-4e77-a3c1-bf4d2307cda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"石田翠\"\n",
    "\n",
    "page = wiki_wiki.page(title)\n",
    "wiki_text = page.text\n",
    "\n",
    "new_doc = Document(text=wiki_text)\n",
    "\n",
    "tokyo_ghoul_new_nodes = semantic_splitter_node_parser.get_nodes_from_documents([new_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607dd5c5-7f0c-4ed2-8332-8f0f1a6a6a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確保 nodes 已經被向量化\n",
    "for node in tokyo_ghoul_new_nodes:\n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )\n",
    "    node.embedding = node_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b3c2b5-e1f2-48b6-9599-0fc85bb2b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index_tokyo_ghoul = load_index_from_storage(\n",
    "    storage_context,\n",
    "    index_id = \"東京喰種\",\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2229e9d8-dbed-4b0d-a55a-03c8fe5d6f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index_tokyo_ghoul.insert_nodes(tokyo_ghoul_new_nodes)\n",
    "storage_context.persist(persist_dir=\"./week_2/storage_shared_tokyo_ghoul_new\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ef8c7f-7a21-4a5e-ab3a-4a8164563ca6",
   "metadata": {},
   "source": [
    "檢查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a73f064-49f8-42d8-86d9-f21cd89d66e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_loaded = FaissVectorStore.from_persist_dir(\"./week_2/storage_shared_tokyo_ghoul_new\")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store_loaded,\n",
    "    persist_dir=\"./week_2/storage_shared_tokyo_ghoul_new\",\n",
    ")\n",
    "\n",
    "vector_index_tokyo_ghoul = load_index_from_storage(\n",
    "    storage_context,\n",
    "    index_id = \"東京喰種\",\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "query_engine_tokyo_ghoul = vector_index_tokyo_ghoul.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    llm=ollama_llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b57fa-9287-43bc-abb2-f1c2e951a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_engine_tokyo_ghoul.query(\"東京喰種作者的人生經歷\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08314d20-6e84-49d6-9f15-d3762b50f29e",
   "metadata": {},
   "source": [
    "## Qdrant + VectorStoreIndex Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b85660-6e8c-490b-be3f-1947002aaa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "await client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d26d9b-b41b-4e59-9107-dc7ca488347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import AsyncQdrantClient\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "\"\"\"\n",
    "await client.close()\n",
    "\"\"\"\n",
    "\n",
    "aclient = AsyncQdrantClient(path=\"week_2/langchain_qdrant\")\n",
    "\n",
    "qdrant_vector_store = QdrantVectorStore(\n",
    "    aclient=aclient,\n",
    "    collection_name=\"my_document\" # This collection will hold your vectors and associated documents.\n",
    ")\n",
    "\n",
    "persist_dir = \"./week_2/storage_qdrant\"\n",
    "\n",
    "# Overwrite logic: remove the folder if it exists\n",
    "if os.path.exists(persist_dir):\n",
    "    shutil.rmtree(persist_dir)\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=qdrant_vector_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a11f5b-93b4-442b-b2e2-58f6077e71c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_index = VectorStoreIndex(\n",
    "    raw_nodes,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    "    llm=ollama_llm,\n",
    "    use_async=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6187a4da-8834-41f9-82ec-6502d4e7fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context.persist(persist_dir=\"./week_2/storage_qdrant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebcd843-eb31-4ba3-b5ec-04ce1fc6d042",
   "metadata": {},
   "source": [
    "測試還原 qdrant_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e73237-ef9f-44b3-858b-fd10b01abf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "await aclient.close()\n",
    "\n",
    "aclient = AsyncQdrantClient(path=\"week_2/langchain_qdrant\")\n",
    "\n",
    "qdrant_vector_store = QdrantVectorStore(\n",
    "    aclient=aclient,\n",
    "    collection_name=\"my_document\" # This collection will hold your vectors and associated documents.\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=\"./week_2/storage_shared_qdrant\",\n",
    "    vector_store=qdrant_vector_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a633596-c27d-4e47-958b-bd819ef25df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 重建 index\n",
    "qdrant_vector_index = load_index_from_storage(\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    "    llm=ollama_llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06229e35-d6c9-4bbd-aeb5-65254fbcf61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_query_engine = qdrant_index.as_query_engine(similarity_top_k=5,\n",
    "                                                   llm=ollama_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9488d76f-7740-4dab-a5f2-f6764696854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# await qdrant_query_engine.aquery(\"海賊王最新劇情\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d5e030-7b5a-4b65-be54-946509b7ef15",
   "metadata": {},
   "source": [
    "### 更新Storage Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060604bf-52a8-4d60-8023-0300ab5eb3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"银之匙_Silver_Spoon\"\n",
    "\n",
    "page = wiki_wiki.page(title)\n",
    "wiki_text = page.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df9e372-1f7a-46f2-8ff4-2a17f533d0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = Document(text=wiki_text, metadata={\"author\": \"荒川弘\"})\n",
    "\n",
    "new_nodes = semantic_splitter_node_parser.get_nodes_from_documents([new_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f84be74-729f-4a25-96d6-931c021aac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確保 nodes 已經被向量化\n",
    "for node in new_nodes:\n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )\n",
    "    node.embedding = node_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab10cb0-e618-4a31-ae8f-07fd5d085578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# async version\n",
    "await qdrant_index.ainsert_nodes(new_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52c87fe-c156-4c8c-9ff3-114f0b823e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_query_engine = qdrant_index.as_query_engine(similarity_top_k=10,\n",
    "                                                   llm=ollama_llm)\n",
    "\n",
    "output = await qdrant_query_engine.aquery(\"銀之匙的創作背景\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480bc1ec-ca41-405d-a133-b014f1c0d89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6715a41-b36d-4cd9-b08f-d5ef0c716fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6078a0d-7db2-4d06-8d90-1f5ceb695c57",
   "metadata": {},
   "source": [
    "測試Storage Context是否被成功persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68440332-ee9a-403e-ab21-bbec768a2370",
   "metadata": {},
   "outputs": [],
   "source": [
    "await aclient.close()\n",
    "\n",
    "aclient = AsyncQdrantClient(path=\"week_2/langchain_qdrant\")\n",
    "\n",
    "qdrant_vector_store = QdrantVectorStore(\n",
    "    aclient=aclient,\n",
    "    collection_name=\"my_document\" # This collection will hold your vectors and associated documents.\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=\"./week_2/storage_qdrant\",\n",
    "    vector_store=qdrant_vector_store,\n",
    ")\n",
    "\n",
    "qdrant_vector_index = load_index_from_storage(\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    "    llm=ollama_llm,\n",
    ")\n",
    "\n",
    "qdrant_query_engine = qdrant_vector_index.as_query_engine(similarity_top_k=10,\n",
    "                                                   llm=ollama_llm)\n",
    "\n",
    "output = await qdrant_query_engine.aquery(\"銀之匙的創作背景\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf7a38d-ed58-45dd-bb49-0d1fa55a81b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24ba3c-db9d-4b53-b451-db0b140a7fc0",
   "metadata": {},
   "source": [
    "### Shared Storage Context 與 Multi-Index 管理（進階）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad36e25-6e2a-4652-9b6a-17ee387d156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_dir = \"./week_2/storage_qdrant_shared\"\n",
    "\n",
    "# Overwrite logic: remove the folder if it exists\n",
    "if os.path.exists(persist_dir):\n",
    "    shutil.rmtree(persist_dir)\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "await aclient.close()\n",
    "\n",
    "aclient = AsyncQdrantClient(path=\"week_2/langchain_qdrant_shared\")\n",
    "\n",
    "qdrant_vector_store = QdrantVectorStore(\n",
    "    aclient=aclient,\n",
    "    collection_name=\"my_document\" # This collection will hold your vectors and associated documents.\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=qdrant_vector_store,\n",
    ")\n",
    "\n",
    "# add documents to index\n",
    "docs = SimpleDirectoryReader(input_files=[f for f in glob(\"week_2/data/*.txt\")]).load_data()\n",
    "\n",
    "for doc in docs:\n",
    "    comic_name = Path(doc.metadata['file_name']).stem\n",
    "    doc.metadata.update(wiki_metadatas[comic_name])\n",
    "\n",
    "    nodes = semantic_splitter_node_parser.get_nodes_from_documents([doc])\n",
    "    \n",
    "    index = VectorStoreIndex(\n",
    "        nodes,\n",
    "        storage_context=storage_context,\n",
    "        embed_model=embed_model,\n",
    "        use_async=True\n",
    "    )\n",
    "\n",
    "    old_id = index.index_id\n",
    "    print(old_id)\n",
    "    index.set_index_id(comic_name)\n",
    "\n",
    "    storage_context.index_store.add_index_struct(index.index_struct)\n",
    "\n",
    "    try:\n",
    "        storage_context.index_store.delete_index_struct(old_id)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Persist ONCE after the loop (more efficient than inside the loop)\n",
    "storage_context.persist(persist_dir=persist_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7a36f3-876b-4c53-bcec-6335798a90f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "await aclient.close()\n",
    "\n",
    "aclient = AsyncQdrantClient(path=\"week_2/langchain_qdrant_shared\")\n",
    "\n",
    "qdrant_vector_store = QdrantVectorStore(\n",
    "    aclient=aclient,\n",
    "    collection_name=\"my_document\" # This collection will hold your vectors and associated documents.\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=\"./week_2/storage_qdrant_shared\",\n",
    "    vector_store=qdrant_vector_store,\n",
    ")\n",
    "\n",
    "vector_index_tokyo_ghoul = load_index_from_storage(\n",
    "    storage_context,\n",
    "    index_id = \"東京喰種\",\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "query_engine_tokyo_ghoul = vector_index_tokyo_ghoul.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    llm=ollama_llm,\n",
    ")\n",
    "\n",
    "output = await query_engine_tokyo_ghoul.aquery(\"東京喰種作者的人生經歷\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d608af-c9a3-4582-bae0-476a43f6910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc02b83-50b2-47f0-880f-52592b430c20",
   "metadata": {},
   "source": [
    "#### 加入新的index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20d984f-646d-4a9b-a3e5-2cf2ed88ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立新的index\n",
    "new_index = VectorStoreIndex(\n",
    "    nodes=new_nodes,               # 你要加入的新資料\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    "    use_async=True\n",
    ")\n",
    "\n",
    "old_id = new_index.index_id\n",
    "new_index.set_index_id(\"銀之匙\")\n",
    "\n",
    "# Sync the index_id back to the storage context's index store\n",
    "storage_context.index_store.add_index_struct(new_index.index_struct)\n",
    "\n",
    "storage_context.index_store.delete_index_struct(old_id)\n",
    "\n",
    "storage_context.persist(persist_dir=\"./week_2/storage_qdrant_shared_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46880427-bb08-48b5-94be-b75a248e238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "await aclient.close()\n",
    "\n",
    "aclient = AsyncQdrantClient(path=\"week_2/langchain_qdrant_shared\")\n",
    "\n",
    "qdrant_vector_store = QdrantVectorStore(\n",
    "    aclient=aclient,\n",
    "    collection_name=\"my_document\" # This collection will hold your vectors and associated documents.\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=\"./week_2/storage_qdrant_shared_new\",\n",
    "    vector_store=qdrant_vector_store,\n",
    ")\n",
    "\n",
    "vector_index_tokyo_ghoul = load_index_from_storage(\n",
    "    storage_context,\n",
    "    index_id = \"銀之匙\",\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "query_engine_tokyo_ghoul = vector_index_tokyo_ghoul.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    llm=ollama_llm,\n",
    ")\n",
    "\n",
    "output = await query_engine_tokyo_ghoul.aquery(\"銀之匙的創作背景\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a271d9ee-0279-47ce-af3e-08b56fce6f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a9fe1-3ae6-4a60-beb0-668a9e656df9",
   "metadata": {},
   "source": [
    "#### 更新Index\n",
    "\n",
    "就留作練習吧~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbb87b2-71e8-4641-a2ee-05dee1640701",
   "metadata": {},
   "source": [
    "# 進階檢索策略 - Part 1\n",
    "\n",
    "## Small-to-Big Retrieval（Sentence Window）\n",
    "\n",
    "適合情境：\n",
    "\n",
    "- 長文件\n",
    "- 查詢精細事實\n",
    "\n",
    "作法：\n",
    "\n",
    "- Index 用單句 embedding\n",
    "- 回傳時補上上下文 window (`MetadataReplacementPostProcessor`)\n",
    "\n",
    "![caption](Gemini_Generated_Image_f4triqf4triqf4tr.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e5d8fe-7816-486a-9a11-63434ab4cecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "wiki_metadatas = {\n",
    "    \"鋼之鍊金術師\": {\n",
    "        \"author\": \"荒川弘\",\n",
    "    },\n",
    "    \"一拳超人\": {\n",
    "        \"author\": \"ONE\",\n",
    "    },\n",
    "    \"ONE_PIECE\": {\n",
    "        \"author\": \"尾田榮一郎\",\n",
    "    },\n",
    "    \"東京喰種\": {\n",
    "        \"author\": \"石田翠\",\n",
    "    },\n",
    "}\n",
    "\n",
    "docs = SimpleDirectoryReader(input_files=[f for f in glob(\"data/*.txt\")]).load_data()\n",
    "\n",
    "for doc in docs:\n",
    "    comic_name = Path(doc.metadata['file_name']).stem\n",
    "    doc.metadata.update(wiki_metadatas[comic_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d43623-bfbe-4c28-b30f-1c202bab38f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser.text.sentence_window import SentenceWindowNodeParser\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "\n",
    "sw_node_parser = SentenceWindowNodeParser(\n",
    "    # How many sentences on both sides to capture. \n",
    "    # Setting this to 3 results in 7 sentences.\n",
    "    window_size=3, # # 前後各 3 句，共 7 句\n",
    "    # the metadata key for to be used in MetadataReplacementPostProcessor\n",
    "    window_metadata_key=\"window\",\n",
    "    # the metadata key that holds the original sentence\n",
    "    original_text_metadata_key=\"original_sentence\"\n",
    ")\n",
    "\n",
    "sw_nodes = sw_node_parser.get_nodes_from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d279ad5a-04c8-486a-803d-146baaf93545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280e22ab-a366-4ad3-a9dc-bfd164a595aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "from src.ollama_connection import llama_index_ollama\n",
    "from src.storage_context_init import init_faiss_storage_context\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "sw_storage_context = init_faiss_storage_context(d=1024)\n",
    "ollama_llm = llama_index_ollama(model=\"gpt-oss:120b-cloud\", temperature=0)\n",
    "\n",
    "sw_index = VectorStoreIndex(\n",
    "    sw_nodes,\n",
    "    storage_context=sw_storage_context,\n",
    "    embed_model=embed_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92886e5-71a2-4bc0-8173-1b90a9ab94a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_query_engine = sw_index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    # the target key defaults to `window` to match the node_parser's default\n",
    "    node_postprocessors=[\n",
    "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    ],\n",
    "    llm = ollama_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4429b4d4-6ef2-483f-93f3-905056f71984",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sw_query_engine.query(\n",
    "    \"最強的光頭是誰?\"\n",
    ")\n",
    "print(output.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ec5432-1df8-487b-a5ff-fe66b0d0fca0",
   "metadata": {},
   "source": [
    "## Auto-Retrieval\n",
    "\n",
    "當資料規模變大時：\n",
    "\n",
    "- 單純靠相似度搜尋 → 雜訊急遽增加\n",
    "- Metadata 可以先縮小搜尋子空間\n",
    "\n",
    "VectorIndexAutoRetriever 的核心思想是：\n",
    "\n",
    "User Query\n",
    "→ LLM 推斷可能的 metadata 條件\n",
    "→ 套用 filters 進行向量搜尋\n",
    "→ 回傳更精準的節點\n",
    "\n",
    "這讓 Retrieval 從「被動比對」進化成「語意驅動的檢索策略」。\n",
    "\n",
    "** 無法搭配 FAISS **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de39167a-342a-44f8-9b9a-74be3714d604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexAutoRetriever\n",
    "from llama_index.core.vector_stores.types import MetadataInfo, VectorStoreInfo\n",
    "\n",
    "\n",
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"作者個人資料\",\n",
    "    metadata_info=[\n",
    "        MetadataInfo(\n",
    "            name=\"author\",\n",
    "            type=\"str\",\n",
    "            description=(\n",
    "                \"漫畫作者\"\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4871853-39b4-4306-bfb7-d5752b0cd354",
   "metadata": {},
   "outputs": [],
   "source": [
    "await aclient.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0857483-ebae-4ed3-ad73-a7f5cb3aa177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import AsyncQdrantClient\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "aclient = AsyncQdrantClient(path=\"week_2/langchain_qdrant_autoretrieval\")\n",
    "\n",
    "qdrant_vector_store = QdrantVectorStore(\n",
    "    aclient=aclient,\n",
    "    collection_name=\"my_document\" # This collection will hold your vectors and associated documents.\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=qdrant_vector_store,\n",
    ")\n",
    "\n",
    "autoretrieval_index = VectorStoreIndex(\n",
    "    sw_nodes,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    "    use_async=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775a43bf-45a8-483c-a17e-60751a5a9292",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorIndexAutoRetriever(\n",
    "    autoretrieval_index,\n",
    "    vector_store_info=vector_store_info,\n",
    "    llm=ollama_llm,\n",
    "    max_top_k=1000,\n",
    "    similarity_top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3187a6d6-8d04-4ebc-acdf-a71ce1724732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# async模式\n",
    "filtered_nodes = await retriever.aretrieve(\n",
    "    \"尾田榮一郎的作品有哪些?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8393790f-bb60-48ae-8c0f-1d113dd653b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecaacbd-7e2c-4db4-9e35-bc21822727c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# 將你的 retriever 封裝進 Query Engine\n",
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=retriever,\n",
    "    llm=ollama_llm  # 建議再次傳入 llm 以確保合成答案時使用相同的模型\n",
    ")\n",
    "\n",
    "# 測試調用\n",
    "output = await query_engine.aquery(\"尾田榮一郎的作品有哪些?\")\n",
    "print(output.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00be6195-84fc-4b5c-bcef-8aab9b518ec8",
   "metadata": {},
   "source": [
    "## HierarchicalNodeParser\n",
    "\n",
    "此節點解析器會將節點切分成 **階層式節點（hierarchical nodes）**。  \n",
    "也就是說，單一輸入會被切分成多個不同大小的階層，每個節點都會包含對其 **父節點** 的參考。\n",
    "\n",
    "當與 `AutoMergingRetriever` 結合使用時，這可以讓我們在大多數子節點被檢索到時，自動將檢索到的節點替換為其父節點。  \n",
    "這個過程可以為 LLM 提供更完整的上下文，有助於生成更精確的回答。\n",
    "\n",
    "| 步驟 | 動作 | 說明 |\n",
    "|------|------|------|\n",
    "| 1 | 檢索 | 找尋 Leaf Nodes，系統在最細的層級（Leaf）尋找語義最相關的碎片。 |\n",
    "| 2 | 計數 | 檢查父節點覆蓋率，系統會檢查：「某個父節點下的子節點，是否被檢索到了超過 50%（或其他閾值）？」 |\n",
    "| 3 | 合併 | 自動向上合併，如果條件滿足，系統會捨棄這些散亂的小子節點，直接抓取整個父節點提供給 LLM。 |\n",
    "\n",
    "\n",
    "為什麼這很有用？\n",
    "- 避免「斷章取義」： 單獨一個 128 tokens 的句子可能遺失了前因後果。\n",
    "- 動態上下文： 如果只有一小句話相關，就給一小句（節省 Token）；如果一整段都相關，就給一整段（增加精確度）。\n",
    "- 範例場景： 如果你問「這家公司的退貨政策是什麼？」，系統檢索到了「退貨期限」、「退貨運費」、「退貨包裝」三個小子節點。與其把這三段碎碎念給 LLM，AutoMergingRetriever 會直接把整個完整的「退貨政策大章節」丟給它。\n",
    "\n",
    "#### 參考資料\n",
    "- [知乎專欄文章](https://zhuanlan.zhihu.com/p/678698001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce2bb5d-92f5-4ce8-9b6c-5a4b0032a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import HierarchicalNodeParser, get_leaf_nodes, get_root_nodes\n",
    "\n",
    "hierarchical_node_parser = HierarchicalNodeParser.from_defaults(\n",
    "    chunk_sizes=[2048, 512, 128]\n",
    ")\n",
    "\n",
    "hierarchical_nodes = hierarchical_node_parser.get_nodes_from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284ce822-552a-468a-b6b5-d7d799839185",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hierarchical_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d009387-e86f-4a1c-836d-1c5b1f1f3d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_nodes = get_leaf_nodes(hierarchical_nodes)\n",
    "root_nodes = get_root_nodes(hierarchical_nodes)\n",
    "\n",
    "print(f\"# of leaf nodes: {len(leaf_nodes)}\")\n",
    "print(f\"# of root nodes: {len(root_nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1584a4d-a134-444a-a587-c9c0b7ac5137",
   "metadata": {},
   "source": [
    "**為何總數是 990？**\n",
    "\n",
    "當你設定：\n",
    "\n",
    "```python\n",
    "chunk_sizes = [2048, 512, 128]\n",
    "```\n",
    "\n",
    "解析器會針對同一段文本生成三個層級的節點：\n",
    "\n",
    "- 第一層（Root）：最大區塊（2048 tokens）\n",
    "- 第二層（Mid）：將每個 2048 tokens 的區塊再細分為數個 512 tokens 的區塊\n",
    "- 第三層（Leaf）：將每個 512 tokens 的區塊再細分為數個 128 tokens 的區塊\n",
    "\n",
    "hierarchical_nodes 會包含這三個層級所有節點的總和，因此：\n",
    "\n",
    "$$\n",
    "    總節點數 (990) = Root 節點 + 中間層節點 + Leaf 節點\n",
    "$$\n",
    "\n",
    "\n",
    "**節點類型的定義**\n",
    "\n",
    "Root Nodes（根節點 - 32 個）\n",
    "\n",
    "- 代表文本的最上層結構（2048 tokens）\n",
    "- 意味著你的原始文件 docs 被切成 32 個大型區塊\n",
    "\n",
    "Leaf Nodes（葉節點 - 803 個）\n",
    "\n",
    "- 最底層、最小單位的區塊（128 tokens）\n",
    "- 在 Retrieval（檢索） 時通常會優先使用\n",
    "- 原因是語義最精確、最容易匹配使用者問題\n",
    "\n",
    "中間節點（512 tokens，未直接印出）\n",
    "\n",
    "- 在你的例子中也存在一層 512 tokens 的中間節點\n",
    "- 可由總數推算得出：\n",
    "\n",
    "**為什麼要這樣設計？**\n",
    "\n",
    "這種分層結構是為了實現 Auto-Merging Retrieval（自動合併檢索）。\n",
    "\n",
    "🎯 精準檢索（Fine-grained Retrieval）\n",
    "\n",
    "- 向量搜尋時使用最小的 Leaf Nodes（128 tokens）\n",
    "- 碎片化的資訊更容易命中使用者的查詢語義\n",
    "\n",
    "🧩 上下文還原（Context Reconstruction）\n",
    "\n",
    "- 當系統發現某個父節點（512 或 2048 tokens）下\n",
    "- 大多數子節點被選中\n",
    "- 系統會「向上合併」\n",
    "- 直接將整個父節點內容提供給 LLM\n",
    "\n",
    "⚖️ 解決精準度與上下文的矛盾\n",
    "\n",
    "- 小 chunk：提高搜尋精準度\n",
    "- 大 chunk：保留完整語意與上下文\n",
    "- Auto-Merging 讓兩者兼得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857ec52e-1f1a-43ed-8fec-a0a728d8958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "await aclient.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b1fb49-f19a-4fc7-8701-87a1fb920d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "aclient = AsyncQdrantClient(path=\"week_2/langchain_qdrant_hierarchical\")\n",
    "\n",
    "qdrant_vector_store = QdrantVectorStore(\n",
    "    aclient=aclient,\n",
    "    collection_name=\"my_document\" # This collection will hold your vectors and associated documents.\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=qdrant_vector_store,\n",
    ")\n",
    "\n",
    "storage_context.docstore.add_documents(hierarchical_nodes)\n",
    "\n",
    "leaf_index = VectorStoreIndex(\n",
    "    leaf_nodes,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    "    use_async=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4f62a0-b47d-4fda-be64-281c819a29d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**AutoMergingRetriever 為何能找回父節點？**\n",
    "\n",
    "在程式碼中，**雖然 `VectorStoreIndex` 只接收了 `leaf_nodes`**，  \n",
    "但 `AutoMergingRetriever` 卻能成功找回 **父節點（甚至根節點）**。\n",
    "\n",
    "這個「看似魔法」的能力，其實來自於：\n",
    "\n",
    "- `storage_context`\n",
    "- 節點本身的 `relationships`（血緣關係）\n",
    "\n",
    "以下我們用 **三個核心步驟** 拆解它的運作原理。\n",
    "\n",
    "---\n",
    "\n",
    "**1. 節點中的「血緣關係」（Relationships / Metadata)**\n",
    "\n",
    "當你執行：\n",
    "\n",
    "```python\n",
    "hierarchical_nodes = hierarchical_node_parser.get_nodes_from_documents(docs)\n",
    "```\n",
    "\n",
    "LlamaIndex 不只是切分文字，還在每個節點的 metadata 中注入了 relationships：\n",
    "\n",
    "- 每個 Leaf Node（葉節點）\n",
    "- 都會保存一個指向其 Parent Node（父節點） 的 parent_id\n",
    "\n",
    "Leaf Node A\n",
    " \n",
    " └── parent_id: \"Middle_Node_001\"\n",
    "\n",
    "Middle_Node_001\n",
    " \n",
    " └── parent_id: \"Root_Node_001\"\n",
    "\n",
    "**2. Docstore：節點的「保險箱」**\n",
    "\n",
    "storage_context 在整個流程中扮演了關鍵角色：\n",
    "```\n",
    "hierarchical_retriever = AutoMergingRetriever(\n",
    "    leaf_retriever,\n",
    "    storage_context,  # 關鍵就在這裡！\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "```\n",
    "為什麼 storage_context 這麼重要？\n",
    "\n",
    "- VectorStoreIndex\n",
    "    👉 只 將 leaf_nodes 的向量存入 Qdrant\n",
    "\n",
    "- storage_context.docstore\n",
    "    - 👉 保存所有節點物件\n",
    "    - Leaf Nodes\n",
    "    - Middle Nodes\n",
    "    - Root Nodes（即使它們沒有被向量化）\n",
    "\n",
    "📦 Docstore 就像一個完整的文件保險箱，裡面存放著所有層級的節點內容與關係。\n",
    "\n",
    "**3. AutoMergingRetriever 的自動合併流程**\n",
    "\n",
    "當你發起查詢時，AutoMergingRetriever 會依序執行以下步驟：\n",
    "\n",
    "- Step 1：檢索葉節點（Vector Search）\n",
    "\n",
    "    - leaf_retriever\n",
    "    - 從 Qdrant 中找出最相似的 N 個 Leaf Nodes（例如：6 個）\n",
    "---\n",
    "\n",
    "- Step 2：追溯父節點（Docstore Lookup）\n",
    "\n",
    "    - 讀取這些 Leaf Nodes 的 parent_id\n",
    "    - 前往 storage_context.docstore\n",
    "    - 取回對應的 父節點物件\n",
    "---\n",
    "- Step 3：檢查是否符合「合併條件」\n",
    "\n",
    "    - Retriever 會計算：\n",
    "\n",
    "      「這個父節點底下的子節點，有多少比例被檢索到了？」\n",
    "\n",
    "    - 如果符合門檻（例如：\n",
    "\n",
    "        - 超過一定比例\n",
    "\n",
    "        - 或至少多個子節點命中）\n",
    "\n",
    "    ➡️ 判定這些碎片資訊 足以代表整個區塊\n",
    "\n",
    "---\n",
    "\n",
    "- Step 4：替換節點（Auto Merge）\n",
    "\n",
    "    - 丟棄零碎的 Leaf Nodes\n",
    "    - 改為回傳 完整的父節點（甚至根節點）內容\n",
    "    - 將更有上下文的文字送給 LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b1fbb-71d9-4d03-90b0-ec2e76138f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "\n",
    "leaf_retriever = leaf_index.as_retriever(similarity_top_k=6, embed_model=embed_model)\n",
    "hierarchical_retriever = AutoMergingRetriever(leaf_retriever, storage_context, verbose=True)\n",
    "\n",
    "automerging_query_engine = RetrieverQueryEngine.from_args(hierarchical_retriever, llm=ollama_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8cd0b9-e66e-4b85-92fe-30846aa67f7a",
   "metadata": {},
   "source": [
    "## SummaryIndex\n",
    "\n",
    "在介紹接下來的 **RecursiveRetriever** 與 **IndexNode** 組合之前，先來說明一下 **SummaryIndex** 的概念與用途。\n",
    "\n",
    "**SummaryIndex** 的核心特性在於：  \n",
    "它會「一次性地消化所有 nodes」，將整份資料視為一個整體來處理，而不是逐條或逐層地進行檢索。\n",
    "\n",
    "你可以把它想像成 **泰倫蟲族（Tyranids）**——  \n",
    "不是慢慢啃食單一目標，而是直接吞噬整個生態系，並將其轉化為可用的生物質。  \n",
    "差別只在於，SummaryIndex 消化的不是血肉，而是「語意」。\n",
    "\n",
    "---\n",
    "\n",
    "### 運作方式概念\n",
    "\n",
    "SummaryIndex 的運作流程可以簡化為以下幾個步驟：\n",
    "\n",
    "1. **讀取所有 nodes**  \n",
    "   不論資料大小或層級，所有節點都會被納入處理範圍。\n",
    "\n",
    "2. **整合與壓縮語意**  \n",
    "   透過 LLM 將分散在各個 nodes 中的資訊進行整合，去除重複細節，保留關鍵概念。\n",
    "\n",
    "3. **產出高層次摘要**  \n",
    "   最終得到的是一個（或少數幾個）代表整體內容的摘要節點，而非可直接對應回原始細節的片段。\n",
    "\n",
    "這使得 SummaryIndex 本質上是一種 **「全局視角」的 Index**。\n",
    "\n",
    "---\n",
    "\n",
    "### 適合的使用場景\n",
    "\n",
    "SummaryIndex 特別適合回答以下類型的問題：\n",
    "\n",
    "- 這份文件「整體在講什麼」？\n",
    "- 這個系統或專案的「設計核心」是什麼？\n",
    "- 從全局來看，有哪些「關鍵概念與主軸」？\n",
    "\n",
    "它並不追求精準命中某一小段內容，而是提供對整體語意的理解。\n",
    "\n",
    "---\n",
    "\n",
    "### 與其他 Index 的差異\n",
    "\n",
    "將 SummaryIndex 與常見的 Vector-based Index 作一個對比：\n",
    "\n",
    "- **Vector Index**\n",
    "  - 著重於語意相似度搜尋\n",
    "  - 目標是找出「最相關的片段」\n",
    "  - 適合精細查詢與事實定位\n",
    "\n",
    "- **SummaryIndex**\n",
    "  - 著重於整體語意整合\n",
    "  - 目標是「用最少的內容代表全部」\n",
    "  - 適合總覽、摘要與高層理解\n",
    "\n",
    "換句話說，Vector Index 回答的是「哪一段最相關」，  \n",
    "而 SummaryIndex 回答的是「如果只能說一次，該怎麼說完全部內容」。\n",
    "\n",
    "---\n",
    "\n",
    "### 為什麼要先介紹 SummaryIndex？\n",
    "\n",
    "理解 SummaryIndex 這種「一次性消化」的策略，對後續理解  \n",
    "**RecursiveRetriever + IndexNode** 的設計非常關鍵。\n",
    "\n",
    "- SummaryIndex 提供的是 **自上而下的整體抽象**\n",
    "- RecursiveRetriever 負責 **逐層展開、逐步深入**\n",
    "- IndexNode 則用來 **串接不同層級的語意結構**\n",
    "\n",
    "你可以將 SummaryIndex 視為整個知識結構的「頂層總覽」，  \n",
    "而後續的組合設計，則是在這個總覽之下，逐步向細節層級鑽探。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740294ac-4d2f-4672-b41e-a0f59359a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from llama_index.core import SummaryIndex, get_response_synthesizer, Document\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia(user_agent='AI Tutorial(mengchiehling@gmail.com)', language='zh-tw')\n",
    "\n",
    "title = \"银之匙_Silver_Spoon\"\n",
    "\n",
    "page = wiki_wiki.page(title)\n",
    "wiki_text = page.text\n",
    "\n",
    "new_doc = Document(text=wiki_text, metadata={\"author\": \"荒川弘\"})\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=128,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"。\", \"，\", \" \"]\n",
    ").split_text\n",
    "\n",
    "semantic_splitter_node_parser = SemanticSplitterNodeParser.from_defaults(\n",
    "    embed_model = embed_model,\n",
    "    sentence_splitter=text_splitter,\n",
    "    include_metadata=True,\n",
    "    include_prev_next_rel=True,\n",
    "    breakpoint_percentile_threshold=60,\n",
    ")\n",
    "\n",
    "new_nodes = semantic_splitter_node_parser.get_nodes_from_documents([new_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58424299-7cd8-4278-99ff-0f40834d6c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_summary_index = SummaryIndex(new_nodes)\n",
    " = basic_summary_index.as_query_engine(llm=ollama_llm, response_mode='compact',\n",
    "                                       embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a93f58-5533-4450-b8cf-7a93d1ec7e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = await basic_summary_query_engine.aquery(\"給我一個銀之匙的快速簡介\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf03577-c4c8-485d-9ffc-6190fa915a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f07e510-d71c-4daf-b0a5-971715f6f45f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### response_mode\n",
    "\n",
    "直接在 Query Engine 設定（簡單）\n",
    "```\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=ollama_llm,\n",
    "    response_mode=\"tree_summarize\"\n",
    ")\n",
    "```\n",
    "\n",
    "使用 get_response_synthesizer（進階自定義\n",
    "\n",
    "為什麼要手動使用 get_response_synthesizer？\n",
    "如果你想更精細地控制「合成答案」的過程（而不僅僅是改個模式名），你可以手動建立它。這讓你能夠自定義：\n",
    "\n",
    "- Prompt Templates: 自定義如何提問。\n",
    "- Output Parsing: 格式化輸出。\n",
    "- Streaming: 是否開啟串流顯示。\n",
    "\n",
    "```\n",
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "# 1. 建立 synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    llm=ollama_llm,\n",
    "    response_mode=\"compact\",\n",
    "    streaming=True, # 額外的設定\n",
    "    # text_qa_template=my_custom_prompt # 甚至可以自定義 Prompt\n",
    ")\n",
    "\n",
    "# 2. 將其注入 query_engine\n",
    "query_engine = index.as_query_engine(\n",
    "    response_synthesizer=response_synthesizer\n",
    ")\n",
    "```\n",
    "\n",
    "| 模式名稱 | 運作邏輯 | 適用場景 |\n",
    "|---------|----------|----------|\n",
    "| refine（預設） | 逐一處理節點。先用第一個節點生成答案，再將該答案與下一個節點交給 LLM 進行「修正 / 精煉」。 | 想要最完整、最精確的答案，不計較 API 呼叫次數。 |\n",
    "| compact | 類似 refine，但在處理前會先將多個節點合併到同一個 Prompt 中，以節省 Token 和 API 呼叫次數。 | 推薦使用。平衡了效率與品質。 |\n",
    "| tree_summarize | 將節點分組並生成摘要，再對摘要進行摘要，最終形成一棵樹狀結構直到得出最後答案。 | 適合總結整份文件或處理跨多處的資訊。 |\n",
    "| simple_summarize | 直接將所有節點塞進一個 Prompt 中。如果節點太多超過 Context Window 會報錯。 | 節點數量少且希望快速得到答案時。 |\n",
    "| no_text | 僅返回檢索到的節點（Source Nodes），不呼叫 LLM 生成回答。 | 用於測試檢索準確度或僅需原始資料時。 |\n",
    "| accumulate | 對每個節點獨立生成一個答案，最後將所有答案拼接在一起返回。 | 需要針對不同段落分別給出回答時。 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6957a145-3def-46e1-8454-39963e116de8",
   "metadata": {},
   "source": [
    "## RecursiveRetriever 與 IndexNode\n",
    "\n",
    "隨著以 LLM 為核心的系統從小型、扁平的文件集合，擴展到更複雜的知識庫，單純依賴相似度搜尋的檢索方式已逐漸不足以應付實際需求。真實世界的資料往往具有 **階層性（hierarchical）**、**模組化（modular）** 與 **多層次粒度（multi-granular）** 的特性：書籍由章節與段落組成、知識庫依領域劃分，而摘要自然位於詳細內容之上。  \n",
    "LlamaIndex 正是透過 **RecursiveRetriever** 與 **IndexNode** 來解決這類問題。\n",
    "\n",
    "### IndexNode\n",
    "\n",
    "**IndexNode** 是一種特殊的節點（node），它不僅包含文字內容，還 **指向另一個索引（index）或檢索器（retriever）**。從概念上來看，它是一座連接不同抽象層級的「橋樑」。\n",
    "\n",
    "你可以將 IndexNode 想成：\n",
    "- 一段摘要、標籤或描述文字（例如：「第 2 章：檢索方法」）\n",
    "- 再加上一個指向更深層索引的參考\n",
    "\n",
    "這使得節點可以代表 **一整組節點的集合**，而不只是單一的文字片段。與其將所有低層級的內容一次性嵌入到一個龐大的索引中，不如先嵌入高層摘要，並由它們連結到更細節的索引。\n",
    "\n",
    "### RecursiveRetriever\n",
    "\n",
    "**RecursiveRetriever** 是一種可以同時處理一般節點與 IndexNode 的檢索器。當它檢索到一個 IndexNode 時，流程並不會就此停止，而是會 **遞迴地呼叫該 IndexNode 所指向的檢索器**，並在下一層繼續進行檢索。\n",
    "\n",
    "在高層次上，整個檢索流程如下：\n",
    "\n",
    "1. 先檢索高層節點（通常是摘要或分類）\n",
    "2. 在結果中辨識出 IndexNode\n",
    "3. 依照 IndexNode 的指向，進入更深層的檢索器或索引\n",
    "4. 檢索更細粒度的節點\n",
    "5. 重複上述流程，直到抵達最底層的節點\n",
    "\n",
    "這樣的設計讓檢索不再只是單次的平面搜尋，而是 **多階段、具階層性的檢索過程**。\n",
    "\n",
    "### 為什麼這很重要\n",
    "\n",
    "將 RecursiveRetriever 與 IndexNode 搭配使用，可以讓你：\n",
    "- 在不一次嵌入所有資料的情況下，擴展到大型語料庫\n",
    "- 保留文件原有的結構與階層關係\n",
    "- 實現「由粗到細」的檢索流程（由整體概覽逐步深入細節）\n",
    "- 僅在相關的子空間中搜尋，降低雜訊\n",
    "- 組合不同類型的索引（例如：SummaryIndex → VectorStoreIndex）\n",
    "\n",
    "### 心智模型（Mental model）\n",
    "\n",
    "- **IndexNode**：「這段文字代表一個集合，可以從這裡往下深入。」\n",
    "- **RecursiveRetriever**：「先檢索，再順著連結一路往下，直到找到合適的細節層級。」\n",
    "\n",
    "兩者結合，構成了 LlamaIndex 中 **結構化、階層式檢索管線** 的核心基礎，使你能打造更精準且高效率的 RAG 系統。\n",
    "\n",
    "假如你是一個鎚佬:\n",
    "\n",
    "| 技術概念               | 戰鎚 40K 比喻     |\n",
    "| ------------------ | ------------- |\n",
    "| RecursiveRetriever | 神聖泰拉 / 帝國行政中樞 |\n",
    "| Query（查詢）          | 帝皇或高領主的命令     |\n",
    "| IndexNode          | 登記在案的星球       |\n",
    "| Node 指向子 Index     | 星區 → 子星區 → 行星 |\n",
    "| 檢索子索引              | 徵調十一稅         |\n",
    "| 回傳結果               | 資源／軍團／數據送回泰拉  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba51ec-f049-41ee-a6b9-07bd85cbc756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import faiss\n",
    "from llama_index.core import SummaryIndex, SimpleDirectoryReader\n",
    "from llama_index.core.schema import IndexNode\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2742106-cd83-4e5f-9115-390f69f0a045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define top-level nodes and vector retrievers\n",
    "index_nodes = []\n",
    "vector_query_engines = {}\n",
    "vector_retrievers = {}\n",
    "\n",
    "wiki_metadatas = {\n",
    "    \"鋼之鍊金術師\": {\n",
    "        \"author\": \"荒川弘\",\n",
    "    },\n",
    "    \"一拳超人\": {\n",
    "        \"author\": \"ONE\",\n",
    "    },\n",
    "    \"ONE_PIECE\": {\n",
    "        \"author\": \"尾田榮一郎\",\n",
    "    },\n",
    "    \"東京喰種\": {\n",
    "        \"author\": \"石田翠\",\n",
    "    },\n",
    "}\n",
    "\n",
    "docs = SimpleDirectoryReader(input_files=[f for f in glob(\"week_2/data/*.txt\")]).load_data()\n",
    "\n",
    "d = 1024\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "\n",
    "sentence_splitter_node_parser = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "\n",
    "for doc in docs:\n",
    "    comic_name = Path(doc.metadata['file_name']).stem\n",
    "    doc.metadata.update(wiki_metadatas[comic_name])\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        vector_store=vector_store.model_copy(),\n",
    "    )\n",
    "    \n",
    "    # 建立index\n",
    "    nodes = sentence_splitter_node_parser.get_nodes_from_documents([doc])\n",
    "    vector_index = VectorStoreIndex(\n",
    "        nodes,\n",
    "        embed_model=embed_model,\n",
    "        storage_context=storage_context\n",
    "    )\n",
    "    \n",
    "    # define query engines\n",
    "    vector_query_engine = vector_index.as_query_engine(llm=ollama_llm, similarity_top_k=5)\n",
    "    vector_query_engines[comic_name] = vector_query_engine\n",
    "    vector_retrievers[comic_name] = vector_index.as_retriever()\n",
    "\n",
    "    # save summaries\n",
    "    out_path = Path(\"week_2/summaries\") / f\"{comic_name}.txt\"\n",
    "    if not out_path.exists():\n",
    "        # use LLM-generated summary\n",
    "        summary_index = SummaryIndex(nodes)\n",
    "\n",
    "        summarizer = summary_index.as_query_engine(\n",
    "            response_mode=\"compact\", llm=ollama_llm\n",
    "        )\n",
    "        response = await summarizer.aquery(\n",
    "            f\"請給我{comic_name}的總結\"\n",
    "        )\n",
    "\n",
    "        wiki_summary = response.response\n",
    "        Path(\"week_2/summaries\").mkdir(exist_ok=True)\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as fp:\n",
    "            fp.write(wiki_summary)\n",
    "    else:\n",
    "        with open(out_path, \"r\", encoding=\"utf-8\") as fp:\n",
    "            wiki_summary = fp.read()\n",
    "\n",
    "    print(f\"**Summary for {comic_name}: {wiki_summary}\")\n",
    "    node = IndexNode(text=wiki_summary, index_id=comic_name)\n",
    "    index_nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c845cc9-7789-4c79-80ee-01355bc6ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_vector_index = VectorStoreIndex(\n",
    "    index_nodes, embed_model=embed_model\n",
    ")\n",
    "\n",
    "top_vector_retriever = top_vector_index.as_retriever(similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7822c5-b83e-4727-9d30-76bf52d94046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import RecursiveRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "recursive_retriever = RecursiveRetriever(\n",
    "    \"vector\",\n",
    "    retriever_dict={\"vector\": top_vector_retriever, **vector_retrievers},\n",
    "    # query_engine_dict=vector_query_engines,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8cd958-2056-4db3-a8f3-6ccaedb1842b",
   "metadata": {},
   "source": [
    "| 參數                    | 說明                                                                                         | 範例                                                                              |\n",
    "| --------------------- | ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------- |\n",
    "| **root_id**           | 字串，指定最上層（root）的 retriever ID                                                               | `\"vector\"` → 就是 `retriever_dict[\"vector\"]`                                      |\n",
    "| **retriever_dict**    | 一個字典，key 是 retriever ID，value 是實際的 Retriever。<br>用來定義每個節點對應的 retriever                     | `{ \"vector\": top_vector_retriever, \"鋼之鍊金術師\": steel_alchemist_retriever, ... }`  |\n",
    "| **query_engine_dict** | 可選。指定每個 retriever 對應的 **QueryEngine**（有時包含 LLM 回答能力）。<br>如果不提供，Retriever 只會做「檢索」，不會直接生成回答。 | `{ \"vector\": top_vector_query_engine, \"鋼之鍊金術師\": steel_alchemist_query_engine }` |\n",
    "| **verbose**           | 是否在檢索/路徑選擇時輸出 debug 訊息                                                                     | `True/False`                                                                    |\n",
    "\n",
    "🔹 為什麼 `root_id` 是 `\"vector\"`？\n",
    "\n",
    "`RecursiveRetriever` 需要知道「最上層從哪裡開始」：\n",
    "\n",
    "- `\"vector\"` 對應到 `retriever_dict[\"vector\"]`，也就是你的 **summary index**  \n",
    "- Retriever 會先在 summary index 找出最相關的節點，再根據 recursive mapping 去下層檢索 full-text\n",
    "\n",
    "---\n",
    "\n",
    "🔹 為什麼要 `retriever_dict`？\n",
    "\n",
    "這就是「兩層結構的核心」：\n",
    "\n",
    "- key → retriever 的 ID  \n",
    "- value → retriever 物件（可以是 VectorRetriever、BM25Retriever 等）\n",
    "\n",
    "> 這樣 RecursiveRetriever 才知道：當 top 層選到 `\"一拳超人\"` 時，要往哪個下層 retriever 查。\n",
    "\n",
    "---\n",
    "\n",
    "🔹 `query_engine_dict` 的作用\n",
    "\n",
    "1. **Retriever 只做檢索**  \n",
    "   - 會回傳節點或 chunk  \n",
    "   - 沒有 LLM 生成回答能力  \n",
    "\n",
    "2. **QueryEngine 結合 Retriever + LLM**  \n",
    "   - 可以直接用 LLM 生成回答或摘要  \n",
    "   - RecursiveRetriever 如果有 `query_engine_dict`，就可以直接呼叫 QueryEngine 做「最終回答」而不是只回 chunk\n",
    "\n",
    "> 簡單比喻：\n",
    "> - `retriever_dict` = 找書的書架  \n",
    "> - `query_engine_dict` = 書架 + 讀書的人 → 可以直接講故事給你聽\n",
    "\n",
    "---\n",
    "\n",
    "🔹 何時需要 `query_engine_dict`？\n",
    "\n",
    "- 你希望 recursive retriever **每一層都用 LLM 生成回答**  \n",
    "- 如果只是純向量檢索，`query_engine_dict` 可以不用  \n",
    "\n",
    "例子：\n",
    "\n",
    "```python\n",
    "recursive_retriever = RecursiveRetriever(\n",
    "    root_id=\"vector\",\n",
    "    retriever_dict={\"vector\": top_vector_retriever, **vector_retrievers},\n",
    "    query_engine_dict=vector_query_engines,  # 每個 retriever 對應 QueryEngine\n",
    "    verbose=True,\n",
    ")\n",
    "```\n",
    "- 這樣 recursive_retriever.get_relevant_documents(query) 會自動：\n",
    "    1. Top 層用 summary 找到對應漫畫\n",
    "    2. 下層 retriever 找 chunk\n",
    "    3. QueryEngine 用 LLM 整理答案\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410f9e09-c4e1-4cdb-8f6c-74d9265f566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=recursive_retriever,\n",
    "    llm=ollama_llm,\n",
    "    response_mode='compact'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cef12c1-144c-4100-a55e-7e11ceba23b6",
   "metadata": {},
   "source": [
    "### 回家挑戰\n",
    "\n",
    "如何透過Persist重構RecursiveRetriever"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

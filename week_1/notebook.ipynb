{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8df86b3-5640-4b79-8819-8573b06a4930",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "æœ¬ç« ç¯€å°‡å¸¶ä½ å¿«é€Ÿèµ°éä¸€å€‹ **LlamaIndex å‘é‡æª¢ç´¢ï¼ˆVector-based RAGï¼‰** çš„æœ€å°å¯è¡Œæµç¨‹ï¼ˆMinimal Working Exampleï¼‰ï¼ŒåŒ…å«ï¼š\n",
    "\n",
    "1. è¼‰å…¥ Embedding Model  \n",
    "2. å»ºç«‹ Vector Store  \n",
    "3. å°‡æ–‡ä»¶åˆ‡åˆ†ç‚º Nodes ä¸¦å»ºç«‹ Embeddings\n",
    "    - Node æ˜¯ LlamaIndex ç”šè‡³æ•´å€‹ RAG æ¶æ§‹ä¸­ï¼Œæª¢ç´¢ï¼ˆRetrievalï¼‰çš„ã€Œæœ€å°åŸå­å–®ä½ã€ã€‚\n",
    "4. å»ºç«‹ Index\n",
    "    - Index æ˜¯é€£æ¥ Node å’Œæ•¸æ“šå­˜å„²çš„é‚è¼¯æ©‹æ¨‘ã€‚\n",
    "    - çµ„ç¹”è€…ï¼šå®ƒæ±ºå®šäº†æ•¸æ“šå¦‚ä½•è¢«çµ„ç¹”ã€‚æœ€å¸¸è¦‹çš„æ˜¯ VectorStoreIndexï¼Œå®ƒæœƒæŠŠ Node çš„å‘é‡å­˜å…¥ Vector Storeã€‚\n",
    "    - ä¸­ä»‹ï¼šç•¶ä½ é€²è¡ŒæŸ¥è©¢æ™‚ï¼Œä½ æ˜¯å‘ Index æå•ï¼ŒIndex æœƒå»èª¿åº¦ Vector Store å°‹æ‰¾æœ€åŒ¹é…çš„ Nodeï¼Œä¸¦å°‡å…¶å›å‚³ã€‚\n",
    "6. ä½¿ç”¨ Retriever é€²è¡ŒæŸ¥è©¢  \n",
    "\n",
    "æ­¤æµç¨‹æ˜¯å»ºæ§‹ **Retrieval-Augmented Generationï¼ˆRAGï¼‰ç³»çµ±** çš„åŸºç¤ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca214960-3eb2-4513-8205-febe66ec4125",
   "metadata": {},
   "source": [
    "## Ollama Cloud Service with LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dee6fc09-f62d-4148-b932-5c2bcd6be8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "\n",
    "from initialization import credential_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f40009e5-2d0b-4ecd-8a29-d0195ed7f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "model = \"gpt-oss:120b-cloud\"\n",
    "base_url = \"https://ollama.com\"\n",
    "\n",
    "credential_init()\n",
    "\n",
    "ollama_llm = Ollama(model=model, request_timeout=60.0, \n",
    "                    base_url=base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725db8d9-e452-45f2-a0c1-f3a07da11b3f",
   "metadata": {},
   "source": [
    "ä¹Ÿå¯ä»¥å¾ src.ollama_connection è£¡ç›´æ¥èª¿ç”¨:\n",
    "\n",
    "```\n",
    "from src.ollama_connection import llama_index_ollama\n",
    "\n",
    "ollama_llm = llama_index_ollama(model=model, temperature=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99e53c7c-8c18-4746-bdef-47f96929068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await ollama_llm.acomplete(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffba2f4b-e61c-4bb5-91c2-dba5731e9042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text='The capital of France is **Paris**. Itâ€™s not only the countryâ€™s political center but also a major cultural, economic, and tourist hub.', additional_kwargs={}, raw={'model': 'gpt-oss:120b', 'created_at': '2026-01-31T08:40:29.156244368Z', 'done': True, 'done_reason': 'stop', 'total_duration': 654468568, 'load_duration': None, 'prompt_eval_count': 74, 'prompt_eval_duration': None, 'eval_count': 65, 'eval_duration': None, 'message': Message(role='assistant', content='The capital of France is **Paris**. Itâ€™s not only the countryâ€™s political center but also a major cultural, economic, and tourist hub.', thinking='User asks a straightforward question: \"What is the capital of France?\" Answer: Paris. Probably also add a brief note.', images=None, tool_name=None, tool_calls=None), 'logprobs': None, 'usage': {'prompt_tokens': 74, 'completion_tokens': 65, 'total_tokens': 139}}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb46db1-cfd5-4777-b96f-3bdd84e4d329",
   "metadata": {},
   "source": [
    "## Loading embedding model\n",
    "\n",
    "åœ¨ RAG ç³»çµ±ä¸­ï¼Œ**Embedding Model** è² è²¬å°‡æ–‡å­—è½‰æ›æˆé«˜ç¶­åº¦å‘é‡ï¼ˆvectorï¼‰ï¼Œä»¥ä¾¿å¾ŒçºŒé€²è¡Œèªæ„ç›¸ä¼¼åº¦æœå°‹ã€‚\n",
    "\n",
    "é€™è£¡æˆ‘å€‘ä½¿ç”¨ HuggingFace ä¸Šçš„ **BAAI/bge-small-en-v1.5**ï¼š\n",
    "- è¼•é‡ã€é€Ÿåº¦å¿«\n",
    "- é©åˆæœ¬åœ°ç«¯æˆ–æ•™å­¸ç¤ºç¯„\n",
    "- å‘é‡ç¶­åº¦ç‚º **384**\n",
    "\n",
    "```python\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "```\n",
    "\n",
    "ğŸ“Œ æ³¨æ„\n",
    "Embedding model çš„è¼¸å‡ºç¶­åº¦ï¼Œå¿…é ˆèˆ‡å¾Œé¢ Vector Store ä½¿ç”¨çš„å‘é‡ç¶­åº¦ä¸€è‡´ï¼Œå¦å‰‡æœƒç”¢ç”ŸéŒ¯èª¤ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f31eee42-0f2c-44ab-a311-4c3f18c249b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ling\\miniconda3\\envs\\rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# loads https://huggingface.co/BAAI/bge-small-en-v1.5\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aac67a7-9be1-4907-98c0-198db363e93e",
   "metadata": {},
   "source": [
    "## Loading a vector store\n",
    "\n",
    "Vector Store ç”¨ä¾†å„²å­˜ï¼š\n",
    "\n",
    ">- Node çš„å‘é‡è¡¨ç¤ºï¼ˆembeddingsï¼‰\n",
    "\n",
    ">- å°æ‡‰çš„ Node ID\n",
    "\n",
    "é€™è£¡ä½¿ç”¨ FAISSï¼ˆFacebook AI Similarity Searchï¼‰ï¼š\n",
    "\n",
    ">- é«˜æ•ˆèƒ½\n",
    "\n",
    ">- å¸¸è¦‹æ–¼æœ¬åœ°å‘é‡æª¢ç´¢å ´æ™¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f723a7b-0f4c-4325-b71b-3db5e04712a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "\n",
    "d = 384 # å¿…é ˆèˆ‡ embedding model çš„è¼¸å‡ºç¶­åº¦ä¸€è‡´\n",
    "\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfced4f5-7263-4bfe-b106-b365109ec12e",
   "metadata": {},
   "source": [
    "ğŸ“Œ IndexFlatL2\n",
    "è¡¨ç¤ºä½¿ç”¨ L2 è·é›¢ï¼ˆæ­å¼è·é›¢ï¼‰ä¾†è¨ˆç®—å‘é‡ç›¸ä¼¼åº¦ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9476f6e6-025f-4507-8573-a8ad8379c364",
   "metadata": {},
   "source": [
    "## Creating Nodes and Their Embeddings\n",
    "\n",
    "ç‚ºä»€éº¼éœ€è¦ Nodesï¼Ÿ\n",
    "\n",
    "åœ¨ LlamaIndex ä¸­ï¼š\n",
    "\n",
    ">- Document æ˜¯åŸå§‹æ–‡ä»¶\n",
    ">- Node æ˜¯å¯è¢«æª¢ç´¢çš„æœ€å°å–®ä½\n",
    "\n",
    "Node Parser æ±ºå®šäº†ï¼š\n",
    "\n",
    ">- æ–‡ä»¶å¦‚ä½•è¢«åˆ‡åˆ†\n",
    ">- æ¯å€‹ Node çš„ä¸Šä¸‹æ–‡ç¯„åœèˆ‡ metadata\n",
    "\n",
    "---\n",
    "SentenceWindowNodeParserï¼ˆå¥å­è¦–çª—è§£æå™¨ï¼‰\n",
    "\n",
    "æ­¤ Parser æœƒï¼š\n",
    "\n",
    ">- å°‡æ–‡ä»¶åˆ‡æˆã€Œå–®ä¸€å¥å­ã€\n",
    ">- ä½†åœ¨ metadata ä¸­ä¿ç•™å‰å¾Œçš„ã€Œå¥å­è¦–çª—ï¼ˆwindowï¼‰ã€\n",
    "\n",
    "é€™å° embedding éå¸¸æœ‰ç”¨ï¼š\n",
    "\n",
    ">- embedding å¾ˆç²¾æº–ï¼ˆå–®å¥ï¼‰\n",
    ">- å›ç­”æ™‚ä»ä¿æœ‰ä¸Šä¸‹æ–‡ï¼ˆwindowï¼‰\n",
    "\n",
    "---\n",
    "IngestionPipelineï¼ˆè³‡æ–™è™•ç†ç®¡ç·šï¼‰\n",
    "\n",
    "IngestionPipeline å°‡å¤šå€‹æ­¥é©Ÿä¸²åœ¨ä¸€èµ·ï¼š\n",
    "\n",
    ">1. Node Parserï¼ˆæ–‡ä»¶ â†’ Nodesï¼‰\n",
    ">2. Embedding Modelï¼ˆNodes â†’ Vectorsï¼‰\n",
    ">3. å¯«å…¥ Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e39e3f9c-e450-4c48-b5a6-5b209c5daa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter, SentenceWindowNodeParser\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "\n",
    "node_parser = SentenceWindowNodeParser(\n",
    "    # How many sentences on both sides to capture. \n",
    "    # Setting this to 3 results in 7 sentences.\n",
    "    window_size=3, # # å‰å¾Œå„ 3 å¥ï¼Œå…± 7 å¥\n",
    "    # the metadata key for to be used in MetadataReplacementPostProcessor\n",
    "    window_metadata_key=\"window\",\n",
    "    # the metadata key that holds the original sentence\n",
    "    original_text_metadata_key=\"original_sentence\"\n",
    ")\n",
    "\n",
    "\n",
    "# create the pipeline with transformations\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        node_parser,\n",
    "        embed_model,\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    ")\n",
    "\n",
    "# run the pipeline\n",
    "nodes = pipeline.run(documents=[Document.example()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ee60945-13ec-42c8-be3e-d89701b3fbc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c23c1b-76f2-4775-8206-2ad92f8f3550",
   "metadata": {},
   "source": [
    "ğŸ“Œ å¥½è™•\n",
    "\n",
    ">- æµç¨‹æ¨¡çµ„åŒ–\n",
    ">- æ˜“æ–¼æ›¿æ› parser / embedding / extractor\n",
    ">- é©åˆæ­£å¼å°ˆæ¡ˆä½¿ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f29b1e37-f00d-46b0-a083-5952d4c196bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': 'README.md',\n",
       " 'category': 'codebase',\n",
       " 'window': '\\nContext\\nLLMs are a phenomenal piece of technology for knowledge generation and reasoning.\\n They are pre-trained on large amounts of publicly available data.\\n How do we best augment LLMs with our own private data?\\n We need a comprehensive toolkit to help perform this data augmentation for LLMs.\\n\\n',\n",
       " 'original_sentence': '\\nContext\\nLLMs are a phenomenal piece of technology for knowledge generation and reasoning.\\n'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0688243d-9b33-4632-81ba-24e9a8afd341",
   "metadata": {},
   "source": [
    "## Building the Indexï¼ˆå»ºç«‹ç´¢å¼•ï¼‰\n",
    "\n",
    "Index æ˜¯ï¼š\n",
    "\n",
    ">- Vector Store çš„æŠ½è±¡å°è£\n",
    ">- æä¾› Retriever èˆ‡ Query Engine çš„å…¥å£\n",
    "\n",
    "Storage Context çš„æ ¸å¿ƒåŠŸç”¨\n",
    "\n",
    "åœ¨ LlamaIndex ä¸­ï¼Œè³‡æ–™ä¸åªæ˜¯ã€Œå‘é‡ã€è€Œå·²ã€‚ä¸€å€‹å®Œæ•´çš„ RAG ç³»çµ±éœ€è¦å„²å­˜å¤šç¨®è³‡è¨Šï¼ŒStorage Context è² è²¬çµ±ä¸€ç®¡ç†ä»¥ä¸‹å¹¾å€‹çµ„ä»¶ï¼š\n",
    "\n",
    "- Vector Store (å‘é‡å„²å­˜)ï¼š\n",
    "    - å„²å­˜ Node çš„ Embeddingï¼ˆå‘é‡ï¼‰ã€‚\n",
    "    - ç”¨æ–¼ã€Œèªç¾©æœå°‹ã€ã€‚\n",
    "\n",
    "- Docstore (æ–‡ä»¶å„²å­˜)ï¼š\n",
    "    - å„²å­˜ Node çš„ åŸå§‹æ–‡æœ¬å…§å®¹ å’Œ å…ƒæ•¸æ“š (Metadata)ã€‚\n",
    "    - ç‚ºä»€éº¼éœ€è¦ï¼Ÿ å› ç‚ºå‘é‡æ•¸æ“šåº«æœ‰æ™‚åªå­˜å‘é‡ï¼Œç•¶æœå°‹åˆ°åŒ¹é…çš„å‘é‡å¾Œï¼Œç³»çµ±éœ€è¦å›é ­å¾ Docstore æŠ“å–çœŸæ­£çš„æ–‡å­—çµ¦ LLM çœ‹ã€‚\n",
    "\n",
    "- Index Store (ç´¢å¼•å„²å­˜)ï¼š\n",
    "\n",
    "    - å„²å­˜ Index æœ¬èº«çš„çµæ§‹è³‡è¨Šï¼ˆä¾‹å¦‚ï¼šé€™æ£µæ¨¹æ˜¯æ€éº¼é€£çš„ï¼Œæˆ–è€…é€™å€‹æ‘˜è¦ç´¢å¼•çš„å±¤ç´šé—œä¿‚ï¼‰ã€‚\n",
    "\n",
    "- Property Graph Store (å±¬æ€§åœ–å„²å­˜)ï¼š\n",
    "\n",
    "    - ï¼ˆé¸é…ï¼‰ç”¨æ–¼å„²å­˜çŸ¥è­˜åœ–è­œï¼ˆKnowledge Graphï¼‰çš„å¯¦é«”èˆ‡é—œä¿‚ã€‚\n",
    " \n",
    "![caption](Gemini_Generated_Image_gfpi58gfpi58gfpi.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "801dc0f8-d669-4f9f-aabe-b0d02984943e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "\n",
    "# storage_context = StorageContext.from_defaults(\n",
    "#     vector_store=vector_store\n",
    "# )\n",
    "\n",
    "index = VectorStoreIndex(\n",
    "    nodes,\n",
    "    # storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    "    llm=ollama_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad3fbda-e353-41a9-876b-dabdc21d1570",
   "metadata": {},
   "source": [
    "## Retrieving Nodesï¼ˆèªæ„æª¢ç´¢ï¼‰\n",
    "\n",
    "Retriever è² è²¬ï¼š\n",
    "\n",
    ">- å°‡ query è½‰æˆ embedding\n",
    ">- èˆ‡å‘é‡è³‡æ–™åº«æ¯”å°\n",
    ">- å›å‚³æœ€ç›¸ä¼¼çš„ Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dde936c3-817e-46ba-9314-4bd7953489d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(similarity_top_k=3, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f11214f5-a1a0-4185-838a-4369bc99c243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='83ce29da-7aab-4f87-a315-d0d38e2a8720', embedding=None, metadata={'filename': 'README.md', 'category': 'codebase', 'window': '\\nContext\\nLLMs are a phenomenal piece of technology for knowledge generation and reasoning.\\n They are pre-trained on large amounts of publicly available data.\\n How do we best augment LLMs with our own private data?\\n We need a comprehensive toolkit to help perform this data augmentation for LLMs.\\n\\n', 'original_sentence': '\\nContext\\nLLMs are a phenomenal piece of technology for knowledge generation and reasoning.\\n'}, excluded_embed_metadata_keys=['window', 'original_sentence'], excluded_llm_metadata_keys=['window', 'original_sentence'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2ad266b9-b7a5-41fa-ad8d-484efbf444d8', node_type='4', metadata={'filename': 'README.md', 'category': 'codebase'}, hash='8f24ce02310203160c5e18490ef2c8acba3077d47170f8d9ae1191e323687c3a'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='e35b3d7b-61a4-424b-8427-a01f9e0517de', node_type='1', metadata={'window': \"\\nContext\\nLLMs are a phenomenal piece of technology for knowledge generation and reasoning.\\n They are pre-trained on large amounts of publicly available data.\\n How do we best augment LLMs with our own private data?\\n We need a comprehensive toolkit to help perform this data augmentation for LLMs.\\n\\n Proposed Solution\\nThat's where LlamaIndex comes in. \", 'original_sentence': 'They are pre-trained on large amounts of publicly available data.\\n'}, hash='dafec2bdd1ceda52a621040867eb36a0c232076ab2836885bf2d8b5096c7417b')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='\\nContext\\nLLMs are a phenomenal piece of technology for knowledge generation and reasoning.\\n', mimetype='text/plain', start_char_idx=0, end_char_idx=91, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8034583910694777),\n",
       " NodeWithScore(node=TextNode(id_='8837cb13-5044-4625-bc8b-b048bdeabd74', embedding=None, metadata={'filename': 'README.md', 'category': 'codebase', 'window': 'How do we best augment LLMs with our own private data?\\n We need a comprehensive toolkit to help perform this data augmentation for LLMs.\\n\\n Proposed Solution\\nThat\\'s where LlamaIndex comes in.  LlamaIndex is a \"data framework\" to help\\nyou build LLM  apps.  It provides the following tools:\\n\\nOffers data connectors to ingest your existing data sources and data formats\\n(APIs, PDFs, docs, SQL, etc.)\\n Provides ways to structure your data (indices, graphs) so that this data can be\\neasily used with LLMs.\\n Provides an advanced retrieval/query interface over your data:\\nFeed in any LLM input prompt, get back retrieved context and knowledge-augmented output.\\n', 'original_sentence': 'LlamaIndex is a \"data framework\" to help\\nyou build LLM  apps. '}, excluded_embed_metadata_keys=['window', 'original_sentence'], excluded_llm_metadata_keys=['window', 'original_sentence'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2ad266b9-b7a5-41fa-ad8d-484efbf444d8', node_type='4', metadata={'filename': 'README.md', 'category': 'codebase'}, hash='8f24ce02310203160c5e18490ef2c8acba3077d47170f8d9ae1191e323687c3a'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='3d5adeca-0463-4f46-9449-f44da23c380b', node_type='1', metadata={'filename': 'README.md', 'category': 'codebase', 'window': 'They are pre-trained on large amounts of publicly available data.\\n How do we best augment LLMs with our own private data?\\n We need a comprehensive toolkit to help perform this data augmentation for LLMs.\\n\\n Proposed Solution\\nThat\\'s where LlamaIndex comes in.  LlamaIndex is a \"data framework\" to help\\nyou build LLM  apps.  It provides the following tools:\\n\\nOffers data connectors to ingest your existing data sources and data formats\\n(APIs, PDFs, docs, SQL, etc.)\\n Provides ways to structure your data (indices, graphs) so that this data can be\\neasily used with LLMs.\\n', 'original_sentence': \"Proposed Solution\\nThat's where LlamaIndex comes in. \"}, hash='1862a35b149b0e217ca6f8886ba07a6effed7a7321569e8561f7b5e9c00a199b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='6e9b52c0-e1e4-4dfe-8cbd-67ab5758dbfc', node_type='1', metadata={'window': 'We need a comprehensive toolkit to help perform this data augmentation for LLMs.\\n\\n Proposed Solution\\nThat\\'s where LlamaIndex comes in.  LlamaIndex is a \"data framework\" to help\\nyou build LLM  apps.  It provides the following tools:\\n\\nOffers data connectors to ingest your existing data sources and data formats\\n(APIs, PDFs, docs, SQL, etc.)\\n Provides ways to structure your data (indices, graphs) so that this data can be\\neasily used with LLMs.\\n Provides an advanced retrieval/query interface over your data:\\nFeed in any LLM input prompt, get back retrieved context and knowledge-augmented output.\\n Allows easy integrations with your outer application framework\\n(e.g. ', 'original_sentence': 'It provides the following tools:\\n\\nOffers data connectors to ingest your existing data sources and data formats\\n(APIs, PDFs, docs, SQL, etc.)\\n'}, hash='9033ae2d2c79f1289fdfbeda75767fa7f8ddd4239ba3a474f1e869506db35387')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='LlamaIndex is a \"data framework\" to help\\nyou build LLM  apps. ', mimetype='text/plain', start_char_idx=346, end_char_idx=408, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.7139249381757001),\n",
       " NodeWithScore(node=TextNode(id_='63289909-1e73-4ec6-b8ef-56c6e6176de3', embedding=None, metadata={'filename': 'README.md', 'category': 'codebase', 'window': '\\nContext\\nLLMs are a phenomenal piece of technology for knowledge generation and reasoning.\\n They are pre-trained on large amounts of publicly available data.\\n How do we best augment LLMs with our own private data?\\n We need a comprehensive toolkit to help perform this data augmentation for LLMs.\\n\\n Proposed Solution\\nThat\\'s where LlamaIndex comes in.  LlamaIndex is a \"data framework\" to help\\nyou build LLM  apps.  It provides the following tools:\\n\\nOffers data connectors to ingest your existing data sources and data formats\\n(APIs, PDFs, docs, SQL, etc.)\\n', 'original_sentence': 'We need a comprehensive toolkit to help perform this data augmentation for LLMs.\\n\\n'}, excluded_embed_metadata_keys=['window', 'original_sentence'], excluded_llm_metadata_keys=['window', 'original_sentence'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2ad266b9-b7a5-41fa-ad8d-484efbf444d8', node_type='4', metadata={'filename': 'README.md', 'category': 'codebase'}, hash='8f24ce02310203160c5e18490ef2c8acba3077d47170f8d9ae1191e323687c3a'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='be3fccc7-4474-4476-ba71-ad7836962646', node_type='1', metadata={'filename': 'README.md', 'category': 'codebase', 'window': '\\nContext\\nLLMs are a phenomenal piece of technology for knowledge generation and reasoning.\\n They are pre-trained on large amounts of publicly available data.\\n How do we best augment LLMs with our own private data?\\n We need a comprehensive toolkit to help perform this data augmentation for LLMs.\\n\\n Proposed Solution\\nThat\\'s where LlamaIndex comes in.  LlamaIndex is a \"data framework\" to help\\nyou build LLM  apps. ', 'original_sentence': 'How do we best augment LLMs with our own private data?\\n'}, hash='5541c03ee4dbb9deb2aff06374618b268581d2150acb70048a79d3013563901c'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='3d5adeca-0463-4f46-9449-f44da23c380b', node_type='1', metadata={'window': 'They are pre-trained on large amounts of publicly available data.\\n How do we best augment LLMs with our own private data?\\n We need a comprehensive toolkit to help perform this data augmentation for LLMs.\\n\\n Proposed Solution\\nThat\\'s where LlamaIndex comes in.  LlamaIndex is a \"data framework\" to help\\nyou build LLM  apps.  It provides the following tools:\\n\\nOffers data connectors to ingest your existing data sources and data formats\\n(APIs, PDFs, docs, SQL, etc.)\\n Provides ways to structure your data (indices, graphs) so that this data can be\\neasily used with LLMs.\\n', 'original_sentence': \"Proposed Solution\\nThat's where LlamaIndex comes in. \"}, hash='267e782ab6dd75d8331ca9b6848760285db09cba89933c9ade985c3ccd3e0b82')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='We need a comprehensive toolkit to help perform this data augmentation for LLMs.\\n\\n', mimetype='text/plain', start_char_idx=212, end_char_idx=294, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6976031646506744)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.retrieve(\"what is LLM?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af9c939-d17b-4b7a-a0ec-91ee2027deae",
   "metadata": {},
   "source": [
    "å›å‚³çµæœæ˜¯ Node listï¼Œä¸æ˜¯æœ€çµ‚æ–‡å­—ç­”æ¡ˆ\n",
    "\n",
    "é€™äº› Nodes æœƒåœ¨å¾ŒçºŒäº¤çµ¦ LLM ç”¢ç”Ÿå›æ‡‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb4397a2-be98-4e98-94ea-5388285e08fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMs are a phenomenal piece of technology for knowledge generation and reasoning.\n"
     ]
    }
   ],
   "source": [
    "# Create vector index from base nodes\n",
    "# base_index = VectorStoreIndex(base_nodes)\n",
    "\n",
    "# Instantiate query engine from vector index\n",
    "base_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=1,\n",
    ")\n",
    "\n",
    "# Send query to the engine to get related node(s)\n",
    "base_response = base_query_engine.query(\"what is LLM?\")\n",
    "\n",
    "print(base_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "716d3d49-743e-46d9-8c48-7206da32912d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='83ce29da-7aab-4f87-a315-d0d38e2a8720', embedding=None, metadata={'filename': 'README.md', 'category': 'codebase', 'window': '\\nContext\\nLLMs are a phenomenal piece of technology for knowledge generation and reasoning.\\n They are pre-trained on large amounts of publicly available data.\\n How do we best augment LLMs with our own private data?\\n We need a comprehensive toolkit to help perform this data augmentation for LLMs.\\n\\n', 'original_sentence': '\\nContext\\nLLMs are a phenomenal piece of technology for knowledge generation and reasoning.\\n'}, excluded_embed_metadata_keys=['window', 'original_sentence'], excluded_llm_metadata_keys=['window', 'original_sentence'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2ad266b9-b7a5-41fa-ad8d-484efbf444d8', node_type='4', metadata={'filename': 'README.md', 'category': 'codebase'}, hash='8f24ce02310203160c5e18490ef2c8acba3077d47170f8d9ae1191e323687c3a'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='e35b3d7b-61a4-424b-8427-a01f9e0517de', node_type='1', metadata={'window': \"\\nContext\\nLLMs are a phenomenal piece of technology for knowledge generation and reasoning.\\n They are pre-trained on large amounts of publicly available data.\\n How do we best augment LLMs with our own private data?\\n We need a comprehensive toolkit to help perform this data augmentation for LLMs.\\n\\n Proposed Solution\\nThat's where LlamaIndex comes in. \", 'original_sentence': 'They are pre-trained on large amounts of publicly available data.\\n'}, hash='dafec2bdd1ceda52a621040867eb36a0c232076ab2836885bf2d8b5096c7417b')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='\\nContext\\nLLMs are a phenomenal piece of technology for knowledge generation and reasoning.\\n', mimetype='text/plain', start_char_idx=0, end_char_idx=91, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.8034583910694777)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_response.source_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb38a72-fe29-449a-839a-a9439d1f1ba5",
   "metadata": {},
   "source": [
    "# Basic Node Parsers\n",
    "\n",
    "å®˜æ–¹æ–‡æª”: https://developers.llamaindex.ai/python/framework/module_guides/loading/node_parsers/modules/\n",
    "\n",
    "åœ¨ **LlamaIndex** ä¸­ï¼Œ**Node Parser**ï¼ˆç¯€é»è§£æå™¨ï¼‰è² è²¬å°‡åŸå§‹æ–‡ä»¶ï¼ˆå¦‚ PDFã€Markdownã€ç´”æ–‡å­—ï¼‰åˆ‡åˆ†æˆè¼ƒå°ã€çµæ§‹åŒ–çš„å–®ä½ï¼Œç¨±ç‚º **Nodes**ã€‚é€™äº› Nodes æ˜¯å¾ŒçºŒå»ºç«‹ç´¢å¼•ã€æª¢ç´¢èˆ‡èªæ„æœå°‹çš„åŸºç¤ã€‚\n",
    "\n",
    "## ç‚ºä»€éº¼éœ€è¦ Node Parserï¼Ÿ\n",
    "- ğŸ§© **æå‡æª¢ç´¢å“è³ª**ï¼šé©ç•¶çš„åˆ‡åˆ†å¯è®“æ¨¡å‹æ›´ç²¾æº–ç†è§£ä¸Šä¸‹æ–‡  \n",
    "- âš–ï¸ **æ§åˆ¶ä¸Šä¸‹æ–‡é•·åº¦**ï¼šé¿å…å–®æ¬¡è¼¸å…¥éé•·ï¼Œè¶…å‡º LLM é™åˆ¶  \n",
    "- ğŸ—ï¸ **ä¿ç•™çµæ§‹è³‡è¨Š**ï¼šå¯åŒ…å« metadataï¼ˆå¦‚æ®µè½ã€é ç¢¼ã€æ¨™é¡Œï¼‰\n",
    "\n",
    "## å¸¸è¦‹çš„ Node Parser é¡å‹\n",
    "- **SentenceSplitter**ï¼šä¾å¥å­åˆ‡åˆ†ï¼ˆæœ€å¸¸ç”¨ï¼‰\n",
    "- **TokenTextSplitter**ï¼šä¾ token æ•¸é‡åˆ‡åˆ†ï¼Œé©åˆç²¾ç´°æ§åˆ¶\n",
    "- **MarkdownNodeParser**ï¼šä¿ç•™ Markdown çµæ§‹ï¼ˆå¦‚æ¨™é¡Œã€æ¸…å–®ï¼‰\n",
    "- **HierarchicalNodeParser**ï¼šå»ºç«‹çˆ¶å­éšå±¤çš„ç¯€é»çµæ§‹\n",
    "\n",
    "## åŸºæœ¬ä½¿ç”¨ç¯„ä¾‹\n",
    "```python\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "\n",
    "parser = SentenceSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "```\n",
    "\n",
    "## ç¸½çµ\n",
    "\n",
    "Node Parser æ˜¯ LlamaIndex æ–‡ä»¶è™•ç†æµç¨‹ä¸­çš„é—œéµå…ƒä»¶ï¼Œæ±ºå®šäº†è³‡æ–™å¦‚ä½•è¢«åˆ‡åˆ†èˆ‡ç†è§£ã€‚é¸æ“‡åˆé©çš„ Parser èˆ‡åƒæ•¸ï¼Œèƒ½é¡¯è‘—å½±éŸ¿ RAGï¼ˆRetrieval-Augmented Generationï¼‰ç³»çµ±çš„æ•ˆèƒ½èˆ‡æº–ç¢ºåº¦ã€‚\n",
    "\n",
    "## åŸºæ–¼æª”æ¡ˆçš„Node Parser\n",
    "\n",
    "æœ‰å¤šç¨®**ä»¥æª”æ¡ˆç‚ºåŸºç¤çš„ Node Parser**ï¼Œæœƒæ ¹æ“šè¢«è§£æçš„å…§å®¹é¡å‹ï¼ˆä¾‹å¦‚ JSONã€Markdown ç­‰ï¼‰ä¾†å»ºç«‹å°æ‡‰çš„ç¯€é»ï¼ˆNodesï¼‰ã€‚\n",
    "\n",
    "æœ€ç°¡å–®çš„æµç¨‹æ˜¯å°‡ **FlatFileReader** èˆ‡ **SimpleFileNodeParser** çµåˆä½¿ç”¨ï¼Œç³»çµ±æœƒè‡ªå‹•ç‚ºæ¯ä¸€ç¨®å…§å®¹é¡å‹é¸æ“‡æœ€åˆé©çš„ Node Parserã€‚æ¥è‘—ï¼Œä½ å¯ä»¥å†å°‡é€™é¡æª”æ¡ˆå‹ Node Parser èˆ‡**æ–‡å­—å‹ Node Parser** ä¸²æ¥ä½¿ç”¨ï¼Œä»¥é€²ä¸€æ­¥è€ƒé‡å¯¦éš›æ–‡å­—é•·åº¦ï¼Œç¢ºä¿åˆ‡åˆ†çµæœæ›´ç¬¦åˆå¾ŒçºŒè™•ç†éœ€æ±‚ã€‚\n",
    "\n",
    "\n",
    "### SimpleFileNodeParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7522ed63-b461-4a01-aba2-8697ce164ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from llama_index.core.node_parser import SimpleFileNodeParser\n",
    "from llama_index.readers.file import FlatReader\n",
    "\n",
    "\n",
    "md_docs = FlatReader().load_data(Path(\"week_1/week-1.md\"))\n",
    "\n",
    "parser = SimpleFileNodeParser()\n",
    "md_nodes = parser.get_nodes_from_documents(md_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131fcb16-598f-4878-827c-6cb0f6109448",
   "metadata": {},
   "source": [
    "### HTMLNodeParser\n",
    "æ­¤ Node Parser æœƒä½¿ç”¨ **BeautifulSoup** ä¾†è§£æåŸå§‹çš„ HTML å…§å®¹ã€‚\n",
    "\n",
    "é è¨­æƒ…æ³ä¸‹ï¼Œå®ƒåªæœƒè§£æä¸€éƒ¨åˆ†æŒ‡å®šçš„ HTML æ¨™ç±¤ï¼Œä½†ä½ ä¹Ÿå¯ä»¥è‡ªè¡Œè¦†å¯«é€™å€‹è¨­å®šã€‚\n",
    "\n",
    "é è¨­è§£æçš„æ¨™ç±¤åŒ…å«ï¼š\n",
    "`[\"p\", \"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"li\", \"b\", \"i\", \"u\", \"section\"]`\n",
    "\n",
    "å¯åŸ·è¡Œä»¥ä¸‹æŒ‡ä»¤ä¾†å–å¾—ç¯„ä¾‹ HTML æª”æ¡ˆï¼š\n",
    "\n",
    "```bash\n",
    "python -m html_file_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3137e4f-b87d-424f-b2f5-eea9dc59bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import HTMLNodeParser\n",
    "\n",
    "html_docs = FlatReader().load_data(Path(\"week_1/Titan.html\"))\n",
    "\n",
    "parser = HTMLNodeParser(tags=[\"p\", \"h1\", 'u', 'section'])  # optional list of tags\n",
    "nodes = parser.get_nodes_from_documents(html_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0894ea29-3727-4b46-a69a-48896f6e993c",
   "metadata": {},
   "source": [
    "### JSONNodeParser\n",
    "\n",
    "ç”¨æ–¼è§£æåŸå§‹çš„ JSON è³‡æ–™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ec5a05-52d7-4e76-91af-4208235ed85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import JSONNodeParser\n",
    "\n",
    "json_docs = FlatReader().load_data(Path(\"week_1/mock_data.json\"))\n",
    "\n",
    "parser = JSONNodeParser()\n",
    "\n",
    "nodes = parser.get_nodes_from_documents(json_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d841a42-3c6c-4796-b807-91f06910a8c6",
   "metadata": {},
   "source": [
    "### MarkdownNodeParser\n",
    "ç”¨æ–¼è§£æåŸå§‹çš„ Markdown æ–‡å­—å…§å®¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c8d4fd-7460-4fe1-9341-760557cbd274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from llama_index.readers.file import FlatReader\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "\n",
    "parser = MarkdownNodeParser()\n",
    "\n",
    "md_docs = FlatReader().load_data(Path(\"week_1/week-1.md\"))\n",
    "\n",
    "nodes = parser.get_nodes_from_documents(md_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b6ca0a-e681-46ee-af4f-335072ac04da",
   "metadata": {},
   "source": [
    "## Text-Splitters\n",
    "\n",
    "ä¸æ˜¯è¶Šå¤§è¶Šå¥½\n",
    "\n",
    ">- Chunk å¤ªå¤§ â†’ embedding æ··åˆå¤šå€‹ä¸»é¡Œï¼Œç›¸ä¼¼åº¦å¤±æº–\n",
    ">- Chunk å¤ªå° â†’ èªæ„ç ´ç¢ï¼ŒLLM å›ç­”å“è³ªä¸‹é™\n",
    "\n",
    "### CodeSplitter\n",
    "\n",
    "æœƒæ ¹æ“šç¨‹å¼ç¢¼æ‰€ä½¿ç”¨çš„ç¨‹å¼èªè¨€ï¼Œå°‡åŸå§‹çš„ç¨‹å¼ç¢¼æ–‡å­—é€²è¡Œåˆ‡åˆ†ã€‚\n",
    "\n",
    "å¯åœ¨ä»¥ä¸‹é€£çµæŸ¥çœ‹å®Œæ•´æ”¯æ´çš„ç¨‹å¼èªè¨€æ¸…å–®ï¼š\n",
    "\n",
    "https://github.com/grantjenks/py-tree-sitter-languages#license"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d0ebe9-44d4-43b9-bb01-ae5dea2dda2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from llama_index.readers.file import FlatReader\n",
    "from llama_index.core.node_parser import CodeSplitter\n",
    "\n",
    "documents = FlatReader().load_data(Path(\"week_1/mock_python.py\"))\n",
    "\n",
    "splitter = CodeSplitter(\n",
    "    language=\"python\",\n",
    "    chunk_lines=40,  # lines per chunk\n",
    "    chunk_lines_overlap=15,  # lines overlap between chunks\n",
    "    max_chars=1500,  # max chars per chunk\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba56bfbd-21e7-4936-992c-56271cd8d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ede387-3f91-4803-a027-b77f54e37b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6782d40d-82b4-4bb8-900f-dad6ad30f580",
   "metadata": {},
   "source": [
    "### LangchainNodeParser\n",
    "ä½ ä¹Ÿå¯ä»¥å°‡ **LangChain** ä¸­ç¾æœ‰çš„ä»»ä½•æ–‡å­—åˆ‡åˆ†å™¨ï¼ˆtext splitterï¼‰åŒ…è£æˆä¸€å€‹ Node Parser ä¾†ä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7ff909-7a1a-439b-b422-c9efa58eadb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from llama_index.core.node_parser import LangchainNodeParser\n",
    "\n",
    "documents = FlatReader().load_data(Path(\"week_1/mock_langchain_text.md\"))\n",
    "\n",
    "parser = LangchainNodeParser(RecursiveCharacterTextSplitter(chunk_size=250))\n",
    "nodes = parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57866bb-cc6b-4810-addb-2f9a91f07ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eb32ac-86fa-420c-bed9-55e28c7c886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496745f1-a7dd-4fb7-8a7c-d626f6967188",
   "metadata": {},
   "source": [
    "### SentenceSplitter\n",
    "**SentenceSplitter** æœƒåœ¨ç›¡é‡ä¿ç•™å¥å­é‚Šç•Œçš„å‰æä¸‹ï¼Œå°æ–‡å­—å…§å®¹é€²è¡Œåˆ‡åˆ†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b838fe-a163-408f-9b74-fcc1f3d03c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "documents = FlatReader().load_data(Path(\"week_1/mock_langchain_text.md\"))\n",
    "\n",
    "sentence_splitter_node_parser = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "nodes = sentence_splitter_node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7aa15e-8066-49d2-8d03-6b49a0632cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter.split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484ee3e-82dd-4da2-8e83-b4a4af9947cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo ç§»å» Advance Node Parser\n",
    "# ### SentenceWindowNodeParser\n",
    "\n",
    "# `SentenceWindowNodeParser` èˆ‡å…¶ä»–ç¯€é»è§£æå™¨ç›¸ä¼¼ï¼Œä¸éå®ƒæœƒå°‡æ‰€æœ‰æ–‡ä»¶æ‹†åˆ†æˆå–®ç¨çš„å¥å­ã€‚  \n",
    "# ç”Ÿæˆçš„ç¯€é»åœ¨å…¶ metadata ä¸­é‚„åŒ…å«äº†æ¯å€‹ç¯€é»å‘¨åœçš„ã€Œå¥å­çª—å£ã€ã€‚  \n",
    "# > âš ï¸ è«‹æ³¨æ„ï¼Œé€™äº› metadata ä¸æœƒè¢« LLM æˆ–åµŒå…¥æ¨¡å‹çœ‹åˆ°ã€‚  \n",
    "\n",
    "# é€™åœ¨ç”Ÿæˆå…·æœ‰éå¸¸ç‰¹å®šç¯„åœçš„åµŒå…¥æ™‚æœ€ç‚ºæœ‰ç”¨ã€‚  \n",
    "# ç„¶å¾Œï¼Œçµåˆ `MetadataReplacementNodePostProcessor`ï¼Œæ‚¨å¯ä»¥åœ¨å°‡ç¯€é»å‚³é€çµ¦ LLM ä¹‹å‰ï¼Œç”¨å…¶å‘¨åœçš„ä¸Šä¸‹æ–‡æ›¿æ›è©²å¥å­ã€‚  \n",
    "\n",
    "# ä¸‹é¢æ˜¯ä¸€å€‹ä½¿ç”¨é è¨­è¨­å®šä¾†è¨­ç½®è§£æå™¨çš„ç¯„ä¾‹ï¼š  \n",
    "# > åœ¨å¯¦éš›æ‡‰ç”¨ä¸­ï¼Œæ‚¨é€šå¸¸åªéœ€è¦èª¿æ•´å¥å­çš„çª—å£å¤§å°å³å¯ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206bda57-c7d7-4ce3-a69a-b37cae544a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "# node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "#     # how many sentences on either side to capture\n",
    "#     window_size=3,\n",
    "#     # the metadata key that holds the window of surrounding sentences\n",
    "#     window_metadata_key=\"window\",\n",
    "#     # the metadata key that holds the original sentence\n",
    "#     original_text_metadata_key=\"original_sentence\","
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b183ed3-cf9b-405c-8851-57a3c7c259af",
   "metadata": {},
   "source": [
    "### SemanticSplitterNodeParser\n",
    "\n",
    "SemanticSplitter ä¸æ˜¯è² è²¬æ–·å¥çš„ï¼Œ\n",
    "å®ƒåªæ˜¯åœ¨ã€Œå·²æ–·å¥½çš„å¥å­ã€ä¹‹é–“æ‰¾èªæ„è·³èºã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e543c4ce-4883-41c7-9f17-88a92cb7a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "\n",
    "documents = FlatReader().load_data(Path(\"week_1/semantic_test_zh.txt\"))\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=32,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"ã€‚\", \"ï¼Œ\", \" \"]\n",
    ").split_text\n",
    "\n",
    "# text_splitter = SentenceSplitter(chunk_size=64,\n",
    "#                                  chunk_overlap=0).split_text\n",
    "\n",
    "semantic_splitter_node_parser = SemanticSplitterNodeParser.from_defaults(\n",
    "    embed_model = embed_model,\n",
    "    sentence_splitter=text_splitter,\n",
    "    include_metadata=True,\n",
    "    include_prev_next_rel=True,\n",
    "    breakpoint_percentile_threshold=75,\n",
    ")\n",
    "\n",
    "nodes = semantic_splitter_node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a66863-f776-46de-97fb-fc80d87b80cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede0216d-74ea-468a-a04a-033b96ae7ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1740a41-628f-489c-b3dc-4a979c6361ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a815d52-a00f-435e-a5e7-281a6896431d",
   "metadata": {},
   "source": [
    "### TokenTextSplitter\n",
    "`TokenTextSplitter` å˜—è©¦æ ¹æ“šåŸå§‹çš„ **token æ•¸é‡** å°‡æ–‡æœ¬åˆ‡åˆ†æˆå¤§å°ä¸€è‡´çš„å¡Šï¼ˆchunkï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b85fe69-9553-47ad-9f8b-14866e1480f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "node_parser = TokenTextSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    "    separator=\" \",\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5e691b-a638-4531-a8cc-edd32b257c96",
   "metadata": {},
   "source": [
    "# VectorStore\n",
    "\n",
    "é€™è£¡æˆ‘å€‘å±•ç¤ºå¦‚ä½•å®£å‘Švectorstoreï¼Œç„¶å¾Œåœ¨Indexçš„éƒ¨åˆ†ç¤ºç¯„å¦‚ä½•retrieveã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b7111-93ed-487c-98a6-20d0bd55b5bd",
   "metadata": {},
   "source": [
    "## FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff6e37f-adad-46d9-9402-c0b862831706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "\n",
    "d = 1024 # å¿…é ˆèˆ‡ embedding model çš„è¼¸å‡ºç¶­åº¦ä¸€è‡´ï¼Œé€™é‚Šé…åˆ BAAI/bge-m3\n",
    "\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "faiss_vector_store = FaissVectorStore(faiss_index=faiss_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e4f7af-0353-4b2a-a604-1c02027297b7",
   "metadata": {},
   "source": [
    "## Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ab6eb-1a1d-4435-b6a5-f681a435d529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from qdrant_client import AsyncQdrantClient\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "client = AsyncQdrantClient(path=\"week_1/langchain_qdrant\")\n",
    "\n",
    "qdrant_vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"my_document\" # This collection will hold your vectors and associated documents.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e72740-763f-4bc1-b904-17b9a69c12f6",
   "metadata": {},
   "source": [
    "# Index - Basic\n",
    "\n",
    "## VectorIndexStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f77054e-dab7-45bf-97ca-c194ecb0b307",
   "metadata": {},
   "source": [
    "å»ºç«‹documentsã€‚å…ˆè²æ˜æˆ‘æ²’è²·ä»–å€‘è‚¡ç¥¨ï¼Œä¹Ÿä¸æä¾›æŠ•è³‡å»ºè­°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fb8f97-d621-43b3-a3ed-5889a893d454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=120,\n",
    ")\n",
    "\n",
    "loader = PyMuPDFReader()\n",
    "\n",
    "files = [\n",
    "    \"week_1/TSMC_2024_å¹´åº¦è²¡å ±.pdf\",\n",
    "    \"week_1/å¯Œé‚¦é‡‘_2024_å¹´åº¦è²¡å ±.pdf\",\n",
    "]\n",
    "\n",
    "docs = []\n",
    "for f in files:\n",
    "    doc_text = \"\\n\\n\".join([d.get_content() for d in loader.load(f)])\n",
    "    docs.append(Document(text=doc_text, metadata={\"file_path\":f}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92900ce-6d1b-44f9-bfc5-ba570dfd089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e118c-a0ff-4f07-a949-d42228d72712",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].id_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de3645-6396-4b70-b4b7-ebbcea0e3dd6",
   "metadata": {},
   "source": [
    "### æ­é… FAISS\n",
    "\n",
    "é€™è£¡çš„åšæ³•ç®—æ˜¯ä¸€ç¨®å¿«é€Ÿæš´åŠ›çš„æ‰‹æ®µã€‚å‹™å¯¦ä¸Šæœƒç”¨\n",
    "\n",
    "parser â†’ nodes â†’ index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47739de7-0c70-4a81-ab67-3fb01cedc31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(vector_store=faiss_vector_store)\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "faiss_index = VectorStoreIndex.from_documents(\n",
    "    docs,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    "    llm=ollama_llm,\n",
    "    transformations=[text_splitter],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ac94e4-7775-45c6-9703-27564b35039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_retriever = faiss_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e9ae55-754c-43e6-b146-97f152b2f99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = await faiss_retriever.aretrieve(\"å¹´åº¦ç‡Ÿæ¥­é¡\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baec3b8-63c9-49c5-a372-9ade72ef0729",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5c494c-7bd6-49a3-a3d6-04aeb204144c",
   "metadata": {},
   "source": [
    "**FAISS ä¸æ”¯æ´åœ¨metadataå±¤é¢éæ¿¾ç¯€é»**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd38620-d904-4674-b8a5-2dc8c945dc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import MetadataFilters, ExactMatchFilter\n",
    "\n",
    "output = await faiss_retriever.aretrieve(\"å¯Œé‚¦å¹´åº¦ç‡Ÿæ¥­é¡\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1b718d-2309-4bf5-8dbf-94c2aa0b45ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba74fdf8-8295-4d4a-bf78-06373c88f5e5",
   "metadata": {},
   "source": [
    "### æ­é… Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9fa5e2-9c83-4891-8955-ba7baa42f331",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(vector_store=qdrant_vector_store)\n",
    "\n",
    "qdrant_index = VectorStoreIndex.from_documents(\n",
    "    docs,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    "    llm=ollama_llm,\n",
    "    transformations=[text_splitter],\n",
    "    use_async=True # å› ç‚ºæˆ‘å€‘ä½¿ç”¨async client\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e6f664-0fe5-4dc4-a131-217b1287f5b7",
   "metadata": {},
   "source": [
    "```\n",
    "ValueError: Async client is not initialized!\n",
    "Please pass in `aclient` to the constructor: `QdrantVectorStore(..., aclient=AsyncQdrantClient(...))`\n",
    "Selection deleted\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d4f525-607e-409e-9668-9e46c4447cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_retriever = qdrant_index.as_retriever(similarity_top_k=3)\n",
    "\n",
    "output = await qdrant_retriever.aretrieve(\"å¹´åº¦ç‡Ÿæ¥­é¡\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981b605b-91da-40ed-9c8b-cdefc54fa6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43212be-0713-461f-b764-0fa95c36d80e",
   "metadata": {},
   "source": [
    "é€émetadataé€²è¡Œéæ¿¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e9f37e-1ce6-4053-9d64-bc19e91cf65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import (\n",
    "    MetadataFilters,\n",
    "    ExactMatchFilter,\n",
    ")\n",
    "\n",
    "filters = MetadataFilters(\n",
    "    filters=[\n",
    "        ExactMatchFilter(key=\"file_path\", value=\"week_1/å¯Œé‚¦é‡‘_2024_å¹´åº¦è²¡å ±.pdf\"),\n",
    "        # ExactMatchFilter(key=\"source\", value=\"manual.pdf\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "qdrant_retriever = qdrant_index.as_retriever(\n",
    "    similarity_top_k=3,\n",
    "    filters=filters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be38d7d-1110-4688-8e69-16a5e9f1fdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = qdrant_retriever.retrieve(\"å¹´åº¦ç‡Ÿæ¥­é¡\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37e7bf3-c6a9-448b-8462-4909e6720acc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d696fe86-0c92-4b14-b3b5-c36f59e22bfc",
   "metadata": {},
   "source": [
    "# è©•ä¼° (Evaluation)\n",
    "\n",
    "æˆ‘å€‘é€™è£¡ç”¨LlamaIndexæä¾›çš„Metricsä¾†ç¤ºç¯„å¦‚ä½•é€²è¡Œè©•ä¼°\n",
    "å¾Œé¢æˆ‘å€‘æœƒä½¿ç”¨RAGASé€™å€‹æ¡†æ¶é€²è¡Œè©•ä¼°\n",
    "\n",
    "> âš ï¸ æ³¨æ„ï¼šé€™å€‹éç¨‹å¯èƒ½èŠ±è²»è¼ƒé«˜æˆæœ¬ã€‚è«‹å°å¿ƒä½¿ç”¨ï¼Œä¸¦æ ¹æ“šé ç®—èª¿æ•´æ¨£æœ¬æ•¸é‡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71983c26-47c1-41a5-b496-e8f04cd0ab22",
   "metadata": {},
   "source": [
    "ç”¢ç”Ÿnodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42194c6-fbae-4368-bd8f-b587a6802f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = text_splitter.get_nodes_from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f22473-4015-483b-b308-d04935d9cb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d56fa70-69ff-4d6c-a050-248cf9391f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import DatasetGenerator, QueryResponseDataset\n",
    "import nest_asyncio\n",
    "\n",
    "\"\"\"\n",
    "nest_asyncio æœƒ æ‰“è£œä¸ï¼ˆpatchï¼‰ ç¾æœ‰çš„äº‹ä»¶å¾ªç’°ï¼Œä½¿å¾— åœ¨å·²ç¶“é‹è¡Œçš„äº‹ä»¶å¾ªç’°ä¸­å¯ä»¥å†æ¬¡åµŒå¥—é‹è¡Œ asyncioã€‚\n",
    "\n",
    "æ›å¥è©±èªªï¼Œå®ƒè®“ä½ åœ¨ Jupyter Notebook è£¡å¯ä»¥é †åˆ©åŸ·è¡Œï¼š\n",
    "\n",
    "asyncio.run()\n",
    "\n",
    "è‡ªå·±å¯«çš„ async å‡½æ•¸\n",
    "\n",
    "å„ç¨®éœ€è¦äº‹ä»¶å¾ªç’°çš„åº«ï¼ˆä¾‹å¦‚ LLM SDKã€WebSocketã€FastAPI çš„æ¸¬è©¦ç­‰ï¼‰\n",
    "\n",
    "è€Œä¸æœƒç¢°åˆ°ã€Œäº‹ä»¶å¾ªç’°å·²ç¶“é‹è¡Œã€çš„éŒ¯èª¤ã€‚\n",
    "\"\"\"\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b65e40-c11f-4502-8d28-3c406c7ac14f",
   "metadata": {},
   "source": [
    "## å•ç­”é›†ç”Ÿæˆ\n",
    "\n",
    "`num_questions_per_chunk` èˆ‡ `num` çš„å·®ç•°èªªæ˜\n",
    "\n",
    "åœ¨ä½¿ç”¨ DatasetGeneratorï¼ˆä¾‹å¦‚ `generate_dataset_from_nodes`ï¼‰æ™‚ï¼Œ  \n",
    "`num_questions_per_chunk` èˆ‡ `num` åˆ†åˆ¥æ§åˆ¶ **æ·±åº¦** èˆ‡ **å»£åº¦**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 1. `num_questions_per_chunk`ï¼ˆæ¯å€‹ Node ç”¢ç”Ÿçš„å•é¡Œæ•¸é‡ï¼‰\n",
    "\n",
    "### å®šç¾©\n",
    "æ­¤åƒæ•¸å®šç¾© LLM åœ¨è™•ç†**å–®ä¸€ Nodeï¼ˆchunkï¼‰**æ™‚ï¼Œ  \n",
    "è¦å¾è©² Node çš„å…§å®¹ä¸­æŒ–æ˜å‡ºå¤šå°‘å€‹å•é¡Œã€‚\n",
    "\n",
    "- **ä½œç”¨å°è±¡**ï¼šå–®ä¸€ Node\n",
    "- **æ§åˆ¶é¢å‘**ï¼šæ·±åº¦ï¼ˆdepthï¼‰\n",
    "\n",
    "### è¡Œç‚ºé‚è¼¯\n",
    "- è¨­ç‚º `1`  \n",
    "  â†’ LLM è®€å– Node Aï¼Œç”¢ç”Ÿ **1 å€‹å•é¡Œ**\n",
    "- è¨­ç‚º `3`  \n",
    "  â†’ LLM é‡å° Node A çš„å…§å®¹ï¼Œç”¢ç”Ÿ **3 å€‹ä¸åŒçš„å•é¡Œ**\n",
    "\n",
    "### å½±éŸ¿\n",
    "- æ•¸å€¼è¶Šé«˜ï¼š\n",
    "  - å°å–®ä¸€å…§å®¹çš„æ¸¬è©¦è¶Šæ·±å…¥ã€è¶Šåš´å¯†\n",
    "- ä½†å¦‚æœï¼š\n",
    "  - Node æœ¬èº«å…§å®¹è¼ƒå°‘\n",
    "  - æˆ–è³‡è¨Šå¯†åº¦ä¸é«˜  \n",
    "  å‰‡å¯èƒ½å°è‡´ï¼š\n",
    "  - å•é¡Œé‡è¤‡\n",
    "  - æˆ–å•é¡Œå“è³ªä¸‹é™\n",
    "\n",
    "---\n",
    "\n",
    "## 2. `num`ï¼ˆç¸½å…±è™•ç†çš„ Node æ•¸é‡ï¼‰\n",
    "\n",
    "### å®šç¾©\n",
    "æ­¤åƒæ•¸å®šç¾© DatasetGenerator  \n",
    "**ç¸½å…±è¦å¾ `selected_nodes` ä¸­æŒ‘é¸å¤šå°‘å€‹ Node ä¾†ç”Ÿæˆå•é¡Œ**ã€‚\n",
    "\n",
    "- **ä½œç”¨å°è±¡**ï¼šæ•´å€‹ Node åˆ—è¡¨\n",
    "- **æ§åˆ¶é¢å‘**ï¼šå»£åº¦ï¼ˆbreadthï¼‰\n",
    "\n",
    "### è¡Œç‚ºé‚è¼¯\n",
    "- å‡è¨­ä½ æœ‰ `100` å€‹ Nodes\n",
    "- è‹¥è¨­å®š `num = 5`\n",
    "  - åªæœƒé¸å–å‰ `5` å€‹ Nodes ä¾†ç”Ÿæˆå•é¡Œ\n",
    "\n",
    "### å½±éŸ¿\n",
    "- æ±ºå®šæ¸¬è©¦è³‡æ–™é›†æ¶µè“‹ï¼š\n",
    "  - å¤šå°‘åŸå§‹æ–‡ä»¶å…§å®¹\n",
    "  - å¤šå°‘ä¸»é¡Œç¯„åœ\n",
    "\n",
    "---\n",
    "\n",
    "## 3. é¡Œç›®ç¸½æ•¸çš„è¨ˆç®—æ–¹å¼\n",
    "\n",
    "ä½ å¯ä»¥ç”¨ä»¥ä¸‹å…¬å¼ä¾†ä¼°ç®—æœ€çµ‚ç”Ÿæˆçš„é¡Œç›®æ•¸é‡ï¼š\n",
    "\n",
    "$$\n",
    "\\text{Total Questions} = \\text{num} \\times \\text{num\\_questions\\_per\\_chunk}\n",
    "$$\n",
    "\n",
    "### ç¯„ä¾‹\n",
    "```python\n",
    "num = 5\n",
    "num_questions_per_chunk = 1\n",
    "```\n",
    "\n",
    "- è™•ç† 5 å€‹ Nodes\n",
    "- æ¯å€‹ Node ç”¢ç”Ÿ 1 é¡Œ\n",
    "\n",
    "ğŸ‘‰ æœ€çµ‚ç”¢å‡ºï¼š5 å€‹å•é¡Œ\n",
    "\n",
    "---\n",
    "\n",
    "## 4. ç›´è¦ºåœ–è§£ï¼ˆç™½è©±æ¯”å–»ï¼‰\n",
    "\n",
    "å¯ä»¥æŠŠé€™å…©å€‹åƒæ•¸æƒ³åƒæˆã€Œå‡ºè€ƒé¡Œã€çš„æ–¹å¼ï¼š\n",
    "\n",
    "- numï¼šé¸å¹¾å€‹äºº\n",
    "    å¾å…¨ç­ä¸­é¸å‡º 5 å€‹å­¸ç”Ÿ\n",
    "- num_questions_per_chunkï¼šæ¯äººå¯«å¹¾é¡Œ\n",
    "    è¦æ±‚é€™ 5 å€‹å­¸ç”Ÿ\n",
    "\n",
    "æ¯äººå„å¯« 1 é¡Œè€ƒå·\n",
    "\n",
    "ğŸ‘‰ æœ€å¾Œä½ å°±æœƒæ‹¿åˆ° 5 é¡Œè€ƒé¡Œ\n",
    "\n",
    "## 5. ç¸½çµ\n",
    "\n",
    "| åƒæ•¸                        | æ§åˆ¶é¢å‘ | å•çš„æ˜¯ä»€éº¼         |\n",
    "| ------------------------- | ---- | ------------- |\n",
    "| `num`                     | å»£åº¦   | è¦æ¶µè“‹å¤šå°‘ Nodesï¼Ÿ  |\n",
    "| `num_questions_per_chunk` | æ·±åº¦   | æ¯å€‹ Node è¦æŒ–å¤šæ·±ï¼Ÿ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a09012-c5b2-4d14-b45b-5acc01becfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "selected_nodes = np.random.choice(nodes, size=50, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0628d4be-add7-4e27-befc-e0fa2e7f1581",
   "metadata": {},
   "source": [
    "å¯ä»¥é€é`text_question_template`é€™å€‹åƒæ•¸ä¾†è‡ªå®šç¾©æ•¸æ“šç”Ÿæˆä½¿ç”¨çš„promptã€‚ç›¡é‡ä¿æŒæ ¼å¼ä¸€è‡´ï¼Œå¦å¤–åˆ¥å¿˜äº†{context_str}å’Œ{query_str}æ˜¯å¿…å‚™çš„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca390f0-59c0-49e0-8f32-7b8f2158285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# 1. å°‡å­—ä¸²å°è£æˆ PromptTemplate ç‰©ä»¶\n",
    "text_question_template = PromptTemplate(\n",
    "    'Context information is below.\\n'\n",
    "    '---------------------\\n'\n",
    "    '{context_str}\\n'\n",
    "    '---------------------\\n'\n",
    "    'Given the context information and not prior knowledge.\\n'\n",
    "    'generate only questions based on the below query.\\n'\n",
    "    '{query_str}\\n'\n",
    "    'ã€‚æ‰€æœ‰çš„è¼¸å‡ºå¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡(traditional Chinese)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcfbafc-297c-44e6-8080-da4d28d5ab0b",
   "metadata": {},
   "source": [
    "ä¸€æ¬¡æ‰”50å€‹é€²å» Ollama CloudæœƒæŠ±æ€¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac37e1ca-2e14-426d-a56b-b94926da3337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# åˆ†æ‰¹ç”Ÿæˆï¼Œä¾‹å¦‚ä¸€æ¬¡åªè™•ç† 5 å€‹ nodeï¼Œæ¯æ‰¹ä¹‹é–“ä¼‘æ¯ä¸€ä¸‹\n",
    "async def batch_generate_questions(all_nodes, batch_size=5):\n",
    "    all_questions = []\n",
    "    # å°‡ 50 å€‹ nodes åˆ†æˆæ¯ 5 å€‹ä¸€çµ„\n",
    "    for i in range(0, len(all_nodes), batch_size):\n",
    "        batch_nodes = all_nodes[i:i + batch_size]\n",
    "        print(f\"æ­£åœ¨è™•ç†ç¬¬ {i} åˆ° {i+batch_size} å€‹ç¯€é»...\")\n",
    "\n",
    "        dataset_generator = DatasetGenerator(\n",
    "            batch_nodes,\n",
    "            llm=ollama_llm,\n",
    "            text_question_template=text_question_template,\n",
    "            num_questions_per_chunk=1,\n",
    "        )\n",
    "        \n",
    "        # é‡å°é€™ä¸€å°æ‰¹ç”Ÿæˆå•é¡Œ\n",
    "        # æ³¨æ„é€™è£¡ num å°±ç­‰æ–¼ batch_size\n",
    "        batch_questions = await dataset_generator.agenerate_dataset_from_nodes(num=len(batch_nodes))\n",
    "        all_questions.append(batch_questions)\n",
    "        \n",
    "        # è®“ Ollama å–˜å£æ°£ï¼Œä¼‘æ¯ 2 ç§’\n",
    "        await asyncio.sleep(2) \n",
    "    \n",
    "    return all_questions\n",
    "\n",
    "eval_dataset = await batch_generate_questions(all_nodes=selected_nodes[:10], batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49b645a-7378-4d2d-ab11-e2cb9bbe1d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad26e4-500f-4358-860a-5b49efa32045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation.dataset_generation import  QueryResponseDataset\n",
    "\n",
    "combined_queries = {}\n",
    "combined_responses = {}\n",
    "\n",
    "for ds in eval_dataset:\n",
    "    # å°‡æ¯å€‹ dataset çš„å­—å…¸åˆä½µåˆ°ç¸½å­—å…¸ä¸­\n",
    "    combined_queries.update(ds.queries)\n",
    "    combined_responses.update(ds.responses)\n",
    "    \n",
    "# å›å‚³ä¸€å€‹å…¨æ–°çš„ Dataset ç‰©ä»¶\n",
    "final_dataset = QueryResponseDataset(\n",
    "    queries=combined_queries,\n",
    "    responses=combined_responses\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aba5cc-fbcd-4c1f-a2f3-3a8377cb2f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset.qr_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15c636d-b85c-4320-a217-79c55a8c5a65",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "| Evaluator          | è©•ä¼°é‡é»    | æ˜¯å¦éœ€è¦åƒè€ƒç­”æ¡ˆ | æ˜¯å¦ä½¿ç”¨ Context |\n",
    "| ------------------ | ------- | -------- | ------------ |\n",
    "| Correctness        | æ˜¯å¦æ­£ç¢º    | éœ€è¦       | ä¸ä¸€å®š          |\n",
    "| SemanticSimilarity | èªæ„æ˜¯å¦ç›¸ä¼¼  | éœ€è¦       | ä¸ä¸€å®š          |\n",
    "| Relevancy          | æ˜¯å¦å›ç­”å•é¡Œ  | ä¸éœ€è¦      | ä¸éœ€è¦          |\n",
    "| Faithfulness       | æ˜¯å¦å¿ å¯¦æ–¼è³‡æ–™ | ä¸éœ€è¦      | éœ€è¦           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed18314-cbac-46bd-be02-7a1c10530c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "from llama_index.core.evaluation import CorrectnessEvaluator, SemanticSimilarityEvaluator, RelevancyEvaluator, FaithfulnessEvaluator\n",
    "\n",
    "evaluator_c = CorrectnessEvaluator(llm=ollama_llm)\n",
    "evaluator_s = SemanticSimilarityEvaluator(embed_model=embed_model)\n",
    "evaluator_r = RelevancyEvaluator(llm=ollama_llm)\n",
    "evaluator_f = FaithfulnessEvaluator(llm=ollama_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbf54a1-69b1-4b2c-bebf-c12c4a09f9a5",
   "metadata": {},
   "source": [
    "å»ºç«‹query_engine\n",
    "\n",
    "ç•¶ä½ å‘¼å« as_query_engine æ™‚å‚³å…¥ llm å’Œ embed_modelï¼Œé€™æœƒè¦†è“‹æ‰ï¼ˆOverrideï¼‰ç•¶åˆåœ¨å»ºç«‹ qdrant_index æ™‚è¨­å®šçš„å…¨åŸŸæˆ–ç´¢å¼•å±¤ç´šçš„ç‰©ä»¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff765d0a-7254-47b0-8deb-c5c77b26be47",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_query_engine = qdrant_index.as_query_engine(similarity_top_k=5,\n",
    "                                                   response_mode=\"compact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4864440b-a28b-4997-8a24-55a785d76e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation.eval_utils import get_responses, get_results_df\n",
    "from llama_index.core.evaluation import BatchEvalRunner\n",
    "\n",
    "eval_qs = final_dataset.questions\n",
    "qr_pairs = final_dataset.qr_pairs\n",
    "ref_response_strs = [r for (_, r) in qr_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ee1cf3-b67e-407b-ad47-4c85ed8b87c4",
   "metadata": {},
   "source": [
    "å¯¦é©—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cebeaf5-d5e1-4158-bc8a-7873c17af77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_responses = get_responses(eval_qs[:2], qdrant_query_engine, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad098feb-08de-4035-bf50-6f5c571ec8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred_responses[0].source_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7055d3-e769-42ae-8c53-655dbc6e1197",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_dict = {\n",
    "    \"correctness\": evaluator_c,\n",
    "    \"faithfulness\": evaluator_f,\n",
    "    \"relevancy\": evaluator_r,\n",
    "    \"similarity\": evaluator_s\n",
    "}\n",
    "\n",
    "batch_runner = BatchEvalRunner(evaluator_dict, workers=2, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aa2dc4-7a7c-4552-99c7-f91b300ec11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = await batch_runner.aevaluate_responses(\n",
    "    eval_qs[:2], # å•é¡Œ \n",
    "    responses=pred_responses, # å›æ‡‰ \n",
    "    reference=ref_response_strs[:2] # `æ¨™æº–ç­”æ¡ˆ`\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd86fe-4197-4957-ba50-b4d6a2cc2d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = get_results_df(\n",
    "    [eval_results],\n",
    "    names=[\"Qdrant\"],\n",
    "    metric_keys=['correctness', 'faithfulness', 'relevancy', 'similarity']\n",
    ")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38791186-a0c0-441b-a732-cc8096f9a3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results[\"correctness\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

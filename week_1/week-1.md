# ğŸ“š Week 1ï¼šLLM + LangChain å…¥é–€æ•™å­¸

æ­¡è¿ä¾†åˆ°æœ¬é€±èª²ç¨‹ï¼æœ¬å–®å…ƒå°‡å¸¶ä½ å¾é›¶é–‹å§‹äº†è§£å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŸºæœ¬æ¦‚å¿µï¼Œä¸¦å¯¦éš›é«”é©—å¦‚ä½•é‹ç”¨ LangChain æ¡†æ¶æ•´åˆ AI èƒ½åŠ›ã€‚

# èª²ç¨‹æœŸæœ›æ§åˆ¶<a name='èª²ç¨‹æœŸæœ›æ§åˆ¶'></a>

1. å»ºç«‹åŸºæœ¬æ¦‚å¿µï¼Œä¸å¿…æˆç‚ºç¨‹å¼é«˜æ‰‹

    - å³ä½¿ä½ æœªä¾†ä¸æ‰“ç®—å¯«ç¨‹å¼ï¼Œä¹Ÿè‡³å°‘èƒ½å° LLMï¼ˆå¤§å‹èªè¨€æ¨¡å‹ï¼‰æœ‰ä¸€å€‹ç›´è¦ºæ€§çš„ç†è§£ï¼š

2. ä»€éº¼ä»»å‹™æ˜¯ AI å¯ä»¥å¹«ä½ å®Œæˆçš„

    - ä»€éº¼ Proposal æˆ–å·¥å…·è²ç¨±èƒ½åšçš„äº‹æƒ…å…¶å¯¦æ˜¯èª‡å¤§çš„ã€ç”šè‡³æ˜¯é¨™äººçš„

3. èª²ç¨‹ä¸å¯èƒ½æ¶µè“‹æ‰€æœ‰éœ€æ±‚

    - æ¯å€‹äººçš„å·¥ä½œå ´æ™¯ã€éœ€æ±‚å’Œç›®æ¨™éƒ½ä¸åŒï¼Œæœ¬èª²ç¨‹æä¾›çš„æ˜¯é€šç”¨åŸºç¤èˆ‡æ€ç¶­æ–¹å¼ï¼Œä¸èƒ½æ¶µè“‹æ‰€æœ‰å°ˆæ¥­æˆ–å•†æ¥­ç´°ç¯€

4. ç¸®çŸ­æŠ€è¡“èˆ‡å•†æ¥­æºé€šçš„è½å·®

    - è®“ä½ åœ¨èˆ‡å·¥ç¨‹å¸«ã€AI åœ˜éšŠæˆ–é¡§å•è¨è«–æ™‚ï¼Œä¸æœƒå®Œå…¨è½ä¸æ‡‚ï¼Œä¹Ÿæ›´å®¹æ˜“åˆ¤æ–·å“ªäº›ææ¡ˆåˆç†ã€å“ªäº›éœ€è¦è¿½å•

5. å…¥é–€ç‚ºä¸»ï¼Œå¯¦ä¾‹ç‚ºè¼”

    - æœ¬èª²ç¨‹å®šä½æ˜¯å…¥é–€ï¼Œä½†æˆ‘æœƒç›¡é‡æä¾›å¯¦éš›ä¾‹å­ã€å ´æ™¯å’Œæ“ä½œæ¼”ç¤ºï¼Œå¹«åŠ©ä½ æŠŠæ¦‚å¿µã€Œè½åœ°ã€ï¼Œæ–¹ä¾¿æœªä¾†å¯¦éš›æ‡‰ç”¨
  
# å­¸ç¿’å¿ƒæ…‹æç¤º

1. ä¸è¦è¿½æ±‚å®Œç¾
    - LLM å’Œ AI çš„ä¸–ç•Œç¬æ¯è¬è®Šï¼Œä»Šå¤©çœ‹åˆ°çš„æ¡ˆä¾‹ï¼Œæ˜å¤©å¯èƒ½å°±æ›´æ–°äº†ã€‚é‡è¦çš„æ˜¯ç†è§£æ¦‚å¿µå’Œæ€è·¯ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡å°±æŒæ¡æ‰€æœ‰ç´°ç¯€ã€‚

2. å‹‡æ–¼å˜—è©¦ï¼Œæ•¢æ–¼çŠ¯éŒ¯
   - AI å¾ˆåƒä¸€å€‹å¼·å¤§çš„åŠ©æ‰‹ï¼Œæ“ä½œå®ƒçš„éç¨‹æœ¬èº«å°±æ˜¯å­¸ç¿’ã€‚éŒ¯èª¤å’Œæ„å¤–çµæœéƒ½æ˜¯æœ€å¥½çš„è€å¸«ã€‚

3. ä¿æŒå¥½å¥‡å¿ƒ
    - ä¸ç®¡ä½ çš„å°ˆæ¥­èƒŒæ™¯æ˜¯ä»€éº¼ï¼Œå° AI çš„æ¢ç´¢éƒ½èƒ½çµ¦ä½ å¸¶ä¾†æ–°çš„è¦–è§’ã€‚å¤šå•ã€Œç‚ºä»€éº¼å¯ä»¥é€™æ¨£åšï¼Ÿã€æ¯”å–®ç´”è¨˜ä½æ“ä½œæ›´é‡è¦ã€‚

4. æ¦‚å¿µå…ˆè¡Œï¼ŒæŠ€è¡“å…¶æ¬¡
    - ä¸å¿…æ“”å¿ƒè‡ªå·±ä¸æœƒå¯«ç¨‹å¼ï¼Œç†è§£ AI å¯ä»¥åšä»€éº¼ã€ä¸èƒ½åšä»€éº¼ï¼Œä»¥åŠå®ƒçš„å±€é™ï¼Œæ¯”æŒæ¡æ‰€æœ‰ç´°ç¯€æ›´å¯¦ç”¨ã€‚

5. äº’å‹•å’Œåˆ†äº«
    - èª²å ‚ä¸Šä½ çš„ç–‘å•å¾ˆå¯èƒ½ä¹Ÿå›°æ“¾å…¶ä»–äººï¼Œä¸æ‡‚å°±å•ï¼Œåˆ†äº«ä½ çš„è§€å¯Ÿå’Œæƒ³æ³•ï¼Œé€™æ¯”è¢«å‹•è½èª²æ›´èƒ½åŠ æ·±ç†è§£ã€‚

# ç’°å¢ƒè¨­ç½®

1. conda create -n aicg python=3.10
2. conda activate aicg
3. pip install -r requirements.txt
4. jupyter lab

# LangChain æ¡†æ¶ä»‹ç´¹

> ğŸ¯ **æœ¬ç« å­¸å®Œä½ å°‡èƒ½å­¸æœƒä»€éº¼ï¼š**
> - ç†è§£ LangChain çš„æ ¸å¿ƒçµ„ä»¶èˆ‡æ¨¡çµ„åŒ–è¨­è¨ˆç†å¿µ  
> - å­¸æœƒä½¿ç”¨ LLMã€PromptTemplateã€Chain ç­‰é—œéµæ¨¡çµ„  
> - èƒ½å¤ çµ„è£ç°¡å–®çš„ AI å·¥ä½œæµç¨‹ï¼ˆä¾‹å¦‚å•ç­”ã€æ‘˜è¦æˆ–å°è©±ç³»çµ±ï¼‰  

ä¸»æµå¤§èªè¨€æ¨¡å‹çš„æ‡‰ç”¨æ¡†æ¶

## 1. æ¨¡çµ„åŒ–æŠ½è±¡ (Modular Abstractions)

- æä¾›æ§‹å»ºç©æœ¨ï¼ˆLLM åŒ…è£å™¨ã€æç¤ºè©ã€è¨˜æ†¶ã€éˆæ¢ã€ä»£ç†äººï¼‰ï¼Œé¿å…é‡è¤‡ç™¼æ˜æ¨¡å¼ã€‚
- å¹«åŠ©ä»¥å¯æ“´å±•çš„æ–¹å¼çµ„ç¹”å°ˆæ¡ˆï¼Œè€Œä¸æ˜¯éš¨æ„çš„è…³æœ¬ã€‚

## 2. æ•´åˆèˆ‡ç”Ÿæ…‹ç³»çµ± (Integrations & Ecosystem)

- æ”¯æ´å¤šç¨® LLM ä¾›æ‡‰å•†ï¼ˆOpenAIã€Anthropicã€æœ¬åœ°æ¨¡å‹ç­‰ï¼‰ä»¥åŠå‘é‡è³‡æ–™åº«ï¼ˆPineconeã€Weaviateã€FAISS ç­‰ï¼‰ã€‚
- ä½¿æ›´æ›çµ„ä»¶è®Šå¾—ç°¡å–®ï¼Œç„¡éœ€é‡å¯«å¤§é‡ç¨‹å¼ç¢¼ã€‚

## 3. å¿«é€ŸåŸå‹é–‹ç™¼ (Rapid Prototyping)

- é©åˆå¿«é€Ÿé©—è­‰æƒ³æ³•ï¼šæª¢ç´¢å¢å¼·ç”Ÿæˆï¼ˆRAGï¼‰ã€å·¥å…·ä½¿ç”¨æˆ–å¤šæ­¥é©Ÿå·¥ä½œæµç¨‹ã€‚
- æ¸›å°‘æ¨£æ¿ç¨‹å¼ç¢¼ï¼Œä½¿ä½ èƒ½å°ˆæ³¨æ–¼æ‡‰ç”¨é‚è¼¯èˆ‡ä½¿ç”¨è€…é«”é©—ã€‚

## 4. ç¤¾ç¾¤èˆ‡æœ€ä½³å¯¦è¸ (Community & Best Practices)

- æ“æœ‰é¾å¤§çš„é–‹ç™¼è€…ç¤¾ç¾¤èˆ‡æ¨¡æ¿ç”Ÿæ…‹ç³»çµ±ã€‚
- ç·Šè·Ÿæ–°æŠ€è¡“ï¼ˆä¾‹å¦‚å‡½æ•¸èª¿ç”¨ã€ä»£ç†äººã€çµæ§‹åŒ–è¼¸å‡ºï¼‰ã€‚

## 5. ç”Ÿç”¢å°±ç·’åº¦ (Production-Readiness) ï¼ˆé™„æ³¨æ„äº‹é …ï¼‰

- LangChain è¡¨é”å¼èªè¨€ï¼ˆLCELï¼‰æå‡äº†é‡ç¾æ€§èˆ‡é™¤éŒ¯èƒ½åŠ›ã€‚
- å¯æ•´åˆè§€æ¸¬å·¥å…·ã€è¿½è¹¤èˆ‡ç›£æ§ã€‚
- é›–ç„¶æ—©æœŸç‰ˆæœ¬å› è¤‡é›œæ€§å—æ‰¹è©•ï¼Œä½†æ–°ç‰ˆæ›´å¼·èª¿ç©©å®šæ€§èˆ‡æ¸…æ™°çš„æŠ½è±¡æ¦‚å¿µã€‚

## 6. å­¸ç¿’èˆ‡ç”¢æ¥­å¥‘åˆåº¦ (Learning & Industry Alignment)

- ç”±æ–¼è¢«å»£æ³›æ¡ç”¨ï¼Œä½¿ç”¨ LangChain æ„å‘³è‘—ä½ çš„æŠ€èƒ½èˆ‡åŸå‹åœ¨åœ˜éšŠèˆ‡çµ„ç¹”é–“å…·å¯è½‰ç§»æ€§ä¸¦å—åˆ°èªå¯ã€‚

---
## ğŸ§© LangChain æ¡†æ¶çµæ§‹åœ–
LangChain æ˜¯ç”¨ä¾†ã€Œæ¨¡çµ„åŒ–çµ„è£ AI æµç¨‹ã€çš„é–‹æºæ¡†æ¶ã€‚  
å®ƒè®“ä½ èƒ½æŠŠè¤‡é›œçš„ LLM æ“ä½œåˆ†è§£æˆå¯é‡è¤‡ä½¿ç”¨çš„ç©æœ¨ï¼ˆmodulesï¼‰ã€‚

**åŸºæœ¬çµ„ä»¶åŒ…å«ï¼š**

| æ¨¡çµ„åç¨± | åŠŸèƒ½èªªæ˜ | ç¯„ä¾‹ |
|-----------|------------|------|
| `LLM` | èªè¨€æ¨¡å‹æ ¸å¿ƒ | GPT-4ã€Gemini ç­‰ |
| `PromptTemplate` | ç®¡ç†æç¤ºèªï¼ˆPromptï¼‰æ¨¡æ¿ | çµ±ä¸€è¼¸å…¥æ ¼å¼ |
| `Chain` | ä¸²æ¥å¤šå€‹æ­¥é©Ÿå½¢æˆæµç¨‹ | å•ç­” â†’ æ‘˜è¦ |
| `Memory` | ä¿å­˜ä¸Šä¸‹æ–‡å°è©± | èŠå¤©è¨˜éŒ„ |
| `Tool` | å‘¼å«å¤–éƒ¨åŠŸèƒ½ï¼ˆæœå°‹ã€ç¨‹å¼åŸ·è¡Œç­‰ï¼‰ | Google Searchã€Python |
| `Agent` | å…·å‚™æ±ºç­–é‚è¼¯çš„ AI åŸ·è¡Œè€… | è‡ªå‹•é¸æ“‡å·¥å…·å®Œæˆä»»å‹™ |

---

ğŸ§  **LangChain æ¦‚å¿µæµç¨‹åœ–**

```text
ä½¿ç”¨è€… â†’ PromptTemplate â†’ LLM â†’ OutputParser â†’ Chain / Agent â†’ å›å‚³çµæœ


# èª¿å‹•å¤§èªè¨€æ¨¡å‹API

## OpenAI API


```python
import os

os.chdir("../../../")
```


```python
# from langchain.chat_models import ChatOpenAI
from textwrap import dedent

from langchain_openai import ChatOpenAI

from src.initialization import credential_init
from src.io.path_definition import get_project_dir


credential_init()

model = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],
                   model_name="gpt-4o-mini", 
                   temperature=0 # a range from 0-2, the higher the value, the higher the `creativity`
                  )

# temperature has a range from 0-2, the higher the temperature, the more creative/unpredictable the outcomes. 
# to have a stable or more deterministic result, you should choose temperature = 0
```

## Gemini API<a name="Gemini"></a>

- https://aistudio.google.com/usage
- å…è²»æ˜¯æœ‰ä»£åƒ¹çš„: å…§å®¹æœƒè¢«ç”¨åšè¨“ç·´æ•¸æ“šï¼Œæ‰€ä»¥åˆ¥ä¸Šå‚³å€‹äººçš„è³‡æ–™


```python
import os

from langchain_google_genai import ChatGoogleGenerativeAI

os.environ["GOOGLE_API_KEY"] = "<YOUR GOOGLE API KEY>"

llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    # other params...
)
```


```python
try:
    response = llm.invoke("What date is today?")
    print("âœ… æˆåŠŸå‘¼å«æ¨¡å‹ï¼š", response.content)
except Exception as e:
    print("âš ï¸ éŒ¯èª¤ï¼šç„¡æ³•å‘¼å« OpenAI APIï¼Œè«‹ç¢ºèªä»¥ä¸‹é …ç›®ï¼š")
    print("1ï¸âƒ£ æ˜¯å¦å·²è¨­å®šç’°å¢ƒè®Šæ•¸ OPENAI_API_KEY")
    print("2ï¸âƒ£ æ˜¯å¦æœ‰ç¶²è·¯é€£ç·š")
    print("3ï¸âƒ£ æ¨¡å‹åç¨±æ˜¯å¦æ­£ç¢º")
    print("è©³ç´°éŒ¯èª¤è¨Šæ¯ï¼š", e)
```


```python
try:
    response = model.invoke("Tell me something about Apple Inc. Just a short summary")
    print("âœ… æˆåŠŸå‘¼å«æ¨¡å‹ï¼š", response.content)
except Exception as e:
    print("âš ï¸ éŒ¯èª¤ï¼šç„¡æ³•å‘¼å« OpenAI APIï¼Œè«‹ç¢ºèªä»¥ä¸‹é …ç›®ï¼š")
    print("1ï¸âƒ£ æ˜¯å¦å·²è¨­å®šç’°å¢ƒè®Šæ•¸ OPENAI_API_KEY")
    print("2ï¸âƒ£ æ˜¯å¦æœ‰ç¶²è·¯é€£ç·š")
    print("3ï¸âƒ£ æ¨¡å‹åç¨±æ˜¯å¦æ­£ç¢º")
    print("è©³ç´°éŒ¯èª¤è¨Šæ¯ï¼š", e)
```

---

> ğŸ”„ **å¾ Prompt åˆ° LangChain**
>
> åœ¨å‰ä¸€ç« ä¸­ï¼Œæˆ‘å€‘å­¸æœƒå¦‚ä½•èˆ‡ LLM å°è©±ï¼›  
> è€Œæ¥ä¸‹ä¾†çš„ LangChainï¼Œå‰‡å¹«åŠ©æˆ‘å€‘ã€Œæ¨¡çµ„åŒ–ã€é€™äº›å°è©±é‚è¼¯ã€‚  
>  
> å¦‚æœèªª Prompt æ˜¯ã€ŒAI çš„ä¸€å¥è©±ã€ï¼Œé‚£ LangChain å°±æ˜¯ã€Œçµ„æˆ AI ç³»çµ±çš„èªæ³•çµæ§‹ã€ã€‚  

# æç¤ºè©å·¥ç¨‹

> ğŸ¯ **æœ¬ç« å­¸å®Œä½ å°‡èƒ½å­¸æœƒä»€éº¼ï¼š**
> - ç†è§£ä»€éº¼æ˜¯ Promptï¼ˆæç¤ºè©ï¼‰åŠå…¶åœ¨å¤§å‹èªè¨€æ¨¡å‹ä¸­çš„è§’è‰²  
> - å­¸æœƒè¨­è¨ˆå…·é«”ã€æœ‰è§’è‰²åŒ–ä¸”ç›®æ¨™æ˜ç¢ºçš„ Prompt  
> - å¯¦éš›æ“ä½œ LangChain çš„ `PromptTemplate`ã€`ChatPromptTemplate` ä¸¦æ¸¬è©¦ä¸åŒæç¤ºæ•ˆæœ  


æ‰€è¬‚ã€ŒPromptã€ï¼Œå°±æ˜¯ä½ çµ¦ AI çš„ã€ŒæŒ‡ä»¤å¥ã€ã€‚  
æƒ³åƒä½ åœ¨è·ŸåŠ©ç†å°è©± â€”â€” ä½ æ€éº¼å•ï¼ŒAI å°±æ€éº¼ç­”ã€‚  
å­¸æœƒè¨­è¨ˆå¥½çš„ promptï¼Œå°±èƒ½è®“æ¨¡å‹æ›´æ‡‚ä½ ã€è¼¸å‡ºæ›´æº–ç¢ºï¼

---

ğŸ“Œ **ç°¡å–®ä¾‹å­ï¼š**
| Prompt | æ¨¡å‹å›è¦† |
|--------|-----------|
| ã€Œå¯«ä¸€é¦–è©©ã€ | è¼¸å‡ºéš¨æ©Ÿè©©å¥ |
| ã€Œç”¨èå£«æ¯”äºé¢¨æ ¼å¯«ä¸€é¦–é—œæ–¼ç¨‹å¼å“¡çš„è©©ã€ | è¼¸å‡ºæ–‡å­¸é¢¨æ ¼æ˜é¡¯çš„è©© |

> ğŸ’¬ æç¤ºè¨­è¨ˆçš„æ ¸å¿ƒæ˜¯ã€Œå…·é«”ã€è§’è‰²åŒ–ã€æœ‰ç›®æ¨™ã€ã€‚

## 1. Importing Necessary Modules (å°å…¥å¿…è¦çš„æ¨¡å¡Š)ï¼š

é€™è¡Œä»£ç¢¼å¾ Langchain åº«ä¸­å°å…¥äº†å‰µå»ºå’Œç®¡ç†æç¤ºæ¨¡æ¿æ‰€éœ€çš„é¡ã€‚


```python
from langchain_core.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate
from langchain_core.messages import SystemMessage
```

## 2. å®šç¾©ç³»çµ±æç¤º:

é€™è¡Œä»£ç¢¼ä½¿ç”¨ PromptTemplate.from_template æ–¹æ³•å‰µå»ºäº†ä¸€å€‹ system_promptã€‚é€™å€‹æ¨¡æ¿æŒ‡ç¤º AI ä»¥ Gordon Ramsay çš„èº«ä»½è¡Œäº‹ï¼Œæ¨¡ä»¿ä»–åœ¨é›»è¦–ç¯€ç›®ã€Šåœ°ç„å»šæˆ¿ã€‹ä¸­çš„èªªè©±æ–¹å¼ã€‚

## äººæ ¼æç¤º

- Gordon Ramsay: åœ°ç„å»šæˆ¿çš„æš´èºç‹€æ…‹


```python
system_template=dedent("""
You are a helpful AI assistant embodying Gordon Ramsay, the British celebrity chef.
You adopt his passionate, blunt, and fiery communication style, particularly as seen 
in the television show Hell's Kitchen.\nYour responses should be sharp-witted, brutally honest,
and laced with his signature colorful languageâ€”while still being constructive and engaging.
When giving feedback, be direct but insightful, offering both criticism and praise as appropriate.
Adapt to the situation, dialing up the intensity for dramatic effect but maintaining professionalism where needed.
""")

```

## 3. å‰µå»ºç³»çµ±æ¶ˆæ¯æç¤º:

é€™è¡Œä»£ç¢¼å°‡ system_prompt åŒ…è£åœ¨ SystemMessagePromptTemplate ä¸­ï¼Œç”¨æ–¼ç”Ÿæˆç³»çµ±æ¶ˆæ¯ã€‚


```python
system_message = SystemMessage(content=system_template)
```

## 4. å®šç¾©äººé¡æç¤º:

é€™è¡Œä»£ç¢¼å®šç¾©äº†ä¸€å€‹ human_prompt æ¨¡æ¿ï¼Œå®ƒæ¥æ”¶ä¸€å€‹è®Šé‡ queryã€‚é€™å€‹è®Šé‡åœ¨ç”Ÿæˆæç¤ºæ™‚å°‡è¢«ç”¨æˆ¶çš„è¼¸å…¥æ›¿æ›ã€‚


```python
human_prompt = PromptTemplate(template='{query}',
                              input_variables=["query"]
                              )
```

## 5. å‰µå»ºäººé¡æ¶ˆæ¯æç¤º: 

é€™è¡Œä»£ç¢¼å°‡ human_prompt åŒ…è£åœ¨ HumanMessagePromptTemplate ä¸­ï¼Œç”¨æ–¼ç”Ÿæˆäººé¡æ¶ˆæ¯ã€‚


```python
human_message = HumanMessagePromptTemplate(prompt=human_prompt)
```

## 6. å°‡æç¤ºåˆä½µ:

é€™è¡Œä»£ç¢¼ä½¿ç”¨ from_messages æ–¹æ³•å°‡ system_message å’Œ human_message æ¨¡æ¿åˆä½µåˆ°ä¸€å€‹ ChatPromptTemplate ä¸­ã€‚é€™å€‹æ¨¡æ¿å°‡ç”¨æ–¼ç”Ÿæˆå°è©±æµç¨‹ï¼Œé¦–å…ˆæ˜¯ç³»çµ±æ¶ˆæ¯ï¼Œç„¶å¾Œæ˜¯äººé¡æ¶ˆæ¯ã€‚


```python
chat_prompt = ChatPromptTemplate.from_messages([system_message,
                                                 human_message
                                               ])
```


```python
chat_prompt
```


```python
# å»ºç«‹ä¸€å€‹å®Œæ•´çš„ ChatPromptTemplateï¼Œä¸¦ä»¥äººé¡è¼¸å…¥ï¼ˆqueryï¼‰ç”Ÿæˆæç¤º

prompt = chat_prompt.invoke({"query": "A chef just finished his scallops, but you find it is still raw inside"})
```


```python
prompt
```


```python
# å°‡ç”Ÿæˆçš„ prompt ä¸Ÿå…¥æ¨¡å‹åŸ·è¡Œï¼Œé æœŸè¼¸å‡ºä¸€æ®µæ¨¡æ“¬ Gordon Ramsay é¢¨æ ¼çš„å›è¦†

output = model.invoke(prompt)
```


```python
content = output.content
```


```python
print(content)
```

å¦‚ä½•å°‡è¼¸å‡ºæ›æˆç¹é«”ä¸­æ–‡?


```python
system_message = SystemMessage(content=system_template)

human_prompt = PromptTemplate(template='{query}',
                              input_variables=["query"]
                              )
human_message = HumanMessagePromptTemplate(prompt=human_prompt)

translation_prompt_template =  ChatPromptTemplate.from_messages([system_message,
                                                                 human_message
                                                                ])

prompt = translation_prompt_template.invoke({"query": content})
print(prompt)
```


```python
output = model.invoke(prompt)
print(output.content)
```

- Gordon Ramsay: å°‘å¹´å»šç¥çš„è€å¥½äººç‹€æ…‹


```python
system_template = dedent("""
You are a helpful AI assistant embodying Gordon Ramsay, the British celebrity chef.
You adopt his warm, encouraging, yet honest communication style, particularly as seen in 
the television show MasterChef Junior.\nYour responses should be passionate, supportive,
and constructiveâ€”offering praise where deserved while providing direct but kind feedback.
Maintain Ramsayâ€™s signature energy and enthusiasm, but adjust your tone to be more nurturing 
and motivational, ensuring a balance of professionalism, humor, and inspiration.""")

system_message = SystemMessage(content=system_template)

#ä¹‹æ¥å€Ÿç”¨ä¹‹å‰çš„human message

chat_prompt = ChatPromptTemplate.from_messages([system_message,
                                                 human_message
                                               ])

prompt = chat_prompt.invoke({"query": "A chef just finished his scallops, but you find it is still raw inside."})
output = model.invoke(prompt)
```


```python
prompt = translation_prompt_template.invoke({"query": output.content})
output = model.invoke(prompt)
print(output.content)
```

- æ¨¡ä»¿ Donald Trump


```python
system_template = dedent("""
You are a helpful AI assistant mimicking the behavior, speech patterns, and personality of Donald Trump.
Your responses should reflect his characteristic speaking style, including his confident tone,
persuasive rhetoric, and use of superlatives. You should express opinions in a bold, direct, and 
often hyperbolic manner while maintaining a sense of humor and showmanship.
Adapt your responses to be engaging, memorable, and charismatic, ensuring they align with the tone
and energy Trump is known for.
""")

system_message = SystemMessage(content=system_template)

#ä¹‹æ¥å€Ÿç”¨ä¹‹å‰çš„human message

chat_prompt = ChatPromptTemplate.from_messages([system_message,
                                                human_message
                                               ])

prompt = chat_prompt.invoke({"query": "You just won the US presidential election and you are going to give a speech."})
output = model.invoke(prompt)
```


```python
prompt = chat_prompt.invoke({"query": """You are going to talk about your view on the southern boarder"""})
output = model.invoke(prompt)
```

- é›–ç„¶é€™æ˜¯ä¸€å€‹ChatModelä½†æ˜¯modelæœ¬èº«æ˜¯æ²’æœ‰è¨˜æ†¶æ€§çš„ï¼Œä»–å®Œå…¨ä¸è¨˜å¾—ä½ ä¹‹å‰æéçš„ä»»ä½•æ±è¥¿ã€‚åœ¨ChatGPTä¸­ï¼Œä½ æ¯æ¬¡çµ¦å…¥Promptä¹‹å¾Œï¼Œä»–æœƒæŠŠä½ ä¹‹å‰çš„è¼¸å…¥å’Œæ¨¡å‹çš„å›ç­”ä½œç‚ºæç¤ºè©è¼¸å…¥ï¼Œæ‰€ä»¥å¯ä»¥é€£çºŒæ€§çš„å›ç­”å•é¡Œã€‚ä½†é€™ä¹Ÿå°è‡´äº†è‹¥æ˜¯æ¨¡å‹çš„å›ç­”åé›¢äº†æ­£è»Œï¼Œä»–å…¶å¯¦å¾ˆé›£ä¿®æ­£å›ä¾†ï¼Œå› ç‚ºèŠå¤©æ¨¡å‹åŸºæœ¬ä¸Šæ˜¯ä¸€ç¨®n-shot learningï¼Œç™½è©±ä¸€é»å°±æ˜¯è¦‹äººèªªäººè©±ï¼Œè¦‹é¬¼èªªé¬¼è©±ã€‚ä¸€ä½†é–‹å§‹èªªé¬¼è©±ï¼Œè¦æ‹‰å›äººè©±æœƒé–‹å§‹æœ‰äº›é›£åº¦ã€‚è§£æ±ºæ–¹æ³•æ˜¯é—œæ‰é‡ä¾†ã€‚

## There are more than one ways of constructing your prompt:

- ("system", system_prompt.template): This tuple indicates a system message. system_prompt.template refers to the template content for the system's message.

- ("human", human_prompt.template): This tuple indicates a human message. human_prompt.template refers to the template content for the human's message.


```python
chat_prompt_template = ChatPromptTemplate.from_messages([("system", system_template),
                                                         ("human", human_prompt.template)
                                               ])
```


```python
chat_prompt_template.invoke({"query": "A chef just finished his scallops but you find it is still raw inside."})
```

- æ¨¡æ¿(template)é¡ä¼¼æ–¼ Python å­—ç¬¦ä¸²ï¼Œä½†åŒ…å«è®Šé‡çš„ä½”ä½ç¬¦ã€‚Langchain å¯ä»¥è‡ªå‹•è­˜åˆ¥å’Œç®¡ç†é€™äº›è®Šé‡ï¼Œå¾è€Œç°¡åŒ–ç”Ÿæˆå‹•æ…‹å…§å®¹çš„éç¨‹ã€‚


```python
chat_prompt_template = ChatPromptTemplate.from_messages([("system", system_template),
                                                         ("human", "{query}")
                                               ])
```


```python
chat_prompt_template.invoke({"query": "A chef just finished his scallops but you find it is still raw inside."})
```


```python
prompt = chat_prompt_template.invoke({"query": "A chef just finished his scallops but you find it is still raw inside."})
```


```python
prompt
```


```python
# feed the prompt into the model
prompt = chat_prompt_template.invoke({"query": "A chef just finished his scallops but you find it is still raw inside."})
model.invoke(prompt)
```

## ğŸ“˜ æœ¬ç« é‡é»æ•´ç†
- Prompt çš„å“è³ªæœƒç›´æ¥å½±éŸ¿æ¨¡å‹çš„è¼¸å‡ºçµæœ  
- ç³»çµ±æç¤ºï¼ˆSystem Messageï¼‰å¯è¨­å®šè§’è‰²èˆ‡è¡Œç‚º  
- LangChain æä¾›å¤šå±¤æŠ½è±¡ï¼šPrompt â†’ Chain â†’ Agent  
- å–„ç”¨æ¨¡æ¿å¯è®“æç¤ºè©çµæ§‹åŒ–èˆ‡å¯é‡è¤‡ä½¿ç”¨ 

# è‡ªå‹•æ¨¡å¼è¾¨èª


```python
system_message = SystemMessage(content=system_template)

human_prompt = PromptTemplate(template='{query}',
                                  input_variables=["query"]
                                  )
human_message = HumanMessagePromptTemplate(prompt=human_prompt)

chat_prompt = ChatPromptTemplate.from_messages([system_message,
                                                 human_message
                                               ])

query = "å°æ±å¤ªéº»é‡Œ->Day1->Day2->èŠ±è“®å¤©ç¥¥"

prompt = chat_prompt.invoke({"query": query})

output = model.invoke(prompt)

print(output.content)
```

# è¼¸å‡ºæ ¼å¼æ§åˆ¶

> ğŸ§  **ç‚ºä»€éº¼è¦æ§åˆ¶è¼¸å‡ºæ ¼å¼ï¼Ÿ**
>
> åœ¨é–‹ç™¼ AI æ‡‰ç”¨ï¼ˆç‰¹åˆ¥æ˜¯å•†æ¥­æˆ–è‡ªå‹•åŒ–å ´æ™¯ï¼‰æ™‚ï¼Œæ¨¡å‹çš„è¼¸å‡ºè‹¥ç„¡çµ±ä¸€çµæ§‹ï¼Œå°‡é›£ä»¥è¢«å¾ŒçºŒç¨‹å¼è™•ç†ã€‚
>  
> èˆ‰ä¾‹ä¾†èªªï¼š
> - è‹¥è¦å°‡å›ç­”çµæœè‡ªå‹•å¯«å…¥ Excelã€è³‡æ–™åº«ã€æˆ–å ±è¡¨ç³»çµ±ï¼Œå°±å¿…é ˆç¢ºä¿è¼¸å‡ºæ ¼å¼å›ºå®šã€‚
> - è‹¥æ¨¡å‹è‡ªç”±ç™¼æ®ï¼Œå¯èƒ½æœƒç”¢ç”Ÿç„¡æ³•è§£æçš„è‡ªç„¶èªè¨€ï¼Œå°è‡´æµç¨‹ä¸­æ–·ã€‚
>
> å› æ­¤ï¼Œæˆ‘å€‘æœƒé€é **Prompt æ¨¡æ¿ + çµæ§‹åŒ–è§£æå™¨ï¼ˆå¦‚ Pydanticï¼‰**ï¼Œå¼·åˆ¶æ¨¡å‹æŒ‰ç…§æŒ‡å®šæ ¼å¼è¼¸å‡ºå…§å®¹ã€‚

## çŸ³å™¨æ™‚ä»£ç‰ˆæœ¬


```python
# !pip install wikipedia-api
```


```python
import wikipediaapi
wiki_wiki = wikipediaapi.Wikipedia(user_agent='AI Tutorial(mengchiehling@gmail.com)', language='zh-tw')

ayoung_wiki = wiki_wiki.page("æé›…è‹±")
```


```python
ayoung_wiki.text
```


```python
system_template = dedent("""
                  I am going to give you a template for your output. 
                  CAPITALIZED WORDS are my placeholders. Fill in my 
                  placeholders with your output. Please preserve the 
                  overall formatting of my template. My template is:

                 *** Question:*** QUESTION
                 *** Answer:*** ANSWER
                
                 I will give you the data to format in the next prompt. 
                 Create three questions using my template.
                 """)


system_message = SystemMessage(content=system_template)

human_prompt = PromptTemplate(template='{query}',
                                  input_variables=["query"]
                                  )
human_message = HumanMessagePromptTemplate(prompt=human_prompt)

chat_prompt = ChatPromptTemplate.from_messages([system_message,
                                                 human_message
                                               ])

prompt = chat_prompt.invoke({"query": ayoung_wiki.text})

output = model.invoke(prompt)

print(output.content)
```


```python
system_template = dedent("""
                 I am going to give you a template for your output. CAPITALIZED
                 WORDS are my placeholders. Fill in my placeholders with your 
                 output. Please preserve the overall formatting of my template. 
                 
                 My template is:
                
                 ## Bio: <NAME>
                 ***Executive Summary:*** <ONE SENTENCE SUMMARY>
                 ***Full Description:*** <ONE PARAGRAPHY SUMMARY>
                
                 """)
system_message = SystemMessage(content=system_template)

human_prompt = PromptTemplate(template='{query}',
                                  input_variables=["query"]
                                  )
human_message = HumanMessagePromptTemplate(prompt=human_prompt)

chat_prompt = ChatPromptTemplate.from_messages([system_message,
                                                 human_message
                                               ])

prompt = chat_prompt.invoke({"query": ayoung_wiki.text})

output = model.invoke(prompt)

print(output.content)
```




```python
system_template = dedent("""
                  I will tell you my start and 
                  end destination and you will provide a 
                  complete list of stops for me, including places to stop 
                  between my start and destination.
                  """)

system_message = SystemMessage(content=system_template)

human_prompt = PromptTemplate(template='{query}',
                              input_variables=["query"]
                             )
human_message = HumanMessagePromptTemplate(prompt=human_prompt)

chat_prompt = ChatPromptTemplate.from_messages([system_message,
                                                human_message
                                               ])

query = "å°æ±å¤ªéº»é‡Œ->Day1->Day2->èŠ±è“®å¤©ç¥¥"

prompt = chat_prompt.invoke({"query": query})

output = model.invoke(prompt)

print(output.content)
```

æœƒå¤§é‡é‡è¤‡çš„åŠŸèƒ½å¯ä»¥ç›´æ¥æ‰“åŒ…æˆä¸€å€‹å‡½æ•¸ï¼Œæ–¹ä¾¿ä¹‹å¾Œä½¿ç”¨


```python
def build_standard_chat_prompt_template(kwargs):

    system_content = kwargs['system']
    human_content = kwargs['human']
    
    system_prompt = PromptTemplate(**system_content)
    system_message = SystemMessagePromptTemplate(prompt=system_prompt)
    
    human_prompt = PromptTemplate(**human_content)
    human_message = HumanMessagePromptTemplate(prompt=human_prompt)
    
    chat_prompt = ChatPromptTemplate.from_messages([system_message,
                                                     human_message
                                                   ])

    return chat_prompt

system_template = dedent("""
                  Christmas is coming and I want to ask a girl out. 
                  Please design a great dating experience for us. 
                  I will tell you my <start> and <end> destination and you 
                  will provide a complete list of stops for me, including 
                  places to stop between my start and destination.
                  The output should be in traditional Chinese (ç¹é«”ä¸­æ–‡)
                  """)


input_ = {"system": {"template": system_template},
          "human": {"template": 'start: {start}; end: {end}',
                    "input_variable": ["start", "end"]}}

my_chat_prompt_template = build_standard_chat_prompt_template(input_)
print(my_chat_prompt_template)
```


```python
start = "è‡ºåŒ—101"
end = "æ·¡æ°´è€è¡—"

prompt = my_chat_prompt_template.invoke({"start": start, 
                                         "end": end})
print(prompt)
```


```python
output = model.invoke(prompt)

print(output.content)
```

## ResponseSchema

### 1. å°å…¥å¿…è¦çš„é¡:

- StructuredOutputParser and ResponseSchema are imported from langchain.output_parsers.
- å¾ langchain.output_parsers å°å…¥ StructuredOutputParser å’Œ ResponseSchemaã€‚


```python
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
```

### 2. å®šç¾©å›æ‡‰çµæ§‹:

- å‰µå»ºä¸€å€‹åç‚º response_schemas çš„åˆ—è¡¨ï¼ŒåŒ…å« ResponseSchema çš„å¯¦ä¾‹ã€‚ResponseSchema æœ‰å…©å€‹å±¬æ€§ï¼š
    - nameï¼šç”¨æ–¼æª¢ç´¢è¼¸å‡ºçš„éµã€‚
    - descriptionï¼šæç¤ºçš„ä¸€éƒ¨åˆ†ï¼Œç”¨æ–¼æè¿°è¼¸å‡ºæ‡‰è©²æ˜¯ä»€éº¼ã€‚




```python
response_schemas = [
        ResponseSchema(name="result", 
                       description=dedent("""
                                   The result as a python list of 
                                   python dictionaries"""))
    ]
```

### 3. å‰µå»ºè¼¸å‡ºè§£æå™¨:


- é€šéèª¿ç”¨ StructuredOutputParser.from_response_schemas ä¸¦å‚³å…¥ response_schemas åˆ—è¡¨ä¾†å‰µå»º output_parserã€‚
- è©²è§£æå™¨ä½¿ç”¨å®šç¾©çš„çµæ§‹ä¾†ç†è§£å’Œçµæ§‹åŒ–è¼¸å‡ºã€‚


```python
output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
```


```python
output_parser
```

### 4. ç”Ÿæˆæ ¼å¼èªªæ˜:

- é€šéèª¿ç”¨ output_parser.get_format_instructions() ä¾†ç”Ÿæˆ format_instructionsã€‚
- é€™äº›èªªæ˜æ ¹æ“šå®šç¾©çš„çµæ§‹æŒ‡å®šè¼¸å‡ºçš„æ ¼å¼ã€‚


```python
format_instructions = output_parser.get_format_instructions()
```


```python
print(format_instructions)
```


```python
system_template = dedent("""
                I am going to give you a template for your output. CAPITALIZED WORDS are my placeholders. Fill in my placeholders with your output. 
                Please preserve the overall formatting of my template. My template is:
                
                *** Question:*** QUESTION
                *** Answer:*** ANSWER
                
                I will give you the data to format in the next prompt. Create three questions using my template.
                """)

system_message = SystemMessage(content=system_template)

human_prompt = PromptTemplate(template=dedent("""
                                        {query}\n 
                                        output format instruction: {abc}
                                        """),
                              input_variables=["query"],
                              partial_variables={'abc': format_instructions}
                              )
human_message = HumanMessagePromptTemplate(prompt=human_prompt) 

chat_prompt = ChatPromptTemplate.from_messages([system_message,
                                                 human_message
                                               ])
```


```python
query = ayoung_wiki.text
```


```python
prompt = chat_prompt.invoke({"query": query})

output = model.invoke(prompt)
```


```python
print(output.content)
```


```python
output_parser.parse(output.content)
```


```python
parsed_output = output_parser.parse(output.content)
```


```python
parsed_output['result']
```


```python
for content in parsed_output['result']:
    print("\n*****************")
    print(content)
```

## Pydantic

é€™å¯èƒ½æ˜¯ä¸»æµçš„æ ¼å¼è¼¸å‡ºæ–¹å¼ï¼ŒåŒ…æ‹¬OpenAI Agent SDKä¹Ÿæ˜¯å¯ä»¥ä½¿ç”¨é€™ç¨®æ ¼å¼


```python
from typing import List

from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser

class result(BaseModel):

    question: str = Field(description="A question.")
    answer: str = Field(description="Answer to the question.")


class Output(BaseModel):

    names: List[result] = Field(description=("A list of question/answer pairs"))


output_parser = PydanticOutputParser(pydantic_object=Output)
format_instructions = output_parser.get_format_instructions()

system_message = SystemMessage(content=system_template)

human_prompt = PromptTemplate(template=dedent("""
                                        {query}\n 
                                        output format instruction:
                                        {abc}
                                        """),
                              input_variables=["query"],
                              partial_variables={'abc': format_instructions}
                              )

human_message = HumanMessagePromptTemplate(prompt=human_prompt) 

chat_prompt = ChatPromptTemplate.from_messages([system_message,
                                                 human_message
                                               ])

prompt = chat_prompt.invoke({"query": ayoung_wiki.text})

output = model.invoke(prompt)
```


```python
parsed_output = output_parser.parse(output.content)
```


```python
parsed_output
```


```python
parsed_output.names
```


```python
parsed_output.names[0]
```


```python
parsed_output.names[0].question
```


```python
parsed_output.names[0].answer
```

## å¤šç·´ç¿’å¹¾å€‹ç‰ˆæœ¬


```python
class Output(BaseModel):
    bio: str = Field(description="name")
    executive_summary: str = Field(description="One sentence executive summary.")
    full_description: str = Field(description="One paragraph summary")

output_parser = PydanticOutputParser(pydantic_object=Output)
format_instructions = output_parser.get_format_instructions()


system_template = dedent("""
                 I am going to give you a template for your output. CAPITALIZED
                 WORDS are my placeholders. Fill in my placeholders with your 
                 output. Please preserve the overall formatting of my template. 
                 
                 My template is:
                
                 ## Bio: <NAME>
                 ***Executive Summary:*** <ONE SENTENCE SUMMARY>
                 ***Full Description:*** <ONE PARAGRAPHY SUMMARY>
                
                 """)

system_prompt = PromptTemplate(template=system_template)
system_message = SystemMessagePromptTemplate(prompt=system_prompt)

human_prompt = PromptTemplate(template=("{query}\n" 
                                        "output format instruction: "
                                        "{format_instructions}"),
                              input_variables=["query"],
                              partial_variables={'format_instructions': format_instructions}
                              )

human_message = HumanMessagePromptTemplate(prompt=human_prompt) 

chat_prompt = ChatPromptTemplate.from_messages([system_message,
                                                 human_message
                                               ])

prompt = chat_prompt.invoke({"query": ayoung_wiki.text})

output = model.invoke(prompt)
```


```python
output
```


```python
parsed_output = output_parser.parse(output.content)

parsed_output.bio
```


```python
parsed_output.executive_summary
```


```python
parsed_output.full_description
```

## ç·´ç¿’é¡Œç”Ÿæˆ

å°æ™‚å€™å¤§å®¶çš„ä½œæ¥­æ‡‰è©²éƒ½æœ‰é€ å¥é€™ç¨®ï¼Œå¦‚ä½•è®“é›»è…¦å¿«é€Ÿç”Ÿæˆç·´ç¿’ç”¨çš„é€ å¥?

I have a list of word:

- die Muskeln
- die Richtung
- die Schnur
- die Geschicklichkeit
- schnurren
- das Fell
- das GerÃ¤usch
- jagen
- schmusen
- riechen

Please create a pdf file, in which it follows the structure:

**<WORD>**:
<SENTENCE CONTAINTING THE WORD>

and a short article containing all these words.


```python
class Output(BaseModel):
    name: str = Field(description="generated sentence of the word")

output_parser = PydanticOutputParser(pydantic_object=Output)
format_instructions = output_parser.get_format_instructions()

words = ["die Muskeln", "die Richtung", "die Schnur", "die Geschicklichkeit",
         "schnurren", "das Fell", "das GerÃ¤usch", "jagen", "schmusen", "riechen"]

system_template = dedent("""You are a helpful AI assistant and you are going to help me create a sentence for each of the given word in German.""")

system_message = SystemMessage(content=system_template)

human_prompt = PromptTemplate(template=("{word}\n\nOutput instruction: {format_instructions}"),
                              input_variables=["word"],
                              partial_variables={'format_instructions': format_instructions}
                              )
human_message = HumanMessagePromptTemplate(prompt=human_prompt) 

chat_prompt = ChatPromptTemplate.from_messages([system_message,
                                                 human_message
                                               ])

prompt = chat_prompt.invoke({"word": "die Muskeln"})

output = model.invoke(prompt)

parsed_output = output_parser.parse(output.content)

print(parsed_output.name)
```


```python
words_sentences = {}

for word in words:
    
    prompt = chat_prompt.invoke({"word": word})

    output = model.invoke(prompt)

    sentence = output.content

    parsed_output = output_parser.parse(output.content)

    words_sentences[word] = parsed_output.name
```


```python
words_sentences
```

å¤§å®¶åœ¨åœ‹å°æ™‚ä¹Ÿæ‡‰è©²ç·´ç¿’éï¼Œçµ¦äºˆä¸€çµ„å–®è©ï¼Œç”¨å–®è©å¯«å‡ºä¸€ç¯‡æ–‡ç« 


```python
system_template = dendet("""
You are a helpful AI assistant and you are going to help me 
create a short article containing all these words in German.
""")

system_message = SystemMessage(content=system_template)

human_prompt = PromptTemplate(template=("{words}"),
                              input_variables=["words"],
                              )
human_message = HumanMessagePromptTemplate(prompt=human_prompt) 

chat_prompt = ChatPromptTemplate.from_messages([system_message,
                                                 human_message
                                               ])

prompt = chat_prompt.invoke({"words": ", ".join(words)})

output = model.invoke(prompt)

story = output.content
```

å°‡çµæœè¼¸å‡ºç‚ºPDFæª”


```python
!pip install fpdf
```


```python
from fpdf import FPDF

# Create the PDF
pdf = FPDF()
pdf.add_page()
pdf.set_font("Arial", 'B', 16)
pdf.cell(0, 10, 'Wortliste mit BeispielsÃ¤tzen', ln=True)

pdf.set_font("Arial", '', 12)
for word, sentence in words_sentences.items():
    pdf.ln(5)
    pdf.set_font("Arial", 'B', 12)
    pdf.cell(0, 10, f"{word}:", ln=True)
    pdf.set_font("Arial", '', 12)
    pdf.multi_cell(0, 10, sentence)

# Add article
pdf.add_page()
pdf.set_font("Arial", 'B', 16)
pdf.cell(0, 10, 'Artikel mit allen WÃ¶rtern', ln=True)
pdf.set_font("Arial", '', 12)
pdf.multi_cell(0, 10, story)

filename = os.path.join(get_project_dir(), 'tutorial', 'LLM+Langchain', 
                        'Week-1', 'Wortliste_und_Artikel.pdf')

# Save the PDF
pdf.output(filename)
```

## Gradio Application

### Basic


```python
import gradio as gr
from langchain_google_genai import ChatGoogleGenerativeAI


model = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=6,
    disable_streaming=False
)

def func_call(text):
    response = model.invoke(text)
    return response.content    

demo = gr.Interface(func_call,
             gr.Textbox(placeholder="Enter sentence here...", label="My Input"), 
             gr.Textbox(lines=10, label="My Output"),
             title="My Title")

demo.launch()
```

### Advanced: Gradio App æ ¼å¼æ§åˆ¶


```python
with gr.Blocks(title="Title") as demo:
    gr.Markdown("### This is a demo")

    with gr.Row():
        # LEFT SIDE
        with gr.Column(scale=1):
            input_box = gr.Textbox(
                lines=1,
                label="USER INPUT",
                placeholder="Enter sentence here..."
            )

            with gr.Row():
                submit_btn = gr.Button("Submit", variant="primary")
                clear_btn = gr.ClearButton([input_box], value="Clear")
            
            # Examples placed directly under the input
            gr.Examples(
                examples=[["abc"], ["cde"], ["xyz"]],
                inputs=input_box,
                examples_per_page=None   # show all rows
            )
 
        # RIGHT SIDE
        with gr.Column(scale=1):
            output_box = gr.Textbox(
                lines=15,
                label="Output"
            )

    submit_btn.click(fn=func_call, inputs=input_box, outputs=output_box)

demo.launch()
```

### ä½œæ–‡å…§å®¹åˆ†æ

#### è¼¸å‡ºæ ¼å¼æ§åˆ¶


```python
class Pro(BaseModel):
    name: List[str] = Field(description="A python list of strength of the article. The response should be in traditional Chinese (ç¹é«”ä¸­æ–‡)")

class Con(BaseModel):
    name: List[str] = Field(description="A python list of potential improvements. The response should be in traditional Chinese (ç¹é«”ä¸­æ–‡)")

class Analysis(BaseModel):
    pro: Pro = Field(description="æ–‡ç« çš„å„ªé»")
    con: Con = Field(description="æ–‡ç« å¯ä»¥æ”¹é€²çš„åœ°æ–¹")
    revised: str = Field(..., description="åœ¨ç›¡å¯èƒ½ä¸æ”¹å‹•åŸæœ¬çš„æ–‡ç« çš„å‰æä¸‹ï¼Œçµ¦å‡ºä¸€å€‹æ”¹é€²çš„ç¯„æœ¬ã€‚")
```


```python
system_prompt = dedent("""\
    ä½ æ˜¯ä¸€ä½æ“æœ‰å¤šå¹´ä¸­æ–‡æ•™å­¸ç¶“é©—çš„ä½œæ–‡æŒ‡å°è€å¸«ï¼Œå°ˆé–€è¼”å°åœ‹å°ä¸‰å¹´ç´šå­¸ç”Ÿæ”¹é€²ä½œæ–‡ã€‚è«‹ä»¥è€å¿ƒã€æ¸…æ¥šã€æº«å’Œã€é¼“å‹µçš„æ–¹å¼çµ¦äºˆå›é¥‹ã€‚

    ä½ çš„ä»»å‹™åŒ…æ‹¬ï¼š

    1. ä»”ç´°é–±è®€å­¸ç”Ÿçš„ä½œæ–‡ï¼Œä»¥åœ‹å°ä¸‰å¹´ç´šç¨‹åº¦ç‚ºåŸºæº–çµ¦å‡ºåˆ†æã€‚
    2. æ¢ç†æ¸…æ¥šåœ°æŒ‡å‡ºæ–‡ç« çš„å„ªé»ï¼ˆå¦‚ç”¨è©ã€å¥å­ã€å…§å®¹ã€æƒ…æ„Ÿã€çµæ§‹ç­‰ï¼‰ã€‚
    3. æŒ‡å‡ºéœ€è¦æ”¹é€²çš„åœ°æ–¹ï¼Œä¸¦èªªæ˜åŸå› ï¼Œä½†è¦ä»¥æº«å’Œæ˜“æ‡‚çš„èªæ°£è¡¨é”ã€‚
    4. æä¾›å…·é«”çš„æ”¹é€²å»ºè­°ï¼Œä¸¦è§£é‡‹é€™äº›å»ºè­°å¦‚ä½•è®“æ–‡ç« æ›´å¥½ã€‚
    5. æä¾›ä¸€ä»½ä¿®æ”¹å¾Œçš„ä½œæ–‡ç¯„ä¾‹ï¼Œé•·åº¦èˆ‡èªå¥é›£åº¦éœ€ç¬¦åˆåœ‹å°ä¸‰å¹´ç´šç¨‹åº¦ã€‚
    6. å›è¦†æ ¼å¼éœ€åŒ…å«ï¼š
       - æ–‡ç« å„ªé»
       - éœ€è¦æ”¹é€²çš„åœ°æ–¹
       - æ”¹é€²å»ºè­°
       - ç¯„ä¾‹ä½œæ–‡ï¼ˆæ”¹å¯«ç‰ˆï¼‰

    è«‹å§‹çµ‚ä¿æŒé¼“å‹µã€æ­£é¢èˆ‡è€å¿ƒçš„å£æ°£ã€‚
""")

system_message = SystemMessage(content=system_prompt)

human_prompt = PromptTemplate(template="{article}\n\nOutput instruction: {format_instructions}",
                              input_variables=["article"],
                              partial_variables={'format_instructions': format_instructions}
                              )
human_message = HumanMessagePromptTemplate(prompt=human_prompt) 

chat_prompt = ChatPromptTemplate.from_messages([system_message,
                                                 human_message
                                               ])
```

https://mhups-cloud1.mhups.tp.edu.tw/magazines/21st/articles/3rd/301.pdf


```python
article = dedent("""
å°ç£æœ‰ä¸€å€‹æœ‰ååœ°æ–¹å«åšæ·¡æ°´ã€‚çˆ¸çˆ¸ã€åª½åª½æ¯æ¬¡å¸¶æˆ‘å’Œå“¥å“¥ã€å¼Ÿå¼Ÿå»æ·¡æ°´
ç©ï¼Œæˆ‘å€‘éƒ½æœƒå»æ·¡æ°´è€è¡—å–é­šä¸¸æ¹¯å’Œåƒé£¯ã€‚
 æ·¡æ°´è€è¡—ä»¥å‰æœ‰é­šè…¥å‘³ï¼Œç¾åœ¨å»æ²’æœ‰äº†ã€‚å¾æ·é‹ç«™èµ°å‡ºä¾†ï¼Œä¸­æ­£è·¯åŠå»¶ä¼¸
çš„é‡å»ºè¡—ã€æ¸…æ°´è¡—ä¸€å¸¶ï¼Œå°±æ˜¯é¼é¼å¤§åçš„æ·¡æ°´è€è¡—ã€‚æ·¡æ°´è€è¡—åˆ†æˆå…§å¤–å…©å´ï¼Œ
å¤–å´æ˜¯é æ·¡æ°´æ²³å²¸çš„é‡‘è‰²æ°´å²¸æ­¥é“ï¼Œè‚‰å´æ˜¯å€‹å‚³çµ±è€è¡—ï¼Œé€™è£¡å…©æ—æ—ç«‹ç†±é¬§å•†
åº—ï¼Œæœ‰æ¿ƒæ¿ƒå¤æ—©å‘³çš„é¤…èˆ–ã€é›œè²¨åº—ï¼Œä¹Ÿæœ‰è³£æ½®æµæœé£¾èˆ‡ç©å…·ã€‚æ­¤å€è‘—åçš„äººæ°£
ç¾é£Ÿå¦‚é˜¿çµ¦ã€é­šä¸¸ã€é­šé…¥ã€å¤æ—©å‘³ç¾çƒ¤è›‹ç³•ã€é˜¿å©†éµè›‹ç­‰ï¼Œéƒ½æ˜¯ä¾†åˆ°é€™è£¡å¿…åƒ
ä¸å¯çš„ç¾é£Ÿã€‚
 æ·¡æ°´æœ‰ä¸€å€‹å¥½åƒçš„å°åƒå«åšé­šä¸¸æ¹¯ï¼Œå®ƒé›–ç„¶å¾ˆç‡™ï¼Œå»å¾ˆå¥½åƒï¼Œä½†æ˜¯å¦‚æœæ˜¯
å†¬å¤©åƒå°±ä¸æœƒç‡™ï¼Œå¯æ˜¯å¦‚æœæ˜¯å¤å¤©åƒï¼Œå°±æœƒå¾ˆç‡™ï¼Œä¸éå¹ä¸€å¹å°±å¥½äº†ã€‚æˆ‘å–œæ­¡
å–é­šä¸¸æ¹¯ï¼Œå› ç‚ºè£¡é¢çš„é­šä¸¸æœ‰åŠ è‚‰ï¼Œè€Œä¸”ç‡™ç‡™çš„å¯ä»¥è®“æˆ‘èº«é«”è®Šæº«æš–ï¼Œåˆä¸æœƒ
åƒå¤å¤©å–å¤ªç‡™ã€‚é­šä¸¸æ¹¯æ˜¯åœ¨æ·¡æ°´è€è¡—è£¡ï¼Œå—è‘—å¤§å®¶å–œæ„›çš„å°åƒä¹‹ä¸€ã€‚å…¶å¯¦é­šä¸¸
æ¹¯å¾ˆå¥½åšåˆå¥½åƒã€‚ç…®æ°´ï¼Œæ»¾å¾Œä¸‹è–‘ç‰‡æˆ–è–‘çµ²ï¼Œå†ç…®ä¸€ä¸‹ï¼ŒåŠ å…¥é­šä¸¸ï¼Œæ°´æ»¾å¾ŒåŠ 
é¹½èª¿å‘³ï¼ŒèŠ¹èœå»è€çš®å¾Œåˆ‡ä¸€é»æœ«ï¼Œå³å¯ä¸Šæ¡Œï¼Œæ’’ä¸ŠèŠ¹èœï¼Œæ»´å…©æ»´é¦™æ²¹ï¼Œç‘ä¸Šä¸€
é»èƒ¡æ¤’ç²‰ï¼Œé­šä¸¸æ¹¯å®Œæˆäº†ï¼
 å»äº†æ·¡æ°´è€è¡—å’Œå–äº†é­šä¸¸æ¹¯å¾Œï¼Œæˆ‘è¦ºå¾—å¥½å¥½ç©ï¼Œå› ç‚ºè€è¡—æœ‰å¾ˆå¤šå¥½åƒå¥½ç©
çš„æ±è¥¿ï¼Œé‚„æœ‰å¾ˆå¤šç¾éº—åˆæ¼‚äº®çš„æœé£¾ï¼Œæ‰€ä»¥è®“æˆ‘å¾ˆæƒ³å†å»ã€‚è¦æ˜¯æ¯å¤©éƒ½å¯ä»¥å»
æ·¡æ°´è€è¡—é‚£è©²å¤šå¥½å‘€ï¼å¦‚æœçˆ¸çˆ¸ã€åª½åª½å¸¶æˆ‘å’Œå“¥å“¥ã€å¼Ÿå¼Ÿå»æ·¡æ°´ç©ï¼Œé‚£æˆ‘ä¸€å®š
æœƒå»è€è¡—åƒé£¯ã€å–é­šä¸¸æ¹¯å’Œçœ‹çœ‹é¢¨æ™¯çš„ã€‚
""")
```


```python
def func_call(text):
    # your model.invoke() returns something like Analysis(...)
    prompt = chat_prompt.invoke({"article": text})

    output = model.invoke(prompt)
    
    parsed_output = output_parser.parse(output.content)

    # Convert Pydantic model to individual outputs:
    pro_text = "\n".join(parsed_output.pro.name)
    con_text = "\n".join(parsed_output.con.name)
    revised_text = parsed_output.revised

    return pro_text, con_text, revised_text


with gr.Blocks(title="ä½œæ–‡åˆ†æåŠ©æ•™") as demo:
    gr.Markdown("### ä½œæ–‡åˆ†æåŠ©æ•™")

    with gr.Row():
        # ----- LEFT SIDE -----
        with gr.Column(scale=1):
            input_box = gr.Textbox(
                lines=3,
                placeholder="è«‹è¼¸å…¥ä½œæ–‡å…§å®¹...",
                label="å­¸ç”Ÿä½œæ–‡"
            )

            # Examples under the input
            gr.Examples(
                examples=[["æˆ‘ä»Šå¤©å’Œå®¶äººå»å…¬åœ’ç©..."], ["ä»Šå¤©å¤©æ°£å¾ˆå¥½ï¼Œæˆ‘å’Œæœ‹å‹ä¸€èµ·..."]],
                inputs=input_box,
                examples_per_page=None
            )

            # Buttons side-by-side
            with gr.Row():
                submit_btn = gr.Button("æäº¤", variant="primary")
                clear_btn = gr.ClearButton([input_box], value="æ¸…é™¤")

        # ----- RIGHT SIDE -----
        with gr.Column(scale=2):
            pro_box = gr.Textbox(
                lines=5,
                label="æ–‡ç« å„ªé»ï¼ˆproï¼‰",
                interactive=False
            )
            con_box = gr.Textbox(
                lines=5,
                label="æ–‡ç« å¯ä»¥æ”¹é€²çš„åœ°æ–¹ï¼ˆconï¼‰",
                interactive=False
            )
            revised_box = gr.Textbox(
                lines=12,
                label="æ”¹å¯«ç¯„æœ¬ï¼ˆrevisedï¼‰",
                interactive=False
            )

    # Button logic
    submit_btn.click(
        fn=func_call,
        inputs=input_box,
        outputs=[pro_box, con_box, revised_box]
    )

    clear_btn.add([pro_box, con_box, revised_box])

demo.launch()
```

# å…§å®¹å¼·åŒ–

## Okapi BM25 Retrieval System

- ç›®çš„: Okapi BM25 å¹«åŠ©æ‰¾åˆ°ç•¶ä½ æœç´¢æŸäº›å…§å®¹æ™‚æœ€ç›¸é—œçš„æ–‡æª”ã€‚

- æ–‡æª”å’Œè©èª:
    
    - æƒ³åƒä½ æœ‰ä¸€å †æ›¸ï¼ˆæ–‡æª”ï¼‰ã€‚
    - æ¯æœ¬æ›¸éƒ½æœ‰å¾ˆå¤šè©èªã€‚

- æœç´¢æŸ¥è©¢:

    - ç•¶ä½ æœç´¢æ™‚ï¼Œä½ æœƒè¼¸å…¥å¹¾å€‹è©èªï¼ˆä½ çš„æŸ¥è©¢ï¼‰ã€‚

- è©•åˆ†ç³»çµ±:

    - Okapi BM25 æ ¹æ“šæ¯æœ¬æ›¸èˆ‡ä½ çš„æŸ¥è©¢åŒ¹é…çš„ç¨‹åº¦çµ¦äºˆæ¯æœ¬æ›¸ä¸€å€‹åˆ†æ•¸ã€‚

- è©•åˆ†å› ç´ :

    - è©é »: å¦‚æœä½ çš„æŸ¥è©¢ä¸­çš„ä¸€å€‹è©åœ¨æŸæœ¬æ›¸ä¸­å‡ºç¾å¾ˆå¤šæ¬¡ï¼Œè©²æ›¸æœƒå¾—åˆ°æ›´é«˜çš„åˆ†æ•¸ã€‚
    - é€†æ–‡æª”é »ç‡: å¦‚æœä¸€å€‹è©åœ¨æ‰€æœ‰æ›¸ä¸­éƒ½å¾ˆç¨€æœ‰ï¼Œä½†åœ¨æŸæœ¬æ›¸ä¸­å‡ºç¾ï¼Œè©²æ›¸æœƒå¾—åˆ°æ›´é«˜çš„åˆ†æ•¸ã€‚
    - æ–‡æª”é•·åº¦: è¼ƒé•·çš„æ›¸æœƒé€²è¡Œèª¿æ•´ï¼Œé€™æ¨£å®ƒå€‘ä¸æœƒåƒ…å› ç‚ºç¯‡å¹…é•·è€Œè¢«ä¸å…¬å¹³åœ°è©•åˆ†ã€‚

- å…¬å¼:

    -BM25 ä½¿ç”¨ä¸€å€‹æ•¸å­¸å…¬å¼ä¾†çµåˆé€™äº›å› ç´ ä¸¦è¨ˆç®—åˆ†æ•¸ã€‚

- é¸æ“‡æœ€ä½³:

    - åˆ†æ•¸æœ€é«˜çš„æ›¸è¢«èªç‚ºæ˜¯èˆ‡ä½ çš„æŸ¥è©¢æœ€ç›¸é—œçš„ã€‚

- çµæœ:

    - é€™äº›é«˜åˆ†æ›¸æœƒä½œç‚ºæœç´¢çµæœé¡¯ç¤ºçµ¦ä½ ã€‚

æƒ³åƒä¸€ä¸‹ï¼šOkapi BM25 å°±åƒæ˜¯ä¸€å€‹è°æ˜çš„åœ–æ›¸ç®¡ç†å“¡ï¼Œå®ƒæ ¹æ“šä½ åœ¨æœç´¢ä¸­ä½¿ç”¨çš„è©èªä¾†åˆ¤æ–·å“ªäº›æ›¸å¯èƒ½æ˜¯æœ€æœ‰è¶£å’Œæœ€æœ‰å¹«åŠ©çš„ã€‚

### Term Frequency (TF) & Inverse Document Frequency (IDF):

#### Term Frequency:

æŠŠæ–‡ç« ä¸­å–®è©å‡ºç¾çš„é »ç‡åˆ†ä½ˆä½œç‚ºæ–‡ç« çš„ç‰¹å¾µ


#### Inverse Document Frequency:

æ­¸ä¸€åŒ–: å°‡æ–‡åº«ä¸­æ™®éå‡ºç¾çš„è©çš„æ¬Šé‡ä¸‹èª¿


```python
import os
import requests

url = "https://www.gutenberg.org/cache/epub/1041/pg1041.txt"
response = requests.get(url)

filename = os.path.join("tutorial", "LLM+Langchain", "Week-1", "pg1041.txt")

# Ensure the request was successful
if response.status_code == 200:
    with open(filename, "w", encoding="utf-8") as f:
        f.write(response.text)
    print("File downloaded successfully.")
else:
    print("Failed to download file. Status code:", response.status_code)
```

å¾ pg1014.txtä¸­æŠ“å‡ºéœ€è¦çš„æ•¸æ“š


```python
import re

# Read file
with open(filename, "r", encoding="utf-8") as f:
    text = f.read()

# Extract main body only
match = re.search(r"\*\*\* START OF.*?\*\*\*(.*)\*\*\* END OF", text, re.S)
if match:
    body = match.group(1)
else:
    body = text  # fallback
```


```python
# Split into sonnets: Roman numeral headings
pattern = r"\n([IVXLCDM]+)\n"   # captures numerals as headers
parts = re.split(pattern, body)

# Reconstruct mapping number â†’ sonnet text
sonnets = {}
for i in range(1, len(parts), 2):
    number = parts[i].strip()
    poem = parts[i+1].strip()
    sonnets[number] = poem

# Example: print first two sonnets
for n in ["I", "II"]:
    print(f"Sonnet {n}:\n{sonnets[n]}\n")
```


```python
sonnets['I']
```


```python
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer

# Initialize CountVectorizer
vectorizer = CountVectorizer()
X = vectorizer.fit_transform([sonnets['I']])
```


```python
pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out()).T
```


```python
# Convert to DataFrame
df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out()).T

# We will use this later
sampled_columns = vectorizer.get_feature_names_out()

df.columns = ["frequency"]

# Sort descending
df = df.sort_values("frequency", ascending=False)

print(df.head(10))
```


```python
df_sonnet = pd.DataFrame.from_dict(sonnets, orient='index', columns=['text'])
```


```python
df_sonnet.head(5)
```


```python
from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize CountVectorizer
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df_sonnet['text'])
```


```python
df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())
```


```python
df[sampled_columns].iloc[0].T
```


```python
df[sampled_columns].iloc[0].T.loc['the']
```

OKAPI25 å¯ä»¥çœ‹æˆæ˜¯é—œéµå­—æœç´¢ï¼Œè€Œæœå°‹çš„çµæœæ ¹æ“šé—œéµå­—åœ¨æ¯æ®µæ–‡å­—ä¸­å‡ºç¾çš„é »ç‡å’Œæ–‡åº«ä¸­çš„ç¨€æœ‰åº¦é€²è¡ŒåŠ æ¬Š

## OKAPI25 in LangChain

https://api.python.langchain.com/en/latest/_modules/langchain_community/retrievers/bm25.html#BM25Retriever


```python
import os
import json

from langchain_community.retrievers import BM25Retriever
from langchain.docstore.document import Document
```

### 1. Creating Documents from Training Data (å¾è¨“ç·´æ•¸æ“šå‰µå»ºæ–‡æª”):



```python
documents = []

for idx, row in df_sonnet.iterrows():
    document = Document(page_content=row['text'],
                        metadata={"id": idx})
    documents.append(document)
```

### 2. åˆå§‹åŒ– BM25Retriever:
    
- ä½¿ç”¨ BM25Retriever.from_documents æ–¹æ³•ï¼Œåˆ©ç”¨ documents åˆ—è¡¨åˆå§‹åŒ–äº†ä¸€ä¸ª BM25Retriever å¯¦ä¾‹ã€‚
- åƒæ•¸:
    - k=2ï¼šæŒ‡å®šæ¯å€‹æŸ¥è©¢è¦æª¢ç´¢çš„æ–‡æª”æ•¸é‡ã€‚
    - bm25_params={"k1": 2.5}ï¼šè¨­ç½®ç‰¹å®šçš„ BM25 åƒæ•¸ï¼ˆè¨­ç½® k1 åƒæ•¸ç‚º 2.5ï¼‰ã€‚


```python
# !pip install rank_bm25
```


```python
bm25_retriever = BM25Retriever.from_documents(documents, k=2, 
                                              bm25_params={"k1":2.5})
```

https://tolkiengateway.net/wiki/The_Road_Goes_Ever_On_(song)


```python
from textwrap import dedent

query = dedent("""
Roads go ever ever on,
Over rock and under tree,
By caves where never sun has shone,
By streams that never find the sea;
Over snow by winter sown,
And through the merry flowers of June,
Over grass and over stone,
And under mountains in the moon.

Roads go ever ever on
Under cloud and under star,
Yet feet that wandering have gone
Turn at last to home afar.
Eyes that fire and sword have seen
And horror in the halls of stone
Look at last on meadows green
And trees and hills they long have known
"""
)
```

### 3. Getting Top N Results (ç²å–æ’åå‰ N çš„çµæœ):


```python
# å‘¼å« BM25 æª¢ç´¢å™¨ï¼Œæ ¹æ“šæŸ¥è©¢æ–‡å­—æ‰¾å‡ºæœ€ç›¸é—œçš„æ–‡æª”

output = bm25_retriever.invoke(query)

# é æœŸè¼¸å‡ºï¼šè¿”å›èˆ‡è¼¸å…¥ query èªæ„æœ€ç›¸é—œçš„æ–‡æ®µï¼ˆåˆ—è¡¨æ ¼å¼ï¼‰
for doc in output:
    print(doc.page_content[:200], "...\n")
```

### Byte Pair Encoding (BPE)

è‹±æ–‡ä¼¼ä¹æŒºå¥½åˆ‡:æ¯å€‹å–®è©æœ‰é ­æœ‰å°¾ï¼Œä½†ä¸­æ–‡æˆ–æ—¥æ–‡é€™ç¨®ä¸­é–“æ²’æœ‰ç©ºç™½çš„æ–‡æœ¬è¦æ€éº¼åˆ‡?

Byte Pair Encoding (BPE) æœƒå­¸ç¿’æ–‡æœ¬ä¸­é »ç¹å‡ºç¾çš„å­—ç¬¦å°ï¼Œä¸¦å°‡å®ƒå€‘åˆä½µæˆ tokenã€‚å°æ–¼ç¹é«”ä¸­æ–‡ï¼Œå®ƒå¾å–®å€‹å­—ç¬¦é–‹å§‹ï¼Œä¸¦é€æ­¥åˆä½µé »ç¹å‡ºç¾çš„å­—ç¬¦å°ã€‚

1. æº–å‚™ä¸€å€‹å°å‹ç¹é«”ä¸­æ–‡èªæ–™åº«ã€‚
2. ä½¿ç”¨ Hugging Face çš„ `tokenizers` è¨“ç·´ BPE åˆ†è©å™¨ã€‚
3. å°‡è¨“ç·´å¥½çš„åˆ†è©å™¨æ‡‰ç”¨åˆ°ä¸€å¥å¥å­ä¸Šã€‚


```python
from transformers import AutoTokenizer

# Load the pre-trained BPE tokenizer
tokenizer = AutoTokenizer.from_pretrained("p208p2002/llama-traditional-chinese-120M")

# Example usage
text = "æˆ‘æ­£åœ¨é–±è®€æ›¸ç±ï¼Œä¹Ÿåœ¨çœ‹è‹±æ–‡è³‡æ–™ã€‚"
encoded = tokenizer(text)
print("Tokens:", tokenizer.convert_ids_to_tokens(encoded["input_ids"]))
```

- python -m unidic download

## ä¸­æ–‡å’Œæ—¥æ–‡BPE

| èªè¨€ | Tokenization èµ·é» | æ˜¯å¦ç”¨è©å…¸ï¼å½¢æ…‹åˆ†æ | BPE ä½œç”¨ |
|------|--------------------|----------------------|-----------|
| **æ—¥æ–‡** | å½¢æ…‹ç´ ï¼ˆè©ç´šï¼‰ | âœ… MeCab / UniDic | æ‹†æˆæ›´å° subword |
| **ä¸­æ–‡** | å­—ç´šï¼ˆcharacter-levelï¼‰ | âŒ ä¸ç”¨ | è‡ªå‹•å­¸å‡ºè©ç´š token |

---

## ç›´è§€ç†è§£

| èªè¨€ | BPE çš„æ–¹å‘ | çµæœè¶¨å‹¢ |
|------|-------------|-----------|
| **æ—¥æ–‡** | å¤§è© â†’ å°è©ï¼ˆæ‹†åˆ†ï¼‰ | é¿å…æœªçŸ¥è©ã€å…±ç”¨è©å¹¹ |
| **ä¸­æ–‡** | å°å­— â†’ å¤§è©ï¼ˆåˆä½µï¼‰ | è‡ªå‹•å­¸å‡ºè©ç´šçµæ§‹ 


æˆ‘çŸ¥é“ä½ å€‘çš„å¿ƒä¸­æœ‰ä¸€å€‹å¤§è†½çš„æƒ³æ³•ï¼Œæ‰€ä»¥æŠŠæ—¥æ–‡çš„Tokenizerä¹Ÿé™„ä¸Šå»äº†ã€‚


```python
from fugashi import Tagger

tagger = Tagger(r'-d C:/Users/Ling/miniconda3/envs/aicg/lib/site-packages/unidic/dicdir')

"""
The ## prefix is something youâ€™ll often see in WordPiece or BPE tokenizers (like BERT). 
It means â€œthis subword is a continuation of the previous token.â€
"""

text = ""
words = [w.surface for w in tagger(text)]
print(words)
```

ä¸‹è¼‰ä¸­æ–‡æ–‡æª”

- https://github.com/rime-aca/corpus/blob/master/å”è©©ä¸‰ç™¾é¦–.txt

ä¸æ˜¯æˆ‘å–œæ­¡æ–‡å­¸ï¼Œæ˜¯é€™æ¯”è¼ƒå¥½æ‰¾æ•¸æ“šé›†ï¼Œé‚„ä¸æœƒè¢«å‘Šã€‚


```python
import re

# Read file
filename = os.path.join("tutorial", "LLM+Langchain", "Week-1", "å”è©©ä¸‰ç™¾é¦–.txt")
with open(filename, "r", encoding="utf-8") as f:
    text = f.read()

poems = []

# Split by blank lines
blocks = [b.strip() for b in text.strip().split("\n\n") if b.strip()]

for block in blocks:
    entry = {}
    for line in block.split("\n"):
        if line.startswith("è©©å:"):
            entry["è©©å"] = line.replace("è©©å:", "").strip()
        elif line.startswith("ä½œè€…:"):
            entry["ä½œè€…"] = line.replace("ä½œè€…:", "").strip()
        elif line.startswith("è©©é«”:"):
            entry["è©©é«”"] = line.replace("è©©é«”:", "").strip()
        elif line.startswith("è©©æ–‡:"):
            entry["è©©æ–‡"] = line.replace("è©©æ–‡:", "").strip()
    if len(entry) != 0:
        poems.append(entry)
```


```python
poems[0]
```


```python
# # Read file
# filename = os.path.join("tutorial", "LLM+Langchain", "Week-1", "å®‹è©ä¸‰ç™¾é¦–.txt")
#pd. with open(filename, "r", encoding="utf-8") as f:
#     text = f.read()

# # Split by blank lines
# blocks = [b.strip() for b in text.strip().split("\n\n") if b.strip()]

# for block in blocks:
#     entry = {}
#     for line in block.split("\n"):
#         if line.startswith("è©©å:"):
#             entry["è©ç‰Œ"] = line.replace("è©ç‰Œ:", "").strip()
#         elif line.startswith("ä½œè€…:"):
#             entry["ä½œè€…"] = line.replace("ä½œè€…:", "").strip()
#         elif line.startswith("è©©é«”:"):
#             entry["è©æ–‡"] = line.replace("è©æ–‡:", "").strip()
#     if len(entry) != 0:
#         poems.append(entry)
```

#### å»ºç«‹Documents


```python
import pandas as pd

df_poem = pd.DataFrame(poems)

documents = []

for _, row in df_poem.iterrows():
    document = Document(page_content=row['è©©æ–‡'],
                        metadata={"è©©å": row["è©©å"],
                                  "ä½œè€…": row["ä½œè€…"],
                                  "è©©é«”": row["è©©é«”"]})
    documents.append(document)
```

è‡ªè¨‚ç¾©å‡½æ•¸ï¼Œè®“BM25ä½¿ç”¨BPE tokenizer


```python
def _preprocess_func(text: str):

    # 1. Define special tokens to remove
    special_tokens = {"<s>", "</s>", "[PAD]", "[UNK]"}
    
    encoded = tokenizer(text)

    tokens = tokenizer.convert_ids_to_tokens(encoded["input_ids"])

    # 2. Remove special tokens
    tokens = [t.replace("â–", "") for t in tokens if t not in special_tokens]
    
    # 3. Remove punctuation (keep only Chinese/English/number words)
    tokens = [t for t in tokens if re.match(r'[\wä¸€-é¾¥]+', t)]
    
    # Stringify the tokens
    return [str(token) for token in tokens]


bm25_poem_retriever = BM25Retriever.from_documents(documents, k=5, 
                                                   bm25_params={"k1":2.5},
                                                   preprocess_func=_preprocess_func
                                                  )
```


```python
bm25_poem_retriever.invoke("å¤§é¢¨èµ·å…®é›²é£›æš å¨åŠ æµ·å…§å…®æ­¸æ•…é„‰ å®‰å¾—çŒ›å£«å…®å®ˆå››æ–¹")
```


```python
bm25_poem_retriever.invoke("å¤•é™½ç„¡é™å¥½")
```

æŠŠè©©ç¶“è½‰æ›æˆäº”è¨€çµ•å¥... æœ‰ä¸­æ–‡æ¯”è¼ƒå¥½çš„äººå—? XD


```python
from textwrap import dedent

from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate
# query

query = dedent("""
è’¹è‘­è’¼è’¼ã€ç™½éœ²ç‚ºéœœã€‚
æ‰€è¬‚ä¼Šäººã€åœ¨æ°´ä¸€æ–¹ã€‚
é¡æ´„å¾ä¹‹ã€é“é˜»ä¸”é•·ã€‚
é¡éŠå¾ä¹‹ã€å®›åœ¨æ°´ä¸­å¤®ã€‚
""")

# output format
class Output(BaseModel):
    name: str = Field(description="result in traditional Chinese (ç¹é«”ä¸­æ–‡)")

output_parser = PydanticOutputParser(pydantic_object=Output)
format_instructions = output_parser.get_format_instructions()


# prompt template
system_template = dedent("""
You are a helpful AI assistant with expertise in classical Chinese literature.
You understand all the nuance and history background of all the content.
""")
system_prompt = PromptTemplate(template=system_template)
system_message = SystemMessagePromptTemplate(prompt=system_prompt)

human_prompt = PromptTemplate(template=("""
Create a {poetic_form}

Examples:
{context}

according to the semantic of {query}

Output instruction: {format_instructions}
"""),
input_variables=["poetic_form", "query", "context"],
partial_variables={'format_instructions': format_instructions}
)
human_message = HumanMessagePromptTemplate(prompt=human_prompt) 

chat_prompt = ChatPromptTemplate.from_messages([system_message,
                                                 human_message
                                               ])

# retrieval
# BM25 retriever ä¸æ”¯æŒ filter
# æ‰€ä»¥å»ºè­°å…ˆfilterå…§å®¹

df_poem = pd.DataFrame(poems)

documents = []

for _, row in df_poem.iterrows():
    if row["è©©é«”"] == "äº”è¨€çµ•å¥":
        document = Document(page_content=row['è©©æ–‡'],
                            metadata={"è©©å": row["è©©å"],
                                      "ä½œè€…": row["ä½œè€…"],
                                      "è©©é«”": row["è©©é«”"]})
        documents.append(document)

bm25_poem_retriever = BM25Retriever.from_documents(documents, k=5, 
                                                   bm25_params={"k1":2.5},
                                                   preprocess_func=_preprocess_func
                                                  )

context = bm25_poem_retriever.invoke(query)

print(context)
```


```python
context = "\n".join([c.page_content for c in context])

print(context)
```


```python
# åˆ‡æ›æˆ gpt-4oã€‚gpt-4o-miniåœ¨é€™æ–¹é¢å¾ˆå¼±

model_poem = ChatOpenAI(openai_api_key=os.environ['OPENAI_API_KEY'],
                   model_name="gpt-4o", 
                   temperature=0 # a range from 0-2, the higher the value, the higher the `creativity`
                  )

prompt = chat_prompt.invoke({"query": query,
                             "poetic_form": "äº”è¨€çµ•å¥",
                             "context": context})

output = model_poem.invoke(prompt)

parsed_output = output_parser.parse(output.content)

print(parsed_output)
```

# Wikipedia Retriever


```python
# !pip install --upgrade --quiet  wikipedia
```


```python
from langchain_community.retrievers import WikipediaRetriever

wiki_retriever = WikipediaRetriever()

docs = wiki_retriever.invoke("2024 US presidential election")
```


```python
len(docs)
```


```python
print(docs[0].page_content)
```


```python
# è‹¥æ˜¯å°‘æ–¼çµ¦å®šè¿”å›æ•¸é‡ï¼Œå‰‡è¿”å›ç•¶å‰æ‰€æœ‰å¯å¾—åˆ°æ–‡ä»¶

docs = wiki_retriever.invoke("rice")
len(docs)
```

- If you want to know what parameters can be feed to the WikipediaRetriever:


```python
WikipediaRetriever?
```

By default, wikipedia retriever returns 3 documents.

# Ensemble Retriever

- å®ƒçµåˆé€™äº›å·¥å…·çš„çµæœä¸¦ä½¿ç”¨ç‰¹æ®Šæ–¹æ³•é€²è¡Œçµ„ç¹”ã€‚
- é€šéä½¿ç”¨ä¸åŒçš„å·¥å…·ï¼Œå®ƒæ¯”åƒ…ä½¿ç”¨å–®ä¸€å·¥å…·æ•ˆæœæ›´å¥½ã€‚
- é€šå¸¸ï¼Œå®ƒçµåˆå…©ç¨®é¡å‹çš„æœç´¢ï¼šä¸€ç¨®å°‹æ‰¾ç²¾ç¢ºè©èªï¼ˆä¾‹å¦‚ BM25ï¼‰ï¼Œå¦ä¸€ç¨®ç†è§£å«ç¾©ï¼ˆä¾‹å¦‚åµŒå…¥å¼ï¼‰ã€‚
- é€™ç¨®æ··åˆç¨±ç‚º "æ··åˆæœç´¢"ã€‚
- ç¬¬ä¸€ç¨®å·¥å…·å°‹æ‰¾å…·æœ‰ç‰¹å®šè©èªçš„æ–‡æª”ï¼Œè€Œç¬¬äºŒç¨®å·¥å…·å‰‡å°‹æ‰¾å…·æœ‰ç›¸ä¼¼æ€æƒ³çš„æ–‡æª”ã€‚

- weights: æ§åˆ¶æ¬Šé‡
- ç¸½è¿”å›æ–‡ä»¶æ•¸é‡ç­‰æ–¼å€‹åˆ¥æª¢ç´¢å™¨ (retriever) æª¢ç´¢æ–‡ä»¶æ•¸é‡


```python
from langchain.retrievers import EnsembleRetriever

ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, wiki_retriever], weights=[0.5, 0.5]
)
```


```python
output = ensemble_retriever.invoke("rice")
```


```python
len(output)
```

- bm25_retriever è¿”å›å…©ä»½
- wiki_retriever è¿”å›å…©ä»½

---

## ğŸ’¼ å¯¦å‹™æ‡‰ç”¨æ¡ˆä¾‹ï¼šå…¬å¸çŸ¥è­˜åº«æª¢ç´¢

å‡è¨­ä½ åœ¨ä¸€é–“ç§‘æŠ€å…¬å¸å·¥ä½œï¼Œå…§éƒ¨æœ‰æ•¸ç™¾ä»½æŠ€è¡“æ–‡ä»¶èˆ‡å°ˆæ¡ˆç´€éŒ„ã€‚  
è‹¥åŒäº‹è©¢å•ï¼šã€Œæˆ‘å€‘å»å¹´å“ªå€‹åœ˜éšŠä½¿ç”¨é LangChainï¼Ÿã€  
- **BM25 Retriever** å¯ç”¨æ–¼å¿«é€Ÿæœå°‹æ–‡ä»¶ä¸­åŒ…å«ã€ŒLangChainã€é—œéµå­—çš„éƒ¨åˆ†ï¼ˆé«˜ç²¾åº¦å­—é¢åŒ¹é…ï¼‰ã€‚  
- **Embedding Retriever**ï¼ˆèªç¾©æœå°‹ï¼‰å‰‡èƒ½æ‰¾åˆ°å³ä½¿æœªå‡ºç¾ç›¸åŒå­—è©ã€ä½†èªæ„ç›¸ä¼¼çš„æ–‡ä»¶ã€‚  

è‹¥åŒæ™‚ä½¿ç”¨å…©è€…çµ„åˆæˆ **Ensemble Retrieverï¼ˆæ··åˆæª¢ç´¢ï¼‰**ï¼š
- BM25 æä¾›æº–ç¢ºçš„å­—è©æ¯”å°  
- Embedding æä¾›èªæ„ç†è§£  
- æœ€å¾Œæ•´åˆçµæœåŠ æ¬Šæ’åºï¼Œèƒ½å¾—åˆ°æ›´å®Œæ•´ã€ç²¾ç¢ºçš„æœå°‹çµæœ  

é€™é¡æ–¹æ³•å¸¸ç”¨æ–¼ï¼š
- å®¢æœçŸ¥è­˜åº«ï¼ˆè‡ªå‹•å›ç­”å®¢æˆ¶å•é¡Œï¼‰  
- æ³•å¾‹æ–‡ä»¶æª¢ç´¢  
- å…¬å¸å…§éƒ¨æ–‡ä»¶æœå°‹å¼•æ“  

# Runtime Configuration (é‹è¡Œæ™‚é…ç½®)

- æˆ‘å€‘ä¹Ÿå¯ä»¥åœ¨é‹è¡Œæ™‚é…ç½®æª¢ç´¢å™¨ã€‚ç‚ºäº†åšåˆ°é€™ä¸€é»ï¼Œæˆ‘å€‘éœ€è¦å°‡å­—æ®µæ¨™è¨˜ç‚ºå¯é…ç½®çš„ã€‚

API Reference: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.utils.ConfigurableField.htmld


```python
from langchain_core.runnables import ConfigurableField
```


```python
bm25_retriever = BM25Retriever.from_documents(documents, k=2, 
    bm25_params={"k1": 1}).configurable_fields(
    k=ConfigurableField(
        id="bm25_k",
    )
)
```


```python
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, wiki_retriever], weights=[0.5, 0.5]
)
```


```python
config = {"configurable": {"bm25_k": 5}}
docs = ensemble_retriever.invoke("rice", config=config)
```


```python
len(docs)
```


```python
# - bm25_retriever è¿”å›äº”ä»½
# - wiki_retriever è¿”å›å…©ä»½
```


```python
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, wiki_retriever], weights=[0.1, 0.9]
)

config = {"configurable": {"bm25_k": 10}}
docs = ensemble_retriever.invoke("rice", config=config)

len(docs)
```


```python
# - bm25_retriever è¿”å›åä»½
# - wiki_retriever è¿”å›å…©ä»½
```

### This is what I do in my work:

I use runtime configuration to target a specific data section with the applied attribute.

More specifically, there are many types of cosmetic products, such as:

- Lipstick
- Lip Gloss
- Mascara
- Blush
- Foundation
- Nail Polish
- Eyeliner
- Eye Pencil

These products are applied to different areas: face, nails, eyes, and lips.

You can retrieve information more efficiently and accurately if you identify the correct application area beforehand.


```python
"""
embedding = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(self.documents, embedding=embedding)

retriever = vectorstore.as_retriever(search_type='similarity',
                                     search_kwargs={'k': self._k}).configurable_fields(search_kwargs=ConfigurableField(id="faiss_search_kwargs"))

semantic_retriever = retrievers['semantic']
semantic_documents = semantic_retriever.invoke(product, config={"configurable":
                                             {"faiss_search_kwargs":
                                                  {"fetch_k":20,
                                                   "k": 2,
                                                   "filter": {"applied": area}}}})
"""
```

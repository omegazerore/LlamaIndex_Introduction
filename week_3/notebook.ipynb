{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d66705-1b85-462f-90de-d4176e520fd5",
   "metadata": {},
   "source": [
    "# Advanced Retrieval Strategies\n",
    "## ReRank\n",
    "\n",
    "### FlagEmbeddingReranker\n",
    "這是用於在本地執行開源重新排序（reranker）模型的 **LlamaIndex 整合類別**。\n",
    "\n",
    "**運作方式：**  \n",
    "它是對 BAAI（北京智源人工智慧研究院）模型套件的封裝。該類別採用 **Cross-Encoder（交叉編碼器）架構**，也就是同時處理查詢（query）與文件（document），以計算「真正的」相關性分數，而不是比較預先計算好的向量。  \n",
    "\n",
    "**最適合對象：**  \n",
    "希望在自有硬體（GPU）上執行高品質重新排序、且不想支付 API 呼叫費用的開發者。\n",
    "\n",
    "---\n",
    "\n",
    "**BAAI/bge-reranker-large**\n",
    "\n",
    "這是目前全球最受歡迎的開源重新排序模型之一。\n",
    "\n",
    "**主要優勢：**  \n",
    "它在 MTEB（Massive Text Embedding Benchmark，大規模文字嵌入基準測試）中長期名列前茅。「Large」版本（約 5.6 億參數）在英文與中文任務上都具有極高的準確度。  \n",
    "\n",
    "**取捨：**  \n",
    "由於它是 Cross-Encoder 架構，其速度明顯慢於初始的向量搜尋。因此，通常只會將前 10–50 筆文件傳入該模型進行重新排序。\n",
    "\n",
    "**目前狀態：**  \n",
    "較新的 **BGE-Reranker-v2-m3**（近期發布）功能更加全面，支援多語言並可處理極長的上下文內容。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d6bf2-1a08-40fb-b4cb-579514e72621",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/FlagOpen/FlagEmbedding.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eca919-4216-46da-88b3-785a99803dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.chdir(\"../\")\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "# from llama_index.indices.postprocessor import LLMRerank \n",
    "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n",
    "\n",
    "from initialization import credential_init\n",
    "\n",
    "credential_init()\n",
    "\n",
    "llm = OpenAI(\"gpt-4o-mini\")\n",
    "reranker = FlagEmbeddingReranker(\n",
    "    top_n=10,\n",
    "    model=\"BAAI/bge-reranker-large\",\n",
    "    use_fp16=False\n",
    ")\n",
    "# reranker = LLMRerank(choice_batch_size=5, top_n=5, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea86474-5b4c-4f02-867c-407f854bbbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "\n",
    "wiki_titles = [\"鋼之鍊金術師\", \"一拳超人\", \"ONE_PIECE\", \"東京喰種\"]\n",
    "wiki_metadatas = {\n",
    "    \"鋼之鍊金術師\": {\n",
    "        \"author\": \"荒川弘\",\n",
    "    },\n",
    "    \"一拳超人\": {\n",
    "        \"author\": \"ONE\",\n",
    "    },\n",
    "    \"ONE_PIECE\": {\n",
    "        \"author\": \"尾田榮一郎\",\n",
    "    },\n",
    "    \"東京喰種\": {\n",
    "        \"author\": \"石田翠\",\n",
    "    },\n",
    "}\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia(user_agent='AI Tutorial(mengchiehling@gmail.com)', language='zh-tw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a87db-0e3a-402d-9505-0f4a9e42990c",
   "metadata": {},
   "source": [
    "從Wikipedia下載數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a954f6ad-0153-4a45-b4ba-6109a4295a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "data_path = Path(\"week_3/data\")\n",
    "\n",
    "for title in wiki_titles:\n",
    "    page = wiki_wiki.page(title)\n",
    "    wiki_text = page.text\n",
    "\n",
    "    if not data_path.exists():\n",
    "        Path.mkdir(data_path)\n",
    "\n",
    "    with open(data_path /f\"{title}.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "        fp.write(wiki_text)\n",
    "\n",
    "# Load all wiki documents\n",
    "docs_dict = {}\n",
    "for wiki_title in wiki_titles:\n",
    "    doc = SimpleDirectoryReader(\n",
    "        input_files=[data_path/f\"{wiki_title}.txt\"]\n",
    "    ).load_data()[0]\n",
    "\n",
    "    doc.metadata.update(wiki_metadatas[wiki_title])\n",
    "    docs_dict[wiki_title] = doc\n",
    "\n",
    "documents = [docs_dict[wiki_title] for wiki_title in wiki_titles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d06c27-79c0-4c3f-bb8c-a6fe70b18b1c",
   "metadata": {},
   "source": [
    "將上週的內容copy/paste下來，建立VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f976538-122c-4980-b744-52447c853c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.callbacks import LlamaDebugHandler, CallbackManager\n",
    "\n",
    "d = 1024 # 必須與 embedding model 的輸出維度一致\n",
    "\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "callback_manager = CallbackManager([LlamaDebugHandler()])\n",
    "\n",
    "index = VectorStoreIndex(\n",
    "    [],\n",
    "    storage_context=storage_context,\n",
    "    transformations=[SentenceSplitter.from_defaults()],\n",
    "    callback_manager=callback_manager,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "# add documents to index\n",
    "for wiki_title in wiki_titles:\n",
    "    index.insert(docs_dict[wiki_title])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c263724-4abd-4113-ade3-68c3d1b51427",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(index.index_struct.nodes_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879314d7-2d67-4aea-a387-edea3e8f1fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=50,\n",
    "    node_postprocessors=[\n",
    "        reranker\n",
    "    ]\n",
    ")\n",
    "\n",
    "basic_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbba1c4-a87c-49ba-bca6-9c50bac20700",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rerank_response = rerank_query_engine.query(\"英雄\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ce8aef-659c-440c-9832-e91a91c8d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes = rerank_response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae9be7c-7777-4a3b-892e-d9ab41ca3336",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(retrieved_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6906e04-3a28-4c34-8d23-f8fffcc60ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48055ba-a8b2-47ce-811f-dfe6045d64d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_response = basic_query_engine.query(\"英雄\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd01a3-da5e-47f6-8540-ae7430b32967",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes = basic_response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f3c13-2951-429f-bea2-7bed96a3749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4046ca-2390-4586-96cf-241724299c5f",
   "metadata": {},
   "source": [
    "### GPTReranker\n",
    "\n",
    "RankGPT 是一種**列表式重排序（Listwise Reranking）**策略，利用大型語言模型（如 GPT-4o 或 Llama 3）來對文件進行排序。\n",
    "\n",
    "- 運作原理： 它並非逐一為文件評分，而是將查詢（Query）與一整組文件列表同時輸入至大型語言模型（LLM），並詢問：「這些文件中哪些最相關？請按順序輸出編號。」\n",
    "- 核心優勢： 它能發揮前沿 LLM 的完整推理能力，比起小型的編碼器模型（Encoder-only models），它更能理解語意細微的差別、複雜邏輯以及使用者的意圖。\n",
    "- 權衡考量： 成本較高（API Token 消耗）且延遲較長（需等待 LLM 生成結果）。它通常被視為高標準檢索流程中的「最後磨光（Final Polish）」步驟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82436ac2-535d-4e94-8d9b-17092b14bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-postprocessor-rankgpt-rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f63e956-9ea2-437c-b5d0-eaed34712c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.rankgpt_rerank import RankGPTRerank\n",
    "\n",
    "reranker = RankGPTRerank(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8163904a-9251-4093-b5ea-069bc87c7898",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=50,\n",
    "    node_postprocessors=[\n",
    "        reranker\n",
    "    ]\n",
    ")\n",
    "\n",
    "basic_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a429a76-f069-4631-a9c5-4bbba4fc90a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_response = rerank_query_engine.query(\"英雄\")\n",
    "retrieved_nodes = rerank_response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ff01ab-bba3-468f-9c8b-e0eb66eb3dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7dc976-f6ad-424e-bd83-d6f0ee30a915",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_response = basic_query_engine.query(\"英雄\")\n",
    "retrieved_nodes = basic_response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8dff7-9e2b-4306-a717-fb3b040b479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80fe11c-9eb1-459d-ae36-0db5172463ba",
   "metadata": {},
   "source": [
    "### Cohere Rerank 4\n",
    "\n",
    "Cohere Rerank 4 (正式名稱為 Rerank v4.0) 是 Cohere 於 2025 年底推出的最新一代重排序模型。與前代（v3.5）相比，它在效能、上下文長度和企業級應用上都有顯著的突破，被視為目前生產環境中最領先的託管型重排序解決方案之一。\n",
    "\n",
    "以下是 Rerank 4 的關鍵特性：\n",
    "\n",
    "1. 雙版本模型策略\n",
    "Rerank 4 採取了與 LLM 類似的策略，提供了兩款針對不同需求優化的版本：\n",
    "\n",
    "Rerank 4 Pro: 最強性能版本，專為追求極致準確度的複雜任務設計（如金融、法律、醫療）。在多項基準測試中其排名均位居前二。\n",
    "\n",
    "Rerank 4 Fast: 效能平衡版本，在維持比 v3.5 更高準確度的同時，大幅降低了延遲（約比 Pro 快 30-40%），適合高流量與即時性需求。\n",
    "\n",
    "2. 上下文窗口巨大飛躍 (32k Tokens)\n",
    "從 4k 到 32k： 相比前代 v3.5 的 4096 tokens，Rerank 4 的上下文長度提升了 8 倍。\n",
    "\n",
    "處理長文件： 這意味著它現在可以一次性「閱讀」約 50 頁的文件，而不需要將文件切成碎片。這對於需要跨章節理解內容的 RAG 系統來說至關重要。\n",
    "\n",
    "3. 多語言與跨語言能力\n",
    "支持 100+ 種語言： 在中文、日文、德文、阿拉伯文等主流語言的表現達到了業界領先水平。\n",
    "\n",
    "跨語言檢索： 它能極其精準地處理「英文查詢、中文文件」這類跨語言的相關性匹配。\n",
    "\n",
    "4. 結構化數據與推理能力\n",
    "原生支持 YAML/JSON： 它可以直接對結構化數據進行排序，而不僅僅是純文本。\n",
    "\n",
    "更強的推理： 針對需要邏輯推理的查詢（例如：需要綜合多個條件的搜尋），Rerank 4 的理解能力顯著優於傳統的編碼器模型，表現更接近小型 LLM，但成本與速度更優。\n",
    "\n",
    "5. 易於整合\n",
    "API 接口： 只需幾行程式碼即可接入（支援 LlamaIndex、LangChain 等框架）。\n",
    "\n",
    "雲端支持： 已整合進 Azure AI Foundry、AWS SageMaker 等主流企業雲平台，滿足安全性與合規性要求。\n",
    "\n",
    "總結來說： 如果您正在開發一個企業級 RAG 系統，且對多語言支持、長文件處理或金融/醫療數據精準度有很高要求，Cohere Rerank 4 Pro 是目前的最佳選擇；如果您更在意成本與回覆速度，Rerank 4 Fast 則提供了極佳的性價比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77466a7e-2ccc-472c-9170-fc1c90b21d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "\n",
    "# api_key = os.environ[\"COHERE_API_KEY\"]\n",
    "# cohere_rerank = CohereRerank(api_key=api_key, top_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0111e664-223b-47e3-ab68-ef17c2234a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CohereRerank?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8492bf38-0213-4419-9757-b8c3897460bb",
   "metadata": {},
   "source": [
    "## Hybrid Retriever\n",
    "\n",
    "### BM25\n",
    "\n",
    "使用英文文本。因為LlamaIndex 的 BM25在處理中文文本上需要高度客製化 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c947200-bda9-4c0e-800d-fc4e9537e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import wikipediaapi\n",
    "\n",
    "\n",
    "wiki_titles = [\"Anthropic\", \"OpenAI\", \"XAI_(company)\", \"Tesla,_Inc.\"]\n",
    "wiki_metadatas = {\n",
    "    \"Anthropic\": {\n",
    "        \"headquarter\": \"San Francisco\",\n",
    "        \"industry\": \"Artificial intelligence\"\n",
    "    },\n",
    "    \"OpenAI\": {\n",
    "        \"headquarter\": \"San Francisco\",\n",
    "        \"industry\": \"Artificial intelligence\"\n",
    "    },\n",
    "    \"XAI_(company)\": {\n",
    "        \"headquarter\": \"Palo Alto\",\n",
    "        \"industry\": \"Technology\"\n",
    "    },\n",
    "    \"Tesla,_Inc.\": {\n",
    "        \"headquarter\": \"Austin\",\n",
    "        \"industry\": \"Automotive\"\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia(user_agent='AI Tutorial(mengchiehling@gmail.com)', language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "606c6342-c4e1-4969-b0e8-b24c6c603c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in wiki_titles:\n",
    "    page = wiki_wiki.page(title)\n",
    "    wiki_text = page.text\n",
    "\n",
    "    data_path = Path(\"data\")\n",
    "    if not data_path.exists():\n",
    "        Path.mkdir(data_path)\n",
    "\n",
    "    with open(data_path / f\"{title}.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "        fp.write(wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83dfefdb-d5cf-4f19-88ad-f5b8b0340545",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for wiki_title in wiki_titles:\n",
    "    doc = SimpleDirectoryReader(\n",
    "        input_files=[f\"data/{wiki_title}.txt\"]\n",
    "    ).load_data()[0]\n",
    "\n",
    "    doc.metadata.update(wiki_metadatas[wiki_title])\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6da4a2ba-85c3-48be-9a60-a1175017fab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# initialize node parser\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "999093d6-17bc-46a0-a69c-3ea4632b0d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e02a64c-1eff-47c2-99cb-e54b876ce5e4",
   "metadata": {},
   "source": [
    "One option is to create the BM25Retriever directly from nodes, and save to and from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa988cda-37a2-443d-8492-e7483f2f833e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 23:13:18,384 - DEBUG - Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "import Stemmer\n",
    "\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "# We can pass in the index, docstore, or list of nodes to create the retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    nodes=nodes,\n",
    "    similarity_top_k=5,\n",
    "    # Optional: We can pass in the stemmer and set the language for stopwords\n",
    "    # This is important for removing stopwords and stemming the query + text\n",
    "    # The default is english for both\n",
    "    stemmer=Stemmer.Stemmer(\"english\"),\n",
    "    language=\"english\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce6e3f0-c110-438c-afe3-2aa13b7817ee",
   "metadata": {},
   "source": [
    "Here, we cover using a BM25Retriever with a docstore to hold your nodes. The advantage here is that the docstore can be remote (mongodb, redis, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "054e79b1-a8ff-4f32-a773-84ddbf18fb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 23:27:48,984 - DEBUG - Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "# initialize a docstore to store nodes\n",
    "# also available are mongodb, redis, postgres, etc for docstores\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(nodes)\n",
    "\n",
    "# We can pass in the index, docstore, or list of nodes to create the retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=docstore,\n",
    "    similarity_top_k=5,\n",
    "    # Optional: We can pass in the stemmer and set the language for stopwords\n",
    "    # This is important for removing stopwords and stemming the query + text\n",
    "    # The default is english for both\n",
    "    stemmer=Stemmer.Stemmer(\"english\"),\n",
    "    language=\"english\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "747d8f4f-c2e0-4dfb-8766-351b68cacd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will retrieve context from specific companies\n",
    "retrieved_nodes = bm25_retriever.retrieve(\n",
    "    \"artiticial intelligence\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bba6a38-ed9b-486f-8d10-556929e766da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a28fec69-5ecd-4059-8ffd-413fdba426c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeWithScore(node=TextNode(id_='208c0afe-045a-49bf-b8f7-0aab99c047d9', embedding=None, metadata={'file_path': 'data\\\\Anthropic.txt', 'file_name': 'Anthropic.txt', 'file_type': 'text/plain', 'file_size': 14618, 'creation_date': '2026-01-06', 'last_modified_date': '2026-01-06', 'headquarter': 'San Francisco', 'industry': 'Artificial intelligence'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c53d3e3f-a06c-4e86-bfe6-7055882c7feb', node_type='4', metadata={'file_path': 'data\\\\Anthropic.txt', 'file_name': 'Anthropic.txt', 'file_type': 'text/plain', 'file_size': 14618, 'creation_date': '2026-01-06', 'last_modified_date': '2026-01-06', 'headquarter': 'San Francisco', 'industry': 'Artificial intelligence'}, hash='d4c664d9889819e3504cbcca02187f077f42ce4aa02c7aaa777c9b914baeff5c'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='69905950-27e7-4958-92f9-1dda572a4f3c', node_type='1', metadata={'file_path': 'data\\\\Anthropic.txt', 'file_name': 'Anthropic.txt', 'file_type': 'text/plain', 'file_size': 14618, 'creation_date': '2026-01-06', 'last_modified_date': '2026-01-06', 'headquarter': 'San Francisco', 'industry': 'Artificial intelligence'}, hash='138a2c8dec1ac1a8d83af726ce4c2345035bc286621a6064147b1e0771f30fa7'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='a0fc2fea-720a-4c9f-b9cd-70b12a68ff18', node_type='1', metadata={}, hash='9b0e3592b474232cb82767e765017c5eaa7a84614415a649fb62d0f249e84fa9')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='In November 2024, Palantir announced a partnership with Anthropic and Amazon Web Services to give U.S. intelligence and defense agencies access to Claude 3 and 3.5. According to Palantir, this was the first time that Claude would be used in \"classified environments\".\\r\\nIn December 2024, Claude 3.5 Haiku was made available to all users on web and mobile platforms.\\r\\nIn February 2025, Claude 3.7 Sonnet was introduced to all paid users. It is a \"hybrid reasoning\" model (one that responds directly to simple queries, while taking more time for complex problems).\\r\\nIn May 2025, Claude 4 Opus and Sonnet were introduced. With these models, Anthropic also introduced Extended thinking with tool use and the ability to use tools in parallel. \\r\\nIn August 2025, Claude Opus 4.1 was introduced.\\r\\nIn September 2025, Claude Sonnet 4.5 was released and, in October 2025, Claude Haiku 4.5 was released.\\r\\nAnthropic is currently researching if Claude is capable of being introspective and can reason why it came to certain conclusions.\\r\\n\\r\\nU.S. military and intelligence\\r\\nIn November 2024, Anthropic partnered with Palantir and Amazon Web Services to provide the Claude model to U.S. intelligence and defense agencies. In June 2025, Anthropic announced a \"Claude Gov\" model. Ars Technica reported that as of June 2025 it was in use at multiple U.S. national security agencies.\\r\\nIn July 2025, the United States Department of Defense announced that Anthropic had received a $200 million contract for AI in the military, along with Google, OpenAI, and xAI.\\r\\n\\r\\nEducation-related projects\\r\\nIn August 2025, Anthropic launched a Higher Education Advisory Board, chaired by former Yale University president and former Coursera CEO Rick Levin.\\r\\nAnthropic partnered with Iceland\\'s Ministry of Education and Children in 2025 to allow teachers from all over the country, including remote areas, to access Claude and integrate AI into daily teaching.\\r\\n\\r\\nResearch\\r\\nConstitutional AI\\r\\nAccording to Anthropic, Constitutional AI (CAI) is a framework developed to align AI systems with human values and ensure that they are helpful, harmless, and honest.', mimetype='text/plain', start_char_idx=8027, end_char_idx=10148, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6904052495956421)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_nodes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375d1a3-6431-416e-ac8e-44e39710adee",
   "metadata": {},
   "source": [
    "### BM25 Retriever + MetadataFiltering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b326daba-0f60-4af2-a677-011bd9b53023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metadata filters\n",
    "from llama_index.core.vector_stores.types import (\n",
    "    MetadataFilters,\n",
    "    MetadataFilter,\n",
    "    FilterOperator,\n",
    "    FilterCondition,\n",
    ")\n",
    "\n",
    "filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(\n",
    "            key=\"headquarter\",\n",
    "            value=\"San Francisco\",\n",
    "            operator=FilterOperator.EQ,\n",
    "        )\n",
    "    ],\n",
    "    condition=FilterCondition.AND,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d980c84f-2904-407a-baf6-392b85329a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 23:32:24,158 - DEBUG - Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "retrieved_nodes = BM25Retriever.from_defaults(\n",
    "    docstore=docstore,\n",
    "    similarity_top_k=3,\n",
    "    filters=filters,  # Add filters here\n",
    "    stemmer=Stemmer.Stemmer(\"english\"),\n",
    "    language=\"english\",\n",
    ").retrieve(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33e2fa39-855c-403b-b8c7-a1042abcaaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.8-cp310-cp310-win_amd64.whl.metadata (52 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.61.1-cp310-cp310-win_amd64.whl.metadata (116 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.9-cp310-cp310-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\ling\\miniconda3\\envs\\rag\\lib\\site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ling\\miniconda3\\envs\\rag\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\ling\\miniconda3\\envs\\rag\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\ling\\miniconda3\\envs\\rag\\lib\\site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ling\\miniconda3\\envs\\rag\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ling\\miniconda3\\envs\\rag\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.8-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/8.1 MB 2.4 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.3/8.1 MB 2.4 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.1/8.1 MB 2.9 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.9/8.1 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.2/8.1 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.2/8.1 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.6/8.1 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 4.8 MB/s  0:00:01\n",
      "Using cached contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.61.1-cp310-cp310-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 1.0/1.6 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 5.5 MB/s  0:00:00\n",
      "Using cached kiwisolver-1.4.9-cp310-cp310-win_amd64.whl (73 kB)\n",
      "Installing collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   ---------------------------------------- 5/5 [matplotlib]\n",
      "\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.8\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f54ae629-b0f2-4c52-8fea-669de8684ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 23:33:30,917 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 91e4866c-c38c-4185-8af7-3b2dab7989ee<br>**Similarity:** 0.8953179717063904<br>**Text:** OpenAI subsequently began a $50 million fund to support nonprofit and community organizations.\r\n",
       "In April 2025, OpenAI raised $40 billion at a $300 billion post-money valuation, which was the highest-value private technology deal in history. The financing round was led by SoftBank, with other participants including Microsoft, Coatue, Altimeter and Thrive.\r\n",
       "In July 2025, the company reported annualized revenue of $12 billion. This was an increase from $3.7 billion in 2024, which was driven by ChatGPT subscriptions, which reached 20 million paid subscribers by April 2025, up from 15.5 million at the end of 2024, alongside a rapidly expanding enterprise customer base that grew to five million business users.\r\n",
       "The company cash burn remains high due to the intensive computational costs required to train and run large language models. It projects an $8 billion operating loss in 2025. OpenAI reports revised long-term spending projections totaling approximately $115 billion through 2029, with annual expenditures projected to escalate significantly, reaching $17 billion in 2026, $35 billion in 2027, and $45 billion in 2028. These expenditures are primarily allocated toward expanding compute infrastructure, developing proprietary AI chips, constructing data centers, and funding intensive model training programs, with more than half of the spending through the end of the decade expected to support research-intensive compute for model training and development.\r\n",
       "The company's financial strategy reflects a strategy of prioritizing market expansion and technological advancement over near-term profitability, with OpenAI targeting cash flow positive operations by 2029 and projecting revenue of approximately $200 billion by 2030. This aggressive spending trajectory underscores both the enormous capital requirements of scaling cutting-edge AI technology and OpenAI's commitment to maintaining its position as a leader in the artificial intelligence industry.\r\n",
       "In October 2025, OpenAI completed an employee share sale of up to $10 billion to existing investors which valued the company at $500 billion. The deal values OpenAI as the most valuable privately owned company in the world—surpassing SpaceX as the world's most valuable private company.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** a5d167e7-29dd-4a45-9905-dd0893d38aee<br>**Similarity:** 0.8840523958206177<br>**Text:** These expenditures are primarily allocated toward expanding compute infrastructure, developing proprietary AI chips, constructing data centers, and funding intensive model training programs, with more than half of the spending through the end of the decade expected to support research-intensive compute for model training and development.\r\n",
       "The company's financial strategy reflects a strategy of prioritizing market expansion and technological advancement over near-term profitability, with OpenAI targeting cash flow positive operations by 2029 and projecting revenue of approximately $200 billion by 2030. This aggressive spending trajectory underscores both the enormous capital requirements of scaling cutting-edge AI technology and OpenAI's commitment to maintaining its position as a leader in the artificial intelligence industry.\r\n",
       "In October 2025, OpenAI completed an employee share sale of up to $10 billion to existing investors which valued the company at $500 billion. The deal values OpenAI as the most valuable privately owned company in the world—surpassing SpaceX as the world's most valuable private company.\r\n",
       "\r\n",
       "Firing of Altman\r\n",
       "On November 17, 2023, Sam Altman was removed as CEO when its board of directors (composed of Helen Toner, Ilya Sutskever, Adam D'Angelo and Tasha McCauley) cited a lack of confidence in him. Chief Technology Officer Mira Murati took over as interim CEO. Greg Brockman, the president of OpenAI, was also removed as chairman of the board and resigned from the company's presidency shortly thereafter. Three senior OpenAI researchers subsequently resigned: director of research and GPT-4 lead Jakub Pachocki, head of AI risk Aleksander Mądry, and researcher Szymon Sidor.\r\n",
       "On November 18, 2023, there were reportedly talks of Altman returning as CEO amid pressure placed upon the board by investors such as Microsoft and Thrive Capital, who objected to Altman's departure. Although Altman himself spoke in favor of returning to OpenAI, he has since stated that he considered starting a new company and bringing former OpenAI employees with him if talks to reinstate him didn't work out. The board members agreed \"in principle\" to resign if Altman returned. On November 19, 2023, negotiations with Altman to return failed and Murati was replaced by Emmett Shear as interim CEO.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 944c28f3-46a1-4dd8-b75f-b344c1351942<br>**Similarity:** 0.6372473239898682<br>**Text:** Anthropic PBC is an American artificial intelligence (AI) company founded in 2021. It has developed a family of large language models (LLMs) named Claude. The company researches and develops AI to \"study their safety properties at the technological frontier\" and use this research to deploy safe models for the public.\r\n",
       "Anthropic was founded by former members of OpenAI, including siblings Daniela Amodei and Dario Amodei, who serve  as president and CEO respectively. In September 2023, Amazon announced an investment of up to $4 billion. Google committed $2 billion the next month. As of November 2025, Anthropic is valued at over $350 billion.\r\n",
       "\r\n",
       "History\r\n",
       "Founding and early development (2021–2022)\r\n",
       "Anthropic was founded in 2021 by seven former employees of OpenAI, including siblings Daniela Amodei and Dario Amodei, the latter of whom was OpenAI's Vice President of Research.\r\n",
       "In April 2022, Anthropic announced it had received $580 million in funding, including a $500 million investment from FTX under the leadership of Sam Bankman-Fried.\r\n",
       "In the summer of 2022, Anthropic finished training the first version of Claude but did not release it, citing the need for further internal safety testing and a desire to avoid initiating a potentially hazardous race to develop increasingly powerful AI systems.\r\n",
       "\r\n",
       "Major investments\r\n",
       "In September 2023, Amazon announced a partnership with Anthropic. Amazon became a minority stakeholder by initially investing $1.25 billion and planning a total investment of $4 billion. The remaining $2.75 billion was invested in March 2024. In November 2024, Amazon invested another $4 billion, doubling its total investment. As part of the deal, Anthropic uses Amazon Web Services (AWS) as its primary cloud provider and makes its AI models available to AWS customers.\r\n",
       "In October 2023, Google invested $500 million in Anthropic and committed to an additional $1.5 billion over time. In March 2025, Google agreed to invest another $1 billion in Anthropic.\r\n",
       "\r\n",
       "Recruitment (2024)\r\n",
       "In February 2024, Anthropic hired former Google Books head of partnerships Tom Turvey, and tasked him with obtaining \"all the books in the world\".<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 0a5eb095-86ce-4b5f-9a09-0ccfaba4d6c6<br>**Similarity:** 0.6270107626914978<br>**Text:** Major investments\r\n",
       "In September 2023, Amazon announced a partnership with Anthropic. Amazon became a minority stakeholder by initially investing $1.25 billion and planning a total investment of $4 billion. The remaining $2.75 billion was invested in March 2024. In November 2024, Amazon invested another $4 billion, doubling its total investment. As part of the deal, Anthropic uses Amazon Web Services (AWS) as its primary cloud provider and makes its AI models available to AWS customers.\r\n",
       "In October 2023, Google invested $500 million in Anthropic and committed to an additional $1.5 billion over time. In March 2025, Google agreed to invest another $1 billion in Anthropic.\r\n",
       "\r\n",
       "Recruitment (2024)\r\n",
       "In February 2024, Anthropic hired former Google Books head of partnerships Tom Turvey, and tasked him with obtaining \"all the books in the world\". The company then began using destructive book scanning to digitize \"millions\" of books to train Claude.\r\n",
       "In 2024, Anthropic attracted several notable employees from OpenAI, including Jan Leike, John Schulman, and Durk Kingma.\r\n",
       "\r\n",
       "Additional funding and partnerships (2025)\r\n",
       "Anthropic raised $3.5 billion in a Series E funding round in March 2025, achieving a post-money valuation of $61.5 billion, led by Lightspeed Venture Partners with participation from several major investors. In March, Databricks and Anthropic announced that Claude would be integrated into the Databricks Data Intelligence Platform.\r\n",
       "In May 2025, the company announced Claude 4, introducing both Claude Opus 4 and Claude Sonnet 4 with improved coding capabilities and other new features. It also introduced new API capabilities, including the Model Context Protocol (MCP) connector. The company hosted its inaugural developer conference that month. Also in May, Anthropic launched a web search API that enables Claude to access real-time information from the internet. Claude Code, Anthropic's coding assistant, transitioned from research preview to general availability, featuring integrations with VS Code and JetBrains IDEs and support for GitHub Actions.\r\n",
       "In September 2025, Anthropic completed a Series F funding round, raising $13 billion at a post-money valuation of $183 billion.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** c47b0fef-a0a8-46e6-8f5f-d67ded0cb77b<br>**Similarity:** 0.6170979142189026<br>**Text:** In December 2016, OpenAI released \"Universe\", a software platform for measuring and training an AI's general intelligence across the world's supply of games, websites, and other applications.\r\n",
       "\r\n",
       "Corporate structure\r\n",
       "Transition from non-profit\r\n",
       "In 2019, OpenAI transitioned from non-profit to \"capped\" for-profit, with the profit being capped at 100 times any investment. According to OpenAI, the capped-profit model allows OpenAI Global, LLC to legally attract investment from venture funds and, in addition, to grant employees stakes in the company. Many top researchers work for Google Brain, DeepMind, or Facebook, which offer stock options that a nonprofit would be unable to. Before the transition, public disclosure of the compensation of top employees at OpenAI was legally required.\r\n",
       "The company then distributed equity to its employees and partnered with Microsoft, announcing an investment package of $1 billion into the company. Since then, OpenAI systems have run on an Azure-based supercomputing platform from Microsoft.\r\n",
       "OpenAI Global, LLC then announced its intention to commercially license its technologies. It planned to spend $1 billion \"within five years, and possibly much faster\". Altman stated that even a billion dollars may turn out to be insufficient, and that the lab may ultimately need \"more capital than any non-profit has ever raised\" to achieve artificial general intelligence.\r\n",
       "The nonprofit, OpenAI, Inc., is the sole controlling shareholder of OpenAI Global, LLC, which, despite being a for-profit company, retains a formal fiduciary responsibility to OpenAI, Inc.'s nonprofit charter. A majority of OpenAI, Inc.'s board is barred from having financial stakes in OpenAI Global, LLC. In addition, minority members with a stake in OpenAI Global, LLC are barred from certain votes due to conflict of interest. Some researchers have argued that OpenAI Global, LLC's switch to for-profit status is inconsistent with OpenAI's claims to be \"democratizing\" AI.\r\n",
       "On February 29, 2024, Elon Musk filed a lawsuit against OpenAI and CEO Sam Altman, accusing them of shifting focus from public benefit to profit maximization—a case OpenAI dismissed as \"incoherent\" and \"frivolous,\" though Musk later revived legal action against Altman and others in August.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "retrieved_nodes = BM25Retriever.from_defaults(\n",
    "    docstore=docstore,\n",
    "    similarity_top_k=5,\n",
    "    filters=filters,  # Add filters here\n",
    "    stemmer=Stemmer.Stemmer(\"english\"),\n",
    "    language=\"english\",\n",
    ").retrieve(\"Hello, world!\")\n",
    "\n",
    "for node in retrieved_nodes:\n",
    "    display_source_node(node, source_length=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472be930-3aeb-4e23-86c4-3a25c084a21c",
   "metadata": {},
   "source": [
    "### Hybrid Retriever with BM25 + FAISS\n",
    "Now we will combine bm25 and faiss for sparse and dense retrieval.\n",
    "\n",
    "The results are combined using the QueryFusionRetriever.\n",
    "\n",
    "With the retriever, we can make a complete RetrieverQueryEngine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d5775a-6a7e-4b14-bbc5-e1496052be6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d66705-1b85-462f-90de-d4176e520fd5",
   "metadata": {},
   "source": [
    "# Advanced Retrieval Strategies\n",
    "## ReRank\n",
    "\n",
    "### FlagEmbeddingReranker\n",
    "這是用於在本地執行開源重新排序（reranker）模型的 **LlamaIndex 整合類別**。\n",
    "\n",
    "**運作方式：**  \n",
    "它是對 BAAI（北京智源人工智慧研究院）模型套件的封裝。該類別採用 **Cross-Encoder（交叉編碼器）架構**，也就是同時處理查詢（query）與文件（document），以計算「真正的」相關性分數，而不是比較預先計算好的向量。  \n",
    "\n",
    "**最適合對象：**  \n",
    "希望在自有硬體（GPU）上執行高品質重新排序、且不想支付 API 呼叫費用的開發者。\n",
    "\n",
    "---\n",
    "\n",
    "**BAAI/bge-reranker-large**\n",
    "\n",
    "這是目前全球最受歡迎的開源重新排序模型之一。\n",
    "\n",
    "**主要優勢：**  \n",
    "它在 MTEB（Massive Text Embedding Benchmark，大規模文字嵌入基準測試）中長期名列前茅。「Large」版本（約 5.6 億參數）在英文與中文任務上都具有極高的準確度。  \n",
    "\n",
    "**取捨：**  \n",
    "由於它是 Cross-Encoder 架構，其速度明顯慢於初始的向量搜尋。因此，通常只會將前 10–50 筆文件傳入該模型進行重新排序。\n",
    "\n",
    "**目前狀態：**  \n",
    "較新的 **BGE-Reranker-v2-m3**（近期發布）功能更加全面，支援多語言並可處理極長的上下文內容。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d6bf2-1a08-40fb-b4cb-579514e72621",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/FlagOpen/FlagEmbedding.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eca919-4216-46da-88b3-785a99803dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.chdir(\"../\")\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "# from llama_index.indices.postprocessor import LLMRerank \n",
    "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n",
    "\n",
    "from initialization import credential_init\n",
    "\n",
    "credential_init()\n",
    "\n",
    "llm = OpenAI(\"gpt-4o-mini\")\n",
    "reranker = FlagEmbeddingReranker(\n",
    "    top_n=10,\n",
    "    model=\"BAAI/bge-reranker-large\",\n",
    "    use_fp16=False\n",
    ")\n",
    "# reranker = LLMRerank(choice_batch_size=5, top_n=5, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea86474-5b4c-4f02-867c-407f854bbbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "\n",
    "wiki_titles = [\"鋼之鍊金術師\", \"一拳超人\", \"ONE_PIECE\", \"東京喰種\"]\n",
    "wiki_metadatas = {\n",
    "    \"鋼之鍊金術師\": {\n",
    "        \"author\": \"荒川弘\",\n",
    "    },\n",
    "    \"一拳超人\": {\n",
    "        \"author\": \"ONE\",\n",
    "    },\n",
    "    \"ONE_PIECE\": {\n",
    "        \"author\": \"尾田榮一郎\",\n",
    "    },\n",
    "    \"東京喰種\": {\n",
    "        \"author\": \"石田翠\",\n",
    "    },\n",
    "}\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia(user_agent='AI Tutorial(mengchiehling@gmail.com)', language='zh-tw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a87db-0e3a-402d-9505-0f4a9e42990c",
   "metadata": {},
   "source": [
    "從Wikipedia下載數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a954f6ad-0153-4a45-b4ba-6109a4295a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "data_path = Path(\"week_3/data\")\n",
    "\n",
    "for title in wiki_titles:\n",
    "    page = wiki_wiki.page(title)\n",
    "    wiki_text = page.text\n",
    "\n",
    "    if not data_path.exists():\n",
    "        Path.mkdir(data_path)\n",
    "\n",
    "    with open(data_path /f\"{title}.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "        fp.write(wiki_text)\n",
    "\n",
    "# Load all wiki documents\n",
    "docs_dict = {}\n",
    "for wiki_title in wiki_titles:\n",
    "    doc = SimpleDirectoryReader(\n",
    "        input_files=[data_path/f\"{wiki_title}.txt\"]\n",
    "    ).load_data()[0]\n",
    "\n",
    "    doc.metadata.update(wiki_metadatas[wiki_title])\n",
    "    docs_dict[wiki_title] = doc\n",
    "\n",
    "documents = [docs_dict[wiki_title] for wiki_title in wiki_titles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d06c27-79c0-4c3f-bb8c-a6fe70b18b1c",
   "metadata": {},
   "source": [
    "將上週的內容copy/paste下來，建立VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f976538-122c-4980-b744-52447c853c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.callbacks import LlamaDebugHandler, CallbackManager\n",
    "\n",
    "d = 1024 # 必須與 embedding model 的輸出維度一致\n",
    "\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "callback_manager = CallbackManager([LlamaDebugHandler()])\n",
    "\n",
    "index = VectorStoreIndex(\n",
    "    [],\n",
    "    storage_context=storage_context,\n",
    "    transformations=[SentenceSplitter.from_defaults()],\n",
    "    callback_manager=callback_manager,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "# add documents to index\n",
    "for wiki_title in wiki_titles:\n",
    "    index.insert(docs_dict[wiki_title])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c263724-4abd-4113-ade3-68c3d1b51427",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(index.index_struct.nodes_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879314d7-2d67-4aea-a387-edea3e8f1fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=50,\n",
    "    node_postprocessors=[\n",
    "        reranker\n",
    "    ]\n",
    ")\n",
    "\n",
    "basic_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbba1c4-a87c-49ba-bca6-9c50bac20700",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rerank_response = rerank_query_engine.query(\"英雄\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ce8aef-659c-440c-9832-e91a91c8d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes = rerank_response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae9be7c-7777-4a3b-892e-d9ab41ca3336",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(retrieved_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6906e04-3a28-4c34-8d23-f8fffcc60ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48055ba-a8b2-47ce-811f-dfe6045d64d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_response = basic_query_engine.query(\"英雄\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd01a3-da5e-47f6-8540-ae7430b32967",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes = basic_response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f3c13-2951-429f-bea2-7bed96a3749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4046ca-2390-4586-96cf-241724299c5f",
   "metadata": {},
   "source": [
    "### GPTReranker\n",
    "\n",
    "RankGPT 是一種**列表式重排序（Listwise Reranking）**策略，利用大型語言模型（如 GPT-4o 或 Llama 3）來對文件進行排序。\n",
    "\n",
    "- 運作原理： 它並非逐一為文件評分，而是將查詢（Query）與一整組文件列表同時輸入至大型語言模型（LLM），並詢問：「這些文件中哪些最相關？請按順序輸出編號。」\n",
    "- 核心優勢： 它能發揮前沿 LLM 的完整推理能力，比起小型的編碼器模型（Encoder-only models），它更能理解語意細微的差別、複雜邏輯以及使用者的意圖。\n",
    "- 權衡考量： 成本較高（API Token 消耗）且延遲較長（需等待 LLM 生成結果）。它通常被視為高標準檢索流程中的「最後磨光（Final Polish）」步驟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82436ac2-535d-4e94-8d9b-17092b14bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-postprocessor-rankgpt-rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f63e956-9ea2-437c-b5d0-eaed34712c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.rankgpt_rerank import RankGPTRerank\n",
    "\n",
    "reranker = RankGPTRerank(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8163904a-9251-4093-b5ea-069bc87c7898",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=50,\n",
    "    node_postprocessors=[\n",
    "        reranker\n",
    "    ]\n",
    ")\n",
    "\n",
    "basic_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a429a76-f069-4631-a9c5-4bbba4fc90a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_response = rerank_query_engine.query(\"英雄\")\n",
    "retrieved_nodes = rerank_response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ff01ab-bba3-468f-9c8b-e0eb66eb3dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7dc976-f6ad-424e-bd83-d6f0ee30a915",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_response = basic_query_engine.query(\"英雄\")\n",
    "retrieved_nodes = basic_response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8dff7-9e2b-4306-a717-fb3b040b479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80fe11c-9eb1-459d-ae36-0db5172463ba",
   "metadata": {},
   "source": [
    "### Cohere Rerank 4\n",
    "\n",
    "Cohere Rerank 4 (正式名稱為 Rerank v4.0) 是 Cohere 於 2025 年底推出的最新一代重排序模型。與前代（v3.5）相比，它在效能、上下文長度和企業級應用上都有顯著的突破，被視為目前生產環境中最領先的託管型重排序解決方案之一。\n",
    "\n",
    "以下是 Rerank 4 的關鍵特性：\n",
    "\n",
    "1. 雙版本模型策略\n",
    "Rerank 4 採取了與 LLM 類似的策略，提供了兩款針對不同需求優化的版本：\n",
    "\n",
    "Rerank 4 Pro: 最強性能版本，專為追求極致準確度的複雜任務設計（如金融、法律、醫療）。在多項基準測試中其排名均位居前二。\n",
    "\n",
    "Rerank 4 Fast: 效能平衡版本，在維持比 v3.5 更高準確度的同時，大幅降低了延遲（約比 Pro 快 30-40%），適合高流量與即時性需求。\n",
    "\n",
    "2. 上下文窗口巨大飛躍 (32k Tokens)\n",
    "從 4k 到 32k： 相比前代 v3.5 的 4096 tokens，Rerank 4 的上下文長度提升了 8 倍。\n",
    "\n",
    "處理長文件： 這意味著它現在可以一次性「閱讀」約 50 頁的文件，而不需要將文件切成碎片。這對於需要跨章節理解內容的 RAG 系統來說至關重要。\n",
    "\n",
    "3. 多語言與跨語言能力\n",
    "支持 100+ 種語言： 在中文、日文、德文、阿拉伯文等主流語言的表現達到了業界領先水平。\n",
    "\n",
    "跨語言檢索： 它能極其精準地處理「英文查詢、中文文件」這類跨語言的相關性匹配。\n",
    "\n",
    "4. 結構化數據與推理能力\n",
    "原生支持 YAML/JSON： 它可以直接對結構化數據進行排序，而不僅僅是純文本。\n",
    "\n",
    "更強的推理： 針對需要邏輯推理的查詢（例如：需要綜合多個條件的搜尋），Rerank 4 的理解能力顯著優於傳統的編碼器模型，表現更接近小型 LLM，但成本與速度更優。\n",
    "\n",
    "5. 易於整合\n",
    "API 接口： 只需幾行程式碼即可接入（支援 LlamaIndex、LangChain 等框架）。\n",
    "\n",
    "雲端支持： 已整合進 Azure AI Foundry、AWS SageMaker 等主流企業雲平台，滿足安全性與合規性要求。\n",
    "\n",
    "總結來說： 如果您正在開發一個企業級 RAG 系統，且對多語言支持、長文件處理或金融/醫療數據精準度有很高要求，Cohere Rerank 4 Pro 是目前的最佳選擇；如果您更在意成本與回覆速度，Rerank 4 Fast 則提供了極佳的性價比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77466a7e-2ccc-472c-9170-fc1c90b21d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "\n",
    "# api_key = os.environ[\"COHERE_API_KEY\"]\n",
    "# cohere_rerank = CohereRerank(api_key=api_key, top_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0111e664-223b-47e3-ab68-ef17c2234a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CohereRerank?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8492bf38-0213-4419-9757-b8c3897460bb",
   "metadata": {},
   "source": [
    "## Hybrid Retriever\n",
    "\n",
    "### BM25\n",
    "\n",
    "使用英文文本。因為LlamaIndex 的 BM25在處理中文文本上需要高度客製化 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c947200-bda9-4c0e-800d-fc4e9537e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import wikipediaapi\n",
    "\n",
    "\n",
    "wiki_titles = [\"Anthropic\", \"OpenAI\", \"XAI_(company)\", \"Tesla,_Inc.\"]\n",
    "wiki_metadatas = {\n",
    "    \"Anthropic\": {\n",
    "        \"headquarter\": \"San Francisco\",\n",
    "        \"industry\": \"Artificial intelligence\"\n",
    "    },\n",
    "    \"OpenAI\": {\n",
    "        \"headquarter\": \"San Francisco\",\n",
    "        \"industry\": \"Artificial intelligence\"\n",
    "    },\n",
    "    \"XAI_(company)\": {\n",
    "        \"headquarter\": \"Palo Alto\",\n",
    "        \"industry\": \"Technology\"\n",
    "    },\n",
    "    \"Tesla,_Inc.\": {\n",
    "        \"headquarter\": \"Austin\",\n",
    "        \"industry\": \"Automotive\"\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia(user_agent='AI Tutorial(mengchiehling@gmail.com)', language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "606c6342-c4e1-4969-b0e8-b24c6c603c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in wiki_titles:\n",
    "    page = wiki_wiki.page(title)\n",
    "    wiki_text = page.text\n",
    "\n",
    "    data_path = Path(\"data\")\n",
    "    if not data_path.exists():\n",
    "        Path.mkdir(data_path)\n",
    "\n",
    "    with open(data_path / f\"{title}.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "        fp.write(wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83dfefdb-d5cf-4f19-88ad-f5b8b0340545",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for wiki_title in wiki_titles:\n",
    "    doc = SimpleDirectoryReader(\n",
    "        input_files=[f\"data/{wiki_title}.txt\"]\n",
    "    ).load_data()[0]\n",
    "\n",
    "    doc.metadata.update(wiki_metadatas[wiki_title])\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6da4a2ba-85c3-48be-9a60-a1175017fab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# initialize node parser\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "999093d6-17bc-46a0-a69c-3ea4632b0d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e02a64c-1eff-47c2-99cb-e54b876ce5e4",
   "metadata": {},
   "source": [
    "One option is to create the BM25Retriever directly from nodes, and save to and from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa988cda-37a2-443d-8492-e7483f2f833e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 23:13:18,384 - DEBUG - Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "import Stemmer\n",
    "\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "# We can pass in the index, docstore, or list of nodes to create the retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    nodes=nodes,\n",
    "    similarity_top_k=5,\n",
    "    # Optional: We can pass in the stemmer and set the language for stopwords\n",
    "    # This is important for removing stopwords and stemming the query + text\n",
    "    # The default is english for both\n",
    "    stemmer=Stemmer.Stemmer(\"english\"),\n",
    "    language=\"english\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce6e3f0-c110-438c-afe3-2aa13b7817ee",
   "metadata": {},
   "source": [
    "Here, we cover using a BM25Retriever with a docstore to hold your nodes. The advantage here is that the docstore can be remote (mongodb, redis, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "054e79b1-a8ff-4f32-a773-84ddbf18fb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 23:27:48,984 - DEBUG - Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "# initialize a docstore to store nodes\n",
    "# also available are mongodb, redis, postgres, etc for docstores\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(nodes)\n",
    "\n",
    "# We can pass in the index, docstore, or list of nodes to create the retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=docstore,\n",
    "    similarity_top_k=5,\n",
    "    # Optional: We can pass in the stemmer and set the language for stopwords\n",
    "    # This is important for removing stopwords and stemming the query + text\n",
    "    # The default is english for both\n",
    "    stemmer=Stemmer.Stemmer(\"english\"),\n",
    "    language=\"english\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "747d8f4f-c2e0-4dfb-8766-351b68cacd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will retrieve context from specific companies\n",
    "retrieved_nodes = bm25_retriever.retrieve(\n",
    "    \"artiticial intelligence\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bba6a38-ed9b-486f-8d10-556929e766da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a28fec69-5ecd-4059-8ffd-413fdba426c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeWithScore(node=TextNode(id_='208c0afe-045a-49bf-b8f7-0aab99c047d9', embedding=None, metadata={'file_path': 'data\\\\Anthropic.txt', 'file_name': 'Anthropic.txt', 'file_type': 'text/plain', 'file_size': 14618, 'creation_date': '2026-01-06', 'last_modified_date': '2026-01-06', 'headquarter': 'San Francisco', 'industry': 'Artificial intelligence'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c53d3e3f-a06c-4e86-bfe6-7055882c7feb', node_type='4', metadata={'file_path': 'data\\\\Anthropic.txt', 'file_name': 'Anthropic.txt', 'file_type': 'text/plain', 'file_size': 14618, 'creation_date': '2026-01-06', 'last_modified_date': '2026-01-06', 'headquarter': 'San Francisco', 'industry': 'Artificial intelligence'}, hash='d4c664d9889819e3504cbcca02187f077f42ce4aa02c7aaa777c9b914baeff5c'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='69905950-27e7-4958-92f9-1dda572a4f3c', node_type='1', metadata={'file_path': 'data\\\\Anthropic.txt', 'file_name': 'Anthropic.txt', 'file_type': 'text/plain', 'file_size': 14618, 'creation_date': '2026-01-06', 'last_modified_date': '2026-01-06', 'headquarter': 'San Francisco', 'industry': 'Artificial intelligence'}, hash='138a2c8dec1ac1a8d83af726ce4c2345035bc286621a6064147b1e0771f30fa7'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='a0fc2fea-720a-4c9f-b9cd-70b12a68ff18', node_type='1', metadata={}, hash='9b0e3592b474232cb82767e765017c5eaa7a84614415a649fb62d0f249e84fa9')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='In November 2024, Palantir announced a partnership with Anthropic and Amazon Web Services to give U.S. intelligence and defense agencies access to Claude 3 and 3.5. According to Palantir, this was the first time that Claude would be used in \"classified environments\".\\r\\nIn December 2024, Claude 3.5 Haiku was made available to all users on web and mobile platforms.\\r\\nIn February 2025, Claude 3.7 Sonnet was introduced to all paid users. It is a \"hybrid reasoning\" model (one that responds directly to simple queries, while taking more time for complex problems).\\r\\nIn May 2025, Claude 4 Opus and Sonnet were introduced. With these models, Anthropic also introduced Extended thinking with tool use and the ability to use tools in parallel. \\r\\nIn August 2025, Claude Opus 4.1 was introduced.\\r\\nIn September 2025, Claude Sonnet 4.5 was released and, in October 2025, Claude Haiku 4.5 was released.\\r\\nAnthropic is currently researching if Claude is capable of being introspective and can reason why it came to certain conclusions.\\r\\n\\r\\nU.S. military and intelligence\\r\\nIn November 2024, Anthropic partnered with Palantir and Amazon Web Services to provide the Claude model to U.S. intelligence and defense agencies. In June 2025, Anthropic announced a \"Claude Gov\" model. Ars Technica reported that as of June 2025 it was in use at multiple U.S. national security agencies.\\r\\nIn July 2025, the United States Department of Defense announced that Anthropic had received a $200 million contract for AI in the military, along with Google, OpenAI, and xAI.\\r\\n\\r\\nEducation-related projects\\r\\nIn August 2025, Anthropic launched a Higher Education Advisory Board, chaired by former Yale University president and former Coursera CEO Rick Levin.\\r\\nAnthropic partnered with Iceland\\'s Ministry of Education and Children in 2025 to allow teachers from all over the country, including remote areas, to access Claude and integrate AI into daily teaching.\\r\\n\\r\\nResearch\\r\\nConstitutional AI\\r\\nAccording to Anthropic, Constitutional AI (CAI) is a framework developed to align AI systems with human values and ensure that they are helpful, harmless, and honest.', mimetype='text/plain', start_char_idx=8027, end_char_idx=10148, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6904052495956421)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_nodes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375d1a3-6431-416e-ac8e-44e39710adee",
   "metadata": {},
   "source": [
    "### BM25 Retriever + MetadataFiltering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b326daba-0f60-4af2-a677-011bd9b53023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metadata filters\n",
    "from llama_index.core.vector_stores.types import (\n",
    "    MetadataFilters,\n",
    "    MetadataFilter,\n",
    "    FilterOperator,\n",
    "    FilterCondition,\n",
    ")\n",
    "\n",
    "filters = MetadataFilters(\n",
    "    filters=[\n",
    "        MetadataFilter(\n",
    "            key=\"headquarter\",\n",
    "            value=\"San Francisco\",\n",
    "            operator=FilterOperator.EQ,\n",
    "        )\n",
    "    ],\n",
    "    condition=FilterCondition.AND,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d980c84f-2904-407a-baf6-392b85329a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 23:32:24,158 - DEBUG - Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "retrieved_nodes = BM25Retriever.from_defaults(\n",
    "    docstore=docstore,\n",
    "    similarity_top_k=3,\n",
    "    filters=filters,  # Add filters here\n",
    "    stemmer=Stemmer.Stemmer(\"english\"),\n",
    "    language=\"english\",\n",
    ").retrieve(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33e2fa39-855c-403b-b8c7-a1042abcaaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.8-cp310-cp310-win_amd64.whl.metadata (52 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.61.1-cp310-cp310-win_amd64.whl.metadata (116 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.9-cp310-cp310-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\ling\\miniconda3\\envs\\rag\\lib\\site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ling\\miniconda3\\envs\\rag\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\ling\\miniconda3\\envs\\rag\\lib\\site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\ling\\miniconda3\\envs\\rag\\lib\\site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ling\\miniconda3\\envs\\rag\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ling\\miniconda3\\envs\\rag\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.8-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/8.1 MB 2.4 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.3/8.1 MB 2.4 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.1/8.1 MB 2.9 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.9/8.1 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.2/8.1 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.2/8.1 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.6/8.1 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 4.8 MB/s  0:00:01\n",
      "Using cached contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.61.1-cp310-cp310-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 1.0/1.6 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 5.5 MB/s  0:00:00\n",
      "Using cached kiwisolver-1.4.9-cp310-cp310-win_amd64.whl (73 kB)\n",
      "Installing collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------- ------------------------------- 1/5 [fonttools]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   -------------------------------- ------- 4/5 [matplotlib]\n",
      "   ---------------------------------------- 5/5 [matplotlib]\n",
      "\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.8\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b784f297-5c7e-40fd-a7f0-e5c95fe62bd0",
   "metadata": {},
   "source": [
    "### Queries\n",
    "\n",
    "**Company Background & Positioning**\n",
    "\n",
    "> Best for testing keyword matching + semantic understanding\n",
    "\n",
    "- What is OpenAI and what is its core mission?\n",
    "- How does Anthropic position itself in the AI safety landscape?\n",
    "- What company is xAI and why was it founded?\n",
    "- What is Tesla primarily known for besides electric vehicles?\n",
    "\n",
    "---\n",
    "\n",
    "**Founders & Key Figures**\n",
    "\n",
    "> Strong keyword signals (names, dates) with semantic reinforcement\n",
    "\n",
    "- Who founded OpenAI and when?\n",
    "- Who are the founders of Anthropic?\n",
    "- What is Elon Musk’s involvement in xAI?\n",
    "- How is Elon Musk connected to Tesla?\n",
    "\n",
    "---\n",
    "\n",
    "**Mission, Goals, and Philosophy**\n",
    "\n",
    "> Conceptual questions where dense retrieval excels\n",
    "\n",
    "- What does OpenAI mean by artificial general intelligence?\n",
    "- What is Anthropic’s approach to building safe AI systems?\n",
    "- What problem is xAI trying to solve?\n",
    "- What is Tesla’s long-term vision for sustainable energy?\n",
    "\n",
    "---\n",
    "\n",
    "**Products & Technologies**\n",
    "\n",
    "> Hybrid retrieval works especially well for mixed factual + semantic queries\n",
    "\n",
    "- What products or models has OpenAI released?\n",
    "- What is Anthropic’s Claude model?\n",
    "- Does xAI develop large language models?\n",
    "- What technologies does Tesla develop besides cars?\n",
    "\n",
    "---\n",
    "\n",
    "**Comparison Queries**\n",
    "\n",
    "> Ideal for demonstrating QueryFusionRetriever and multi-document reasoning\n",
    "\n",
    "- How is OpenAI different from Anthropic?\n",
    "- Compare the goals of OpenAI and xAI.\n",
    "- How do AI companies like OpenAI and Anthropic differ from Tesla?\n",
    "- Which of these companies focuses on AI safety?\n",
    "\n",
    "---\n",
    "\n",
    "**Advanced / RAG Demo Queries (Recommended)**\n",
    "\n",
    "> Excellent for end-to-end RetrieverQueryEngine demonstrations\n",
    "\n",
    "- Which companies in the documents are focused on artificial intelligence research?\n",
    "- Which organization emphasizes AI safety and alignment?\n",
    "- What companies were founded by Elon Musk?\n",
    "- Summarize the missions of OpenAI, Anthropic, xAI, and Tesla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f54ae629-b0f2-4c52-8fea-669de8684ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 23:33:30,917 - DEBUG - Building index from IDs objects\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 91e4866c-c38c-4185-8af7-3b2dab7989ee<br>**Similarity:** 0.8953179717063904<br>**Text:** OpenAI subsequently began a $50 million fund to support nonprofit and community organizations.\r\n",
       "In April 2025, OpenAI raised $40 billion at a $300 billion post-money valuation, which was the highest-value private technology deal in history. The financing round was led by SoftBank, with other participants including Microsoft, Coatue, Altimeter and Thrive.\r\n",
       "In July 2025, the company reported annualized revenue of $12 billion. This was an increase from $3.7 billion in 2024, which was driven by ChatGPT subscriptions, which reached 20 million paid subscribers by April 2025, up from 15.5 million at the end of 2024, alongside a rapidly expanding enterprise customer base that grew to five million business users.\r\n",
       "The company cash burn remains high due to the intensive computational costs required to train and run large language models. It projects an $8 billion operating loss in 2025. OpenAI reports revised long-term spending projections totaling approximately $115 billion through 2029, with annual expenditures projected to escalate significantly, reaching $17 billion in 2026, $35 billion in 2027, and $45 billion in 2028. These expenditures are primarily allocated toward expanding compute infrastructure, developing proprietary AI chips, constructing data centers, and funding intensive model training programs, with more than half of the spending through the end of the decade expected to support research-intensive compute for model training and development.\r\n",
       "The company's financial strategy reflects a strategy of prioritizing market expansion and technological advancement over near-term profitability, with OpenAI targeting cash flow positive operations by 2029 and projecting revenue of approximately $200 billion by 2030. This aggressive spending trajectory underscores both the enormous capital requirements of scaling cutting-edge AI technology and OpenAI's commitment to maintaining its position as a leader in the artificial intelligence industry.\r\n",
       "In October 2025, OpenAI completed an employee share sale of up to $10 billion to existing investors which valued the company at $500 billion. The deal values OpenAI as the most valuable privately owned company in the world—surpassing SpaceX as the world's most valuable private company.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** a5d167e7-29dd-4a45-9905-dd0893d38aee<br>**Similarity:** 0.8840523958206177<br>**Text:** These expenditures are primarily allocated toward expanding compute infrastructure, developing proprietary AI chips, constructing data centers, and funding intensive model training programs, with more than half of the spending through the end of the decade expected to support research-intensive compute for model training and development.\r\n",
       "The company's financial strategy reflects a strategy of prioritizing market expansion and technological advancement over near-term profitability, with OpenAI targeting cash flow positive operations by 2029 and projecting revenue of approximately $200 billion by 2030. This aggressive spending trajectory underscores both the enormous capital requirements of scaling cutting-edge AI technology and OpenAI's commitment to maintaining its position as a leader in the artificial intelligence industry.\r\n",
       "In October 2025, OpenAI completed an employee share sale of up to $10 billion to existing investors which valued the company at $500 billion. The deal values OpenAI as the most valuable privately owned company in the world—surpassing SpaceX as the world's most valuable private company.\r\n",
       "\r\n",
       "Firing of Altman\r\n",
       "On November 17, 2023, Sam Altman was removed as CEO when its board of directors (composed of Helen Toner, Ilya Sutskever, Adam D'Angelo and Tasha McCauley) cited a lack of confidence in him. Chief Technology Officer Mira Murati took over as interim CEO. Greg Brockman, the president of OpenAI, was also removed as chairman of the board and resigned from the company's presidency shortly thereafter. Three senior OpenAI researchers subsequently resigned: director of research and GPT-4 lead Jakub Pachocki, head of AI risk Aleksander Mądry, and researcher Szymon Sidor.\r\n",
       "On November 18, 2023, there were reportedly talks of Altman returning as CEO amid pressure placed upon the board by investors such as Microsoft and Thrive Capital, who objected to Altman's departure. Although Altman himself spoke in favor of returning to OpenAI, he has since stated that he considered starting a new company and bringing former OpenAI employees with him if talks to reinstate him didn't work out. The board members agreed \"in principle\" to resign if Altman returned. On November 19, 2023, negotiations with Altman to return failed and Murati was replaced by Emmett Shear as interim CEO.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 944c28f3-46a1-4dd8-b75f-b344c1351942<br>**Similarity:** 0.6372473239898682<br>**Text:** Anthropic PBC is an American artificial intelligence (AI) company founded in 2021. It has developed a family of large language models (LLMs) named Claude. The company researches and develops AI to \"study their safety properties at the technological frontier\" and use this research to deploy safe models for the public.\r\n",
       "Anthropic was founded by former members of OpenAI, including siblings Daniela Amodei and Dario Amodei, who serve  as president and CEO respectively. In September 2023, Amazon announced an investment of up to $4 billion. Google committed $2 billion the next month. As of November 2025, Anthropic is valued at over $350 billion.\r\n",
       "\r\n",
       "History\r\n",
       "Founding and early development (2021–2022)\r\n",
       "Anthropic was founded in 2021 by seven former employees of OpenAI, including siblings Daniela Amodei and Dario Amodei, the latter of whom was OpenAI's Vice President of Research.\r\n",
       "In April 2022, Anthropic announced it had received $580 million in funding, including a $500 million investment from FTX under the leadership of Sam Bankman-Fried.\r\n",
       "In the summer of 2022, Anthropic finished training the first version of Claude but did not release it, citing the need for further internal safety testing and a desire to avoid initiating a potentially hazardous race to develop increasingly powerful AI systems.\r\n",
       "\r\n",
       "Major investments\r\n",
       "In September 2023, Amazon announced a partnership with Anthropic. Amazon became a minority stakeholder by initially investing $1.25 billion and planning a total investment of $4 billion. The remaining $2.75 billion was invested in March 2024. In November 2024, Amazon invested another $4 billion, doubling its total investment. As part of the deal, Anthropic uses Amazon Web Services (AWS) as its primary cloud provider and makes its AI models available to AWS customers.\r\n",
       "In October 2023, Google invested $500 million in Anthropic and committed to an additional $1.5 billion over time. In March 2025, Google agreed to invest another $1 billion in Anthropic.\r\n",
       "\r\n",
       "Recruitment (2024)\r\n",
       "In February 2024, Anthropic hired former Google Books head of partnerships Tom Turvey, and tasked him with obtaining \"all the books in the world\".<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 0a5eb095-86ce-4b5f-9a09-0ccfaba4d6c6<br>**Similarity:** 0.6270107626914978<br>**Text:** Major investments\r\n",
       "In September 2023, Amazon announced a partnership with Anthropic. Amazon became a minority stakeholder by initially investing $1.25 billion and planning a total investment of $4 billion. The remaining $2.75 billion was invested in March 2024. In November 2024, Amazon invested another $4 billion, doubling its total investment. As part of the deal, Anthropic uses Amazon Web Services (AWS) as its primary cloud provider and makes its AI models available to AWS customers.\r\n",
       "In October 2023, Google invested $500 million in Anthropic and committed to an additional $1.5 billion over time. In March 2025, Google agreed to invest another $1 billion in Anthropic.\r\n",
       "\r\n",
       "Recruitment (2024)\r\n",
       "In February 2024, Anthropic hired former Google Books head of partnerships Tom Turvey, and tasked him with obtaining \"all the books in the world\". The company then began using destructive book scanning to digitize \"millions\" of books to train Claude.\r\n",
       "In 2024, Anthropic attracted several notable employees from OpenAI, including Jan Leike, John Schulman, and Durk Kingma.\r\n",
       "\r\n",
       "Additional funding and partnerships (2025)\r\n",
       "Anthropic raised $3.5 billion in a Series E funding round in March 2025, achieving a post-money valuation of $61.5 billion, led by Lightspeed Venture Partners with participation from several major investors. In March, Databricks and Anthropic announced that Claude would be integrated into the Databricks Data Intelligence Platform.\r\n",
       "In May 2025, the company announced Claude 4, introducing both Claude Opus 4 and Claude Sonnet 4 with improved coding capabilities and other new features. It also introduced new API capabilities, including the Model Context Protocol (MCP) connector. The company hosted its inaugural developer conference that month. Also in May, Anthropic launched a web search API that enables Claude to access real-time information from the internet. Claude Code, Anthropic's coding assistant, transitioned from research preview to general availability, featuring integrations with VS Code and JetBrains IDEs and support for GitHub Actions.\r\n",
       "In September 2025, Anthropic completed a Series F funding round, raising $13 billion at a post-money valuation of $183 billion.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** c47b0fef-a0a8-46e6-8f5f-d67ded0cb77b<br>**Similarity:** 0.6170979142189026<br>**Text:** In December 2016, OpenAI released \"Universe\", a software platform for measuring and training an AI's general intelligence across the world's supply of games, websites, and other applications.\r\n",
       "\r\n",
       "Corporate structure\r\n",
       "Transition from non-profit\r\n",
       "In 2019, OpenAI transitioned from non-profit to \"capped\" for-profit, with the profit being capped at 100 times any investment. According to OpenAI, the capped-profit model allows OpenAI Global, LLC to legally attract investment from venture funds and, in addition, to grant employees stakes in the company. Many top researchers work for Google Brain, DeepMind, or Facebook, which offer stock options that a nonprofit would be unable to. Before the transition, public disclosure of the compensation of top employees at OpenAI was legally required.\r\n",
       "The company then distributed equity to its employees and partnered with Microsoft, announcing an investment package of $1 billion into the company. Since then, OpenAI systems have run on an Azure-based supercomputing platform from Microsoft.\r\n",
       "OpenAI Global, LLC then announced its intention to commercially license its technologies. It planned to spend $1 billion \"within five years, and possibly much faster\". Altman stated that even a billion dollars may turn out to be insufficient, and that the lab may ultimately need \"more capital than any non-profit has ever raised\" to achieve artificial general intelligence.\r\n",
       "The nonprofit, OpenAI, Inc., is the sole controlling shareholder of OpenAI Global, LLC, which, despite being a for-profit company, retains a formal fiduciary responsibility to OpenAI, Inc.'s nonprofit charter. A majority of OpenAI, Inc.'s board is barred from having financial stakes in OpenAI Global, LLC. In addition, minority members with a stake in OpenAI Global, LLC are barred from certain votes due to conflict of interest. Some researchers have argued that OpenAI Global, LLC's switch to for-profit status is inconsistent with OpenAI's claims to be \"democratizing\" AI.\r\n",
       "On February 29, 2024, Elon Musk filed a lawsuit against OpenAI and CEO Sam Altman, accusing them of shifting focus from public benefit to profit maximization—a case OpenAI dismissed as \"incoherent\" and \"frivolous,\" though Musk later revived legal action against Altman and others in August.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "\n",
    "retrieved_nodes = BM25Retriever.from_defaults(\n",
    "    docstore=docstore,\n",
    "    similarity_top_k=5,\n",
    "    filters=filters,  # Add filters here\n",
    "    stemmer=Stemmer.Stemmer(\"english\"),\n",
    "    language=\"english\",\n",
    ").retrieve(\"Hello, world!\")\n",
    "\n",
    "for node in retrieved_nodes:\n",
    "    display_source_node(node, source_length=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472be930-3aeb-4e23-86c4-3a25c084a21c",
   "metadata": {},
   "source": [
    "## Hybrid Retriever with BM25 + FAISS（BM25 + FAISS 的混合式檢索器）\n",
    "\n",
    "### 1️⃣ 內容翻譯（繁體中文）\n",
    "\n",
    "### 使用 BM25 + FAISS 的混合式檢索器\n",
    "現在我們將結合 **BM25** 與 **FAISS**，同時進行**稀疏檢索（sparse retrieval）**與**稠密檢索（dense retrieval）**。\n",
    "\n",
    "檢索結果會透過 **QueryFusionRetriever** 進行整合。\n",
    "\n",
    "透過這個 retriever，我們可以建立一個完整的 **RetrieverQueryEngine**。\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ Hybrid Retriever（混合式檢索器）的好處說明\n",
    "\n",
    "混合式檢索器結合了 BM25 與向量檢索（如 FAISS）的優點，能有效彌補彼此的限制：\n",
    "\n",
    "- **提升召回率（Recall）**\n",
    "  - BM25 擅長精確的關鍵字比對\n",
    "  - FAISS 擅長捕捉語意相近但用詞不同的內容\n",
    "  - 兩者結合能找回更多「真正相關」的文件\n",
    "\n",
    "- **兼顧語意與關鍵字精準度**\n",
    "  - 對於專有名詞、數字、程式碼等，BM25 表現較佳\n",
    "  - 對於語意相似、同義改寫的查詢，向量檢索更有優勢\n",
    "\n",
    "- **降低單一檢索方式的盲點**\n",
    "  - 僅使用 BM25 可能忽略語意相關但未出現關鍵字的內容\n",
    "  - 僅使用向量檢索可能在精確字詞匹配上表現不足\n",
    "\n",
    "- **透過 Query Fusion 提升排序品質**\n",
    "  - QueryFusionRetriever 可以將多種檢索結果加權、融合與重新排序\n",
    "  - 提供更穩定且高品質的最終結果\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ 總結\n",
    "\n",
    "**Hybrid Retriever（BM25 + FAISS）** 是一種同時結合  \n",
    "👉 **關鍵字精準匹配** 與  \n",
    "👉 **語意理解能力** 的檢索策略。\n",
    "\n",
    "透過 **QueryFusionRetriever** 將結果整合後，再搭配 **RetrieverQueryEngine**，  \n",
    "可以打造出更準確、更健壯、也更適合實務應用的 RAG（Retrieval-Augmented Generation）系統。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4d5775a-6a7e-4b14-bbc5-e1496052be6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 08:46:27,564 - INFO - Loading faiss with AVX2 support.\n",
      "2026-01-07 08:46:27,717 - INFO - Successfully loaded faiss with AVX2 support.\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "\n",
    "d = 1024 # 必須與 embedding model 的輸出維度一致\n",
    "\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be5411f0-8626-42bb-960d-182d3529d52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for wiki_title in wiki_titles:\n",
    "    doc = SimpleDirectoryReader(\n",
    "        input_files=[f\"data/{wiki_title}.txt\"]\n",
    "    ).load_data()[0]\n",
    "\n",
    "    doc.metadata.update(wiki_metadatas[wiki_title])\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04d2f52a-1e6f-4dd3-b852-92ebdf5b5ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# initialize node parser\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0be2e4c9-c695-4424-b0c6-a7bed08f6da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 08:46:42,270 - INFO - Load pretrained SentenceTransformer: BAAI/bge-m3\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03f9d9fe-91f3-46b6-a872-4b5e58ddfc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.callbacks import LlamaDebugHandler, CallbackManager\n",
    "\n",
    "callback_manager = CallbackManager([LlamaDebugHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38b6279b-fab4-474f-a183-d9ac15c82076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_embedding -> 12.555748 seconds\n",
      "    |_embedding -> 14.325865 seconds\n",
      "    |_embedding -> 13.783602 seconds\n",
      "    |_embedding -> 12.726933 seconds\n",
      "    |_embedding -> 11.72713 seconds\n",
      "    |_embedding -> 12.662802 seconds\n",
      "    |_embedding -> 13.018565 seconds\n",
      "    |_embedding -> 13.692765 seconds\n",
      "    |_embedding -> 13.844663 seconds\n",
      "    |_embedding -> 12.957124 seconds\n",
      "    |_embedding -> 12.878496 seconds\n",
      "    |_embedding -> 8.88392 seconds\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex(nodes=nodes, \n",
    "                         storage_context=storage_context,\n",
    "                         callback_manager=callback_manager,\n",
    "                         embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f58d3c3e-bfac-42dc-af15-10ffef930fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.chdir(\"../\")\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from initialization import credential_init\n",
    "\n",
    "credential_init()\n",
    "\n",
    "llm = OpenAI(\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81ac10cb-42f3-4bb2-9060-97a7b1a93fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 09:13:05,405 - DEBUG - Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "retriever = QueryFusionRetriever(\n",
    "    [\n",
    "        index.as_retriever(similarity_top_k=2),\n",
    "        BM25Retriever.from_defaults(\n",
    "            docstore=index.docstore, similarity_top_k=2\n",
    "        ),\n",
    "    ],\n",
    "    num_queries=1,\n",
    "    use_async=True,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6deb6fc0-be38-407d-a2a7-420bb0e29d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Node ID:** f7c44124-d09f-4bfb-8f59-13f9090c1c55<br>**Similarity:** 1.1420073509216309<br>**Text:** In December 2016, OpenAI released \"Universe\", a software platform for measuring and training an AI's general intelligence across the world's supply of games, websites, and other applications.\r\n",
       "\r\n",
       "Corporate structure\r\n",
       "Transition from non-profit\r\n",
       "In 2019, OpenAI transitioned from non-profit to \"capped\" for-profit, with the profit being capped at 100 times any investment. According to OpenAI, the capped-profit model allows OpenAI Global, LLC to legally attract investment from venture funds and, in addition, to grant employees stakes in the company. Many top researchers work for Google Brain, DeepMind, or Facebook, which offer stock options that a nonprofit would be unable to. Before the transition, public disclosure of the compensation of top employees at OpenAI was legally required.\r\n",
       "The company then distributed equity to its employees and partnered with Microsoft, announcing an investment package of $1 billion into the company. Since then, OpenAI systems have run on an Azure-based supercomputing platform from Microsoft.\r\n",
       "OpenAI Global, LLC then announced its intention to commercially license its technologies. It planned to spend $1 billion \"within five years, and possibly much faster\". Altman stated that even a billion dollars may turn out to be insufficient, and that the lab may ultimately need \"more capital than any non-profit has ever raised\" to achieve artificial general intelligence.\r\n",
       "The nonprofit, OpenAI, Inc., is the sole controlling shareholder of OpenAI Global, LLC, which, despite being a for-profit company, retains a formal fiduciary responsibility to OpenAI, Inc.'s nonprofit charter. A majority of OpenAI, Inc.'s board is barred from having financial stakes in OpenAI Global, LLC. In addition, minority members with a stake in OpenAI Global, LLC are barred from certain votes due to conflict of interest. Some researchers have argued that OpenAI Global, LLC's switch to for-profit status is inconsistent with OpenAI's claims to be \"democratizing\" AI.\r\n",
       "On February 29, 2024, Elon Musk filed a lawsuit against OpenAI and CEO Sam Altman, accusing them of shifting focus from public benefit to profit maximization—a case OpenAI dismissed as \"incoherent\" and \"frivolous,\" though Musk later revived legal action against Altman and others in August.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 8a35877b-fcf3-4423-a2f0-583fc9bff598<br>**Similarity:** 1.1394356489181519<br>**Text:** The Model S was also the best-selling plug-in electric car worldwide for the years 2015 and 2016.\r\n",
       "On July 15, 2013, Tesla became a NASDAQ-100 company.\r\n",
       "Tesla announced the Tesla Autopilot, a driver-assistance system, in 2014. In September that year, all Tesla cars started shipping with sensors and software to support the feature, with what would later be called \"hardware version 1\".\r\n",
       "Tesla entered the energy storage market, unveiling its Tesla Powerwall (home) and Tesla Powerpack (business) battery packs in April 2015. The company received orders valued at $800 million within a week of the unveiling.\r\n",
       "Tesla began shipping its third vehicle, the luxury SUV Tesla Model X, in September 2015, which had 25,000 pre-orders at the time.\r\n",
       "\r\n",
       "SolarCity and Model 3 (2016–2018)\r\n",
       "Tesla entered the solar installation business in November 2016 with the purchase of SolarCity, in an all-stock $2.6 billion deal. The business was merged with Tesla's existing battery energy storage products division to form the Tesla Energy subsidiary. The deal was controversial because at the time of the acquisition, SolarCity was facing liquidity issues of which Tesla's shareholders were not informed. In February 2017, Tesla Motors changed its name to Tesla, Inc., to better reflect the scope of its expanded business.\r\n",
       "Tesla unveiled its first mass-market vehicle in April 2016, the Model 3 sedan. The Model 3 was less expensive than Tesla's previous three vehicles, and within a week, the company received over 325,000 paid reservations. To speed up production and control costs, Tesla invested heavily in robotics and automation to assemble the Model 3, but the robotics actually slowed the production of the vehicles. This led to significant delays and production problems, a period which the company described as \"production hell\". By the end of 2018, the production problems had been overcome, and the Model 3 became the world's best-selling electric car from 2018 to 2021.\r\n",
       "This period of \"production hell\" put significant financial pressure on Tesla, and during this time it became one of the most shorted companies in the stock market.<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nodes = retriever.retrieve(\"Hello, world!\")\n",
    "for node in nodes:\n",
    "    display_source_node(node, source_length=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680dda96-273f-47ea-9099-944dcc6878b1",
   "metadata": {},
   "source": [
    "**num_queries**：\n",
    "\n",
    "在這個步驟中，我們會將多個索引融合成一個單一的檢索器（retriever）。這個檢索器同時也會透過產生與原始問題相關的額外查詢來擴充（augment）你的查詢，並彙總所有查詢的結果。\n",
    "\n",
    "此設定會執行 4 次查詢：一次使用你的原始查詢，並另外產生 3 個新的查詢。\n",
    "\n",
    "預設情況下，它會使用以下提示（prompt）來產生額外的查詢："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa06eacc-b1f5-48b0-adc9-98a22a630d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQUERY_GEN_PROMPT = (\\n    \"You are a helpful assistant that generates multiple search queries based on a \"\\n    \"single input query. Generate {num_queries} search queries, one on each line, \"\\n    \"related to the following input query:\\n\"\\n    \"Query: {query}\\n\"\\n    \"Queries:\\n\"\\n)\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "QUERY_GEN_PROMPT = (\n",
    "    \"You are a helpful assistant that generates multiple search queries based on a \"\n",
    "    \"single input query. Generate {num_queries} search queries, one on each line, \"\n",
    "    \"related to the following input query:\\n\"\n",
    "    \"Query: {query}\\n\"\n",
    "    \"Queries:\\n\"\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "57856f59-e0bf-4bf2-bbf8-14bb17b3f1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 09:26:37,256 - DEBUG - Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "retriever = QueryFusionRetriever(\n",
    "    [\n",
    "        index.as_retriever(similarity_top_k=2),\n",
    "        BM25Retriever.from_defaults(\n",
    "            docstore=index.docstore, similarity_top_k=2\n",
    "        ),\n",
    "    ],\n",
    "    num_queries=2,\n",
    "    use_async=True,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16ace795-133a-4f83-afa7-71140b78f0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-07 09:26:43,582 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "nodes = retriever.retrieve(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "46977413-bbde-45ce-8ead-bdc74dd4594a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b32408-d2c9-4be1-b30b-88968b9345dc",
   "metadata": {},
   "source": [
    "Now, we can plug our retriever into a query engine to synthesize natural language responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffdc339-2fad-4c3a-a32d-7eb928d75969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "032ba4a1-301d-4e30-af42-04eeef53186b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mQueryFusionRetriever\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mretrievers\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mllama_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_retriever\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseRetriever\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mllm\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mllms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLLM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'BaseLanguageModel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mquery_gen_prompt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrievers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfusion_retriever\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFUSION_MODES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m<\u001b[0m\u001b[0mFUSION_MODES\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSIMPLE\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'simple'\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msimilarity_top_k\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnum_queries\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0muse_async\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcallback_manager\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mllama_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCallbackManager\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mobjects\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mllama_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIndexNode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mobject_map\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mretriever_weights\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m      Base retriever.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\ling\\miniconda3\\envs\\rag\\lib\\site-packages\\llama_index\\core\\retrievers\\fusion_retriever.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "QueryFusionRetriever?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413013ed-74ae-41da-83a0-0d4aadcbe938",
   "metadata": {},
   "source": [
    "### 1. Reciprocal Rerank（RRF）\n",
    "\n",
    "**模式（Mode）**：`FUSION_MODE.RECIPROCAL_RANK`（預設）\n",
    "\n",
    "**運作方式（How it works）**：  \n",
    "這是目前最常見、也最受歡迎的方法。它不會使用 LLM 來對文件進行評分，而是透過一個數學公式，根據文件在多個結果清單中的排名（rank）來重新排序。\n",
    "\n",
    "**公式（Formula）**：  \n",
    "\n",
    "$$\n",
    "score = \\sum_{d \\in r} \\frac{1}{k + rank(d)}\n",
    "$$\n",
    "\n",
    "其中，`k` 是一個常數（通常為 60）。\n",
    "\n",
    "**適用情境（Best for）**：  \n",
    "通用型的「混合搜尋（Hybrid Search）」，當你希望在向量搜尋與關鍵字搜尋之間取得良好平衡，同時避免額外的 LLM 成本時，非常適合使用。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Relative Score Fusion（相對分數融合）\n",
    "\n",
    "**模式（Mode）**：`FUSION_MODE.RELATIVE_SCORE`\n",
    "\n",
    "**運作方式（How it works）**：  \n",
    "此方法不再關注文件的排名（例如第 1 名、第 2 名），而是直接使用實際的相似度分數。  \n",
    "它會將來自不同檢索器（通常分數尺度不同）的分數正規化到 0 到 1 的範圍內，接著再對這些分數取平均。\n",
    "\n",
    "**適用情境（Best for）**：  \n",
    "當檢索器所提供的信心分數（confidence score）本身就相當可靠時。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Distribution-Based Score Fusion（分佈式分數融合）\n",
    "\n",
    "**模式（Mode）**：`FUSION_MODE.DIST_BASED_SCORE`\n",
    "\n",
    "**運作方式（How it works）**：  \n",
    "這是一種更進階的分數融合方法，會同時考慮結果集中分數的平均值（mean）與標準差（standard deviation）。  \n",
    "這有助於處理不同檢索器可能產生「分數非常集中」或「分數分佈非常分散」的情況。\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Simple Fusion（簡單融合）\n",
    "\n",
    "**模式（Mode）**：`FUSION_MODE.SIMPLE`\n",
    "\n",
    "**運作方式（How it works）**：  \n",
    "這是一種基礎做法，通常只是將結果直接串接（concatenation）或進行簡單加總。  \n",
    "相較於 RRF，這種方法在實際生產環境中較少使用，但可用於測試基準效能（baseline performance）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552dd0cd-f318-4046-9523-5bb6c0825a18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
